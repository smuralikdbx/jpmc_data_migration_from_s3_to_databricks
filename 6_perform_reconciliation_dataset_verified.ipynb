{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccdad64f-6ef5-48ec-b2c5-8a67e31de4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CCBR Migration Data Reconciliation Notebook\n",
    "Purpose:\n",
    "This notebook performs comprehensive data reconciliation between S3 source data and \n",
    "Databricks managed tables for the CCBR migration project. It executes multiple \n",
    "reconciliation types and stores results in a clean, simplified enhanced results table.\n",
    "\n",
    "Reconcilation Types Performed:\n",
    "\n",
    "1. ROW_COUNT\n",
    "   - Compares total record counts between S3 source and managed target tables\n",
    "   - Status: PASS/FAIL based on exact match\n",
    "   - Metrics: Source count vs Target count\n",
    "\n",
    "2. COLUMN_COUNT  \n",
    "   - Compares number of columns between source and target schemas\n",
    "   - Status: PASS/FAIL based on exact match\n",
    "   - Metrics: Source column count vs Target column count\n",
    "\n",
    "3. SCHEMA_VALIDATION\n",
    "   - Validates schema compatibility and identifies column mismatches\n",
    "   - Status: PASS if schemas match exactly, FAIL if differences found\n",
    "   - Summary: Lists specific missing columns in source/target with names\n",
    "   - Example: \"Missing in source: [col1, col2] | Missing in target: [col3]\"\n",
    "\n",
    "4. VERIFIED\n",
    "   - Compares Databricks managed table statistics with computed S3 equivalents\n",
    "   - Statistics: num_nulls, distinct_count, min/max, avg_col_len, max_col_len\n",
    "   - Comprehensive statistical comparison across all available metrics\n",
    "   - Summary: \"Statistics comparison: X/Y stats match (Z%)\"\n",
    "     - Methods: abs_sum (numeric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b93a7f-eb69-441d-9924-020b6f52736e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Parameter Setup and Configuration\n",
    "\n",
    "Initializes all required Databricks widget parameters for dynamic runtime configuration. Captures catalog, schema, and table names for results, mapping, and candidate metadata. These parameters allow the notebook to be executed across different environments without code changes.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `catalog_name`: Target Unity Catalog name\n",
    "- `schema_name`: Schema for storing reconciliation results\n",
    "- `managed_schema`: Schema containing managed tables to validate\n",
    "- `results_table`: Main results table for reconciliation outcomes\n",
    "- `mapping_table`: Source-to-target mapping configuration\n",
    "- `candidate_table`: Table metadata and partition information\n",
    "- `run_id` & `job_id`: Execution tracking identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578a5193-613c-420c-a7e7-894000e95f70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parameter Setup"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter Setup and Configuration\n",
    "dbutils.widgets.text(\"catalog\", \"\") #smuralik_catalog\n",
    "dbutils.widgets.text(\"schema\", \"\") #jpmc_ccbr\n",
    "dbutils.widgets.text(\"recon_results_table\", \"\") #ccbr_migration_recon_results\n",
    "dbutils.widgets.text(\"dataset_mapping_table\", \"\") #ccbr_migration_dataset_mapping\n",
    "dbutils.widgets.text(\"candidate_table\", \"\") #ccbr_migration_table_candidates\n",
    "dbutils.widgets.text(\"partition_audit_table\",\"\") #89055_ctg_prod_exp.default.dataset_tags\n",
    "dbutils.widgets.text(\"bucket_name\",\"\") \n",
    "dbutils.widgets.text(\"parquet_schema_table\",\"\")\n",
    "dbutils.widgets.text(\"inventory_table\",\"\") #ccbr_migration_table_inventory\n",
    "dbutils.widgets.text(\"dataset_name\",\"\") #ccbr_migration_table_inventory\n",
    "\n",
    "\n",
    "# dbutils.widgets.text(\"run_id\", \"\")\n",
    "# dbutils.widgets.text(\"job_id\", \"\")\n",
    "# dbutils.widgets.text(\"managed_table_schema\", \"\") #jpmc_ccbr_managed_tables\n",
    "# dbutils.widgets.text(\"dataset_inventory_mapping_volume_location\", \"\")\n",
    "\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema\")\n",
    "RESULTS_TABLE = dbutils.widgets.get(\"recon_results_table\")\n",
    "MAPPING_TABLE = dbutils.widgets.get(\"dataset_mapping_table\")\n",
    "CANDIDATES_TABLE = dbutils.widgets.get(\"candidate_table\")\n",
    "inventory_table = dbutils.widgets.get(\"inventory_table\")\n",
    "PARTITION_AUDIT_TABLE = dbutils.widgets.get(\"partition_audit_table\")\n",
    "parquet_schema_table = dbutils.widgets.get(\"parquet_schema_table\")\n",
    "BUCKET_NAME=dbutils.widgets.get(\"bucket_name\") \n",
    "dataset_name = dbutils.widgets.get(\"dataset_name\") #ccbr_migration_table_inventory\n",
    "\n",
    "\n",
    "# RUN_ID = dbutils.widgets.get(\"run_id\")\n",
    "# JOB_ID = dbutils.widgets.get(\"job_id\")\n",
    "#Partition_Schema = 'dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp'\n",
    "#parquet_schema_table = 'ccbr_migration_parquet_schemas'\n",
    "\n",
    "# MANAGED_SCHEMA = dbutils.widgets.get(\"managed_table_schema\")\n",
    "# VOLUME_LOCATION = dbutils.widgets.get(\"dataset_inventory_mapping_volume_location\")\n",
    "\n",
    "\n",
    "RESULTS_TABLE_FQN = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{RESULTS_TABLE}\"\n",
    "MAPPING_TABLE_FQN = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MAPPING_TABLE}\"\n",
    "CANDIDATES_TABLE_FQN = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{CANDIDATES_TABLE}\"\n",
    "PARTITION_AUDIT_TABLE_FQN = f\"{PARTITION_AUDIT_TABLE}\"\n",
    "inventory_table_FQN = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{inventory_table}\"\n",
    "# execution_id=\"\"\n",
    "\n",
    "print(f\"Results table: {RESULTS_TABLE_FQN}\")\n",
    "print(f\"Mapping table: {MAPPING_TABLE_FQN}\")\n",
    "print(f\"Candidates table: {CANDIDATES_TABLE_FQN}\")\n",
    "print(f\"Audit table: {PARTITION_AUDIT_TABLE_FQN}\")\n",
    "print(f\"Inventory table: {inventory_table_FQN}\")\n",
    "print(f\"dataset_name : {dataset_name}\")\n",
    "#89055_ctg_prod_exp.dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp.ccbr_migration_table_inventory\n",
    "#89055_ctg_prod_exp.dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp.ccbr_migration_table_inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55d3958-5861-477d-a3ca-bab1576145db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Schema Definition\n",
    "\n",
    "Defines the comprehensive schema structure for the reconciliation results table. This enhanced schema captures detailed metrics, execution metadata, and error information for each reconciliation check.\n",
    "\n",
    "**Schema Components:**\n",
    "- Execution metadata (ID, timestamp, duration)\n",
    "- Reconciliation type and status\n",
    "- Source and target metrics (JSON format)\n",
    "- Statistical comparison results\n",
    "- Summary and error logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240d03b4-121b-499e-b07a-4184c6d3a07a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema Definition"
    }
   },
   "outputs": [],
   "source": [
    "# Define recon results schema for enhanced results table with error_log column\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "recon_results_schema = StructType([\n",
    "    StructField(\"execution_id\", StringType(), True),\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"recon_type\", StringType(), True),\n",
    "    StructField(\"exec_timestamp\", TimestampType(), True),\n",
    "    StructField(\"exec_duration\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"source_metrics\", StringType(), True),\n",
    "    StructField(\"target_metrics\", StringType(), True),\n",
    "    StructField(\"metrics_compared\", IntegerType(), True),\n",
    "    StructField(\"metrics_matched\", IntegerType(), True),\n",
    "    StructField(\"metrics_different\", IntegerType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"error_log\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9e40db-237d-4787-a979-0bc40a2b98b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Core Helper Functions\n",
    "\n",
    "Implements fundamental utility functions for data operations and schema analysis. These functions provide the foundation for reading data, comparing schemas, and performing basic reconciliation calculations.\n",
    "\n",
    "**Key Functions:**\n",
    "- `read_source_parquet()`: Reads S3 parquet data with error handling\n",
    "- `get_schema_dict()`: Extracts detailed schema information including precision/scale\n",
    "- `compare_schemas()`: Comprehensive schema comparison with type and precision validation\n",
    "- `compute_variance()`: Calculates percentage differences between metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d9a323-0e13-43ba-ac1b-ece711c17433",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions"
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Helper Functions\n",
    "# Core utility functions for data operations\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_extract, collect_list, lit, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DecimalType, DateType, TimestampType, BinaryType, ShortType, ByteType, FloatType, DoubleType\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "# Configure Spark to display full content without truncation\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 10000)\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "\n",
    "# Configure pandas display options\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# this method is used for getting the base schema \n",
    "def parse_type(dtype):\n",
    "    dtype = dtype.lower().strip()\n",
    "\n",
    "    if dtype.startswith(\"decimal\"):\n",
    "        scale = dtype[dtype.find(\"(\") + 1 : dtype.find(\")\")].split(\",\")\n",
    "        return DecimalType(int(scale[0]), int(scale[1]))\n",
    "\n",
    "    if dtype in (\"int8\", \"byte\"):\n",
    "        return ByteType()\n",
    "    \n",
    "    if dtype.startswith(\"bytetype\"):\n",
    "        return ByteType()\n",
    "\n",
    "    if dtype in (\"int16\", \"smallint\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype.startswith(\"shorttype\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype in (\"int32\", \"integer\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype.startswith(\"integertype\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype == \"int64\":\n",
    "        return LongType()\n",
    "\n",
    "    if dtype.startswith(\"longtype\"):\n",
    "        return LongType()\n",
    "    \n",
    "    if dtype in (\"string\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"stringtype\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"date32\") or dtype == \"date\":\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"datetype\"):\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"floattype\"):\n",
    "        return FloatType()\n",
    "    \n",
    "    if dtype.startswith(\"doubletype\"):\n",
    "        return DoubleType()\n",
    "\n",
    "    if dtype.startswith(\"timestamp\"):\n",
    "        # Handles 'timestamp', 'timestamp[ms]', 'timestamp[us]' etc.\n",
    "        return TimestampType()\n",
    "\n",
    "    if dtype == \"bool\":\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype.startswith(\"booleantype\"):\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype == \"binary\":\n",
    "        return BinaryType()\n",
    "    \n",
    "    if dtype.startswith(\"binarytype\"):\n",
    "        return BinaryType()\n",
    "    \n",
    "    raise ValueError(f\"Unsupported type: {dtype}\")\n",
    "\n",
    "def read_source_parquet(bucket: str, prefix: str, dataset_name: str) -> DataFrame:\n",
    "    # Reads parquet files from S3 path and returns a DataFrame\n",
    "    #Get latest Schema\n",
    "    partition_key_combination = \"edp_run_id, snapshot_date\"\n",
    "    inventory_schema_df = spark.sql(f\"\"\"SELECT distinct edp_run_id, snapshot_date\n",
    "                                FROM \n",
    "                                (\n",
    "                                    select distinct edp_run_id, try_cast(snapshot_date as date) as snapshot_date\n",
    "                                    from {inventory_table_FQN}\n",
    "                                    where 1 = 1\n",
    "                                    and extension is not null \n",
    "                                    and lower(partition_key) = '{partition_key_combination}'\n",
    "                                    and s3_bucket_name = '{bucket}' \n",
    "                                    and bucket_prefix = '{prefix}'\n",
    "                                    and load_status ='loaded'\n",
    "                                    and try_cast(snapshot_date as date) >= '2020-01-01'\n",
    "                                ) inventory\n",
    "                                join\n",
    "                                (\n",
    "                                    select distinct run_id, try_cast(run_tag_value as date) as run_tag_value \n",
    "                                    from {PARTITION_AUDIT_TABLE_FQN}\n",
    "                                    where dataset_name = '{dataset_name}'\n",
    "                                    and lower(run_tag_key) = 'snapshot_date'\n",
    "                                    and try_cast(run_tag_value as date) >= '2020-01-01'\n",
    "                                ) partition\n",
    "                                on inventory.edp_run_id = partition.run_id\n",
    "                                and inventory.snapshot_date = partition.run_tag_value\n",
    "                                \"\"\")\n",
    "    if not inventory_schema_df.isEmpty():\n",
    "        df_latest_snapshot = (inventory_schema_df\n",
    "                                .orderBy(col(\"snapshot_date\").desc())\n",
    "                                .limit(1)\n",
    "                                )\n",
    "\n",
    "        latest_snapshot_row = df_latest_snapshot.collect()[0]\n",
    "\n",
    "        latest_edp_run_id = latest_snapshot_row[\"edp_run_id\"]\n",
    "        latest_snapshot_date = latest_snapshot_row[\"snapshot_date\"]\n",
    "        schema_json_df = spark.sql(f\"\"\"select schema_json \n",
    "                                    from {CATALOG_NAME}.{SCHEMA_NAME}.{parquet_schema_table}\n",
    "                                    where 1 = 1\n",
    "                                    and s3_bucket_name = '{bucket}'\n",
    "                                    and bucket_prefix = '{prefix}'\n",
    "                                    and lower(file_path) like '%edp_run_id={latest_edp_run_id}/snapshot_date={latest_snapshot_date}%'\n",
    "                                    \"\"\")\n",
    "\n",
    "        # Extract the schema_json value into a Python string variable\n",
    "        schema_json = schema_json_df.first()['schema_json']\n",
    "\n",
    "        schema_dict = json.loads(schema_json)\n",
    "\n",
    "        # Convert to StructType\n",
    "        base_schema = StructType([\n",
    "            StructField(f[\"name\"], parse_type(f[\"type\"]), f[\"nullable\"])\n",
    "            for f in schema_dict[\"fields\"]\n",
    "        ])\n",
    "\n",
    "        partition_columns = [partition_column.strip() for partition_column in partition_key_combination.split(\",\") if partition_column.strip()]\n",
    "\n",
    "        # You can define mapping for known types here\n",
    "        partition_type_map = {\n",
    "            \"edp_run_id\": StringType(),\n",
    "            \"snapshot_date\": DateType()\n",
    "        }\n",
    "\n",
    "        extended_fields = base_schema.fields.copy()\n",
    "\n",
    "        for partition_column in partition_columns:\n",
    "            col_type = partition_type_map.get(partition_column, StringType())  # default to STRING\n",
    "            extended_fields.append(StructField(partition_column, col_type, True))\n",
    "\n",
    "        extended_schema = StructType(extended_fields)\n",
    "        # print(extended_schema)\n",
    "\n",
    "        path = f\"s3://{bucket}/{prefix}\" \n",
    "        s3_df=spark.read.format(\"parquet\").schema(extended_schema).option(\"ignoreCorruptFiles\", \"true\").load(path)\n",
    "        #display(s3_df)\n",
    "\n",
    "        partition_audit_df=spark.sql(f\"\"\" \n",
    "                                        select distinct dataset_name, run_id, run_tag_value from {PARTITION_AUDIT_TABLE_FQN}\n",
    "                                        where lower(run_tag_key) = 'snapshot_date' \n",
    "                                        AND dataset_name ='{dataset_name}'\n",
    "                                        AND try_cast(run_tag_value as date) >= '2020-01-01'\n",
    "                                    \"\"\")\n",
    "        # display(partition_audit_df)\n",
    "        #print(bucket)\n",
    "        Inventory_df=spark.sql(f\"\"\" \n",
    "                                    select distinct edp_run_id,snapshot_date from {inventory_table_FQN}\n",
    "                                    where load_status in ('loaded','failed')\n",
    "                                    and s3_bucket_name = '{bucket}'\n",
    "                                    and bucket_prefix = '{prefix}'\n",
    "                                    and extension is not null\n",
    "                                    and try_cast(snapshot_date as date) >= '2020-01-01'\n",
    "                                \"\"\")\n",
    "\n",
    "        if partition_audit_df.count() > 0 and Inventory_df.count()>0: \n",
    "            # display(partition_audit_df)\n",
    "            filter_df = (\n",
    "                s3_df.join(\n",
    "                    partition_audit_df,\n",
    "                    on=[\n",
    "                        to_date(s3_df[\"snapshot_date\"], 'yyyy-MM-dd') == to_date(partition_audit_df[\"run_tag_value\"],'yyyy-MM-dd'),\n",
    "                        s3_df[\"edp_run_id\"] == partition_audit_df[\"run_id\"]\n",
    "                    ],\n",
    "                    how=\"inner\"\n",
    "                )\n",
    "                .join (\n",
    "                    Inventory_df,\n",
    "                    on=[\n",
    "                        to_date(s3_df[\"snapshot_date\"], 'yyyy-MM-dd') == to_date(Inventory_df[\"snapshot_date\"],'yyyy-MM-dd'),\n",
    "                        s3_df[\"edp_run_id\"] == Inventory_df[\"edp_run_id\"]\n",
    "                    ],\n",
    "                    how=\"inner\"\n",
    "                )\n",
    "                .select(s3_df[\"*\"])\n",
    "                .filter(to_date(col(\"snapshot_date\"), 'yyyy-MM-dd') >= lit('2020-01-01'))\n",
    "            )\n",
    "            return filter_df\n",
    "        else:\n",
    "            return s3_df.filter(to_date(col(\"snapshot_date\"), 'yyyy-MM-dd') >= lit('2020-01-01'))\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "# df=read_source_parquet(\"app-id-89055-dep-id-109792-uu-id-wj1in46hrkbl\", \"trusted/ops/auto_srvc_coll/credit_losses/\",\"credit_losses\")\n",
    "# display(df)\n",
    "# # df.select(\"snapshot_date\", \"edp_run_id\").distinct().show(truncate=False)\n",
    "\n",
    "def get_schema_dict(df: DataFrame) -> dict:\n",
    "    # Converts DataFrame schema to a dictionary {column_name: (data_type, precision, scale)}\n",
    "    schema_info = {}\n",
    "    for field in df.schema.fields:\n",
    "        data_type = field.dataType.simpleString()\n",
    "        precision = None\n",
    "        scale = None\n",
    "        \n",
    "        # Extract precision and scale for decimal types\n",
    "        if hasattr(field.dataType, 'precision') and hasattr(field.dataType, 'scale'):\n",
    "            precision = field.dataType.precision\n",
    "            scale = field.dataType.scale\n",
    "            \n",
    "        schema_info[field.name] = {\n",
    "            'data_type': data_type,\n",
    "            'precision': precision,\n",
    "            'scale': scale\n",
    "        }\n",
    "    \n",
    "    return schema_info\n",
    "\n",
    "def compare_schemas(src_schema: dict, tgt_schema: dict):\n",
    "    # Enhanced schema comparison including precision and scale\n",
    "    src_schema = {k.lower() : v for k,v in src_schema.items()}\n",
    "    tgt_schema = {k.lower() : v for k,v in tgt_schema.items()}\n",
    "    missing_in_src = []\n",
    "    missing_in_tgt = []\n",
    "    mismatched = []\n",
    "    precision_mismatched = []\n",
    "    \n",
    "    # Check for missing columns\n",
    "    for col in tgt_schema:\n",
    "        if col not in src_schema:\n",
    "            missing_in_src.append(col)\n",
    "    \n",
    "    for col in src_schema:\n",
    "        if col not in tgt_schema:\n",
    "            missing_in_tgt.append(col)\n",
    "    \n",
    "    # Check for data type and precision mismatches\n",
    "    for col in src_schema:\n",
    "        if col in tgt_schema:\n",
    "            src_info = src_schema[col]\n",
    "            tgt_info = tgt_schema[col]\n",
    "            \n",
    "            # Check data type mismatch\n",
    "            if src_info['data_type'] != tgt_info['data_type']:\n",
    "                mismatched.append(f\"{col} (src: {src_info['data_type']}, tgt: {tgt_info['data_type']})\")\n",
    "            \n",
    "            # Check precision/scale mismatch for decimal types\n",
    "            elif (src_info['precision'] != tgt_info['precision'] or \n",
    "                  src_info['scale'] != tgt_info['scale']):\n",
    "                if src_info['precision'] is not None or tgt_info['precision'] is not None:\n",
    "                    precision_mismatched.append(\n",
    "                        f\"{col} (src: p={src_info['precision']},s={src_info['scale']}, \"\n",
    "                        f\"tgt: p={tgt_info['precision']},s={tgt_info['scale']})\"\n",
    "                    )\n",
    "    \n",
    "    return missing_in_src, missing_in_tgt, mismatched, precision_mismatched\n",
    "\n",
    "def compute_variance(src_cnt: int, tgt_cnt: int) -> float:\n",
    "    # Computes percentage variance between source and target counts\n",
    "    if src_cnt == 0:\n",
    "        return 0.0\n",
    "    return round(abs(src_cnt - tgt_cnt) / src_cnt * 100.0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ea8915-7e03-4415-9b6c-40e38a759021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Advanced Column Validation Functions\n",
    "\n",
    "Provides sophisticated statistical analysis capabilities for both Databricks managed tables and source data. Computes comprehensive column-level statistics including null counts, distinct values, min/max values, and absolute sums for numeric columns.\n",
    "\n",
    "**Key Functions:**\n",
    "- `get_table_stats()`: Extracts Databricks table statistics with fallback computation\n",
    "- `compute_source_stats()`: Calculates equivalent statistics for source S3 data\n",
    "- Handles both managed table metadata and direct computation\n",
    "- Supports numeric aggregations (abs_sum) and string length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63b9a55-1915-465a-b7fd-41975e80f1d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Column Validation Functions"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced column-level validation and statistics computation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from pyspark.sql import DataFrame\n",
    "numeric_cats = {\"int\", \"long\", \"double\", \"float\", \"decimal\", \"bigint\"}\n",
    "DEFAULT_NUM_COLUMNS = 20\n",
    "\n",
    "def get_table_stats(df: DataFrame, selected_columns: list = None, column_types: dict = None):\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Get MIN, MAX, ABS_SUM statistics for selected columns only\n",
    "    Computes all stats in a single aggregation query for performance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_df = df\n",
    "        target_df = target_df.toDF(*[c.lower() for c in target_df.columns])\n",
    "        if selected_columns:\n",
    "            selected_columns = [c.lower() for c in selected_columns]\n",
    "        # If no columns specified, return empty\n",
    "        if not selected_columns:\n",
    "            print(f\"Warning: No columns selected for stats computation\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"Computing target stats for {len(selected_columns)} columns...\")\n",
    "        \n",
    "        # Build aggregation expressions for all columns in ONE query\n",
    "        agg_exprs = []\n",
    "        \n",
    "        for col_name in selected_columns:\n",
    "            # Check if column exists in DataFrame\n",
    "            if col_name not in target_df.columns:\n",
    "                print(f\"Warning: Column {col_name} not found in target table\")\n",
    "                continue\n",
    "            \n",
    "            # MIN and MAX for all columns\n",
    "            agg_exprs.append(min(col(col_name)).alias(f\"{col_name}_min\"))\n",
    "            agg_exprs.append(max(col(col_name)).alias(f\"{col_name}_max\"))\n",
    "            \n",
    "            # ABS_SUM only for numeric columns\n",
    "            is_numeric = False\n",
    "            if column_types:\n",
    "                for cat in ['int', 'decimal']:\n",
    "                    if col_name in column_types.get(cat, []):\n",
    "                        is_numeric = True\n",
    "                        break\n",
    "            \n",
    "            if is_numeric:\n",
    "                agg_exprs.append(sum(abs(col(col_name))).alias(f\"{col_name}_abs_sum\"))\n",
    "        \n",
    "        if not agg_exprs:\n",
    "            print(\"Warning: No valid aggregation expressions built\")\n",
    "            return {}\n",
    "        \n",
    "        # Execute single aggregation query\n",
    "        stats_row = target_df.agg(*agg_exprs).collect()[0]\n",
    "        \n",
    "        # Parse results into structured format\n",
    "        col_stats = {}\n",
    "        for col_name in selected_columns:\n",
    "            if col_name not in target_df.columns:\n",
    "                continue\n",
    "                \n",
    "            col_stats[col_name] = {}\n",
    "            \n",
    "            # Get MIN and MAX\n",
    "            min_key = f\"{col_name}_min\"\n",
    "            max_key = f\"{col_name}_max\"\n",
    "            \n",
    "            if min_key in stats_row.asDict():\n",
    "                col_stats[col_name]['min'] = str(stats_row[min_key]) if stats_row[min_key] is not None else 'null'\n",
    "            \n",
    "            if max_key in stats_row.asDict():\n",
    "                col_stats[col_name]['max'] = str(stats_row[max_key]) if stats_row[max_key] is not None else 'null'\n",
    "            \n",
    "            # Get ABS_SUM if it exists\n",
    "            abs_sum_key = f\"{col_name}_abs_sum\"\n",
    "            if abs_sum_key in stats_row.asDict():\n",
    "                col_stats[col_name]['abs_sum'] = str(stats_row[abs_sum_key]) if stats_row[abs_sum_key] is not None else '0'\n",
    "        \n",
    "        return col_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting stats {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return {}\n",
    "    \n",
    "def compute_source_stats(df: DataFrame, selected_columns: list = None, column_types: dict = None):\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Compute MIN, MAX, ABS_SUM statistics for selected columns only\n",
    "    Computes all stats in a single aggregation query for performance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.toDF(*[c.lower() for c in df.columns])\n",
    "        if selected_columns:\n",
    "            selected_columns = [c.lower() for c in selected_columns]\n",
    "        # If no columns specified, return empty\n",
    "        if not selected_columns:\n",
    "            print(f\"Warning: No columns selected for stats computation\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"Computing source stats for {len(selected_columns)} columns...\")\n",
    "        \n",
    "        # Build aggregation expressions for all columns in ONE query\n",
    "        agg_exprs = []\n",
    "        \n",
    "        for col_name in selected_columns:\n",
    "            # Check if column exists in DataFrame\n",
    "            if col_name not in df.columns:\n",
    "                print(f\"Warning: Column {col_name} not found in source DataFrame\")\n",
    "                continue\n",
    "            \n",
    "            # MIN and MAX for all columns\n",
    "            agg_exprs.append(min(col(col_name)).alias(f\"{col_name}_min\"))\n",
    "            agg_exprs.append(max(col(col_name)).alias(f\"{col_name}_max\"))\n",
    "            \n",
    "            # ABS_SUM only for numeric columns\n",
    "            is_numeric = False\n",
    "            if column_types:\n",
    "                for cat in ['int', 'decimal']:\n",
    "                    if col_name in column_types.get(cat, []):\n",
    "                        is_numeric = True\n",
    "                        break\n",
    "            \n",
    "            if is_numeric:\n",
    "                agg_exprs.append(sum(abs(col(col_name))).alias(f\"{col_name}_abs_sum\"))\n",
    "        \n",
    "        if not agg_exprs:\n",
    "            print(\"Warning: No valid aggregation expressions built\")\n",
    "            return {}\n",
    "        \n",
    "        # Execute single aggregation query\n",
    "        stats_row = df.agg(*agg_exprs).collect()[0]\n",
    "        \n",
    "        # Parse results into structured format\n",
    "        col_stats = {}\n",
    "        for col_name in selected_columns:\n",
    "            if col_name not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            col_stats[col_name] = {}\n",
    "            \n",
    "            # Get MIN and MAX\n",
    "            min_key = f\"{col_name}_min\"\n",
    "            max_key = f\"{col_name}_max\"\n",
    "            \n",
    "            if min_key in stats_row.asDict():\n",
    "                col_stats[col_name]['min'] = str(stats_row[min_key]) if stats_row[min_key] is not None else 'null'\n",
    "            \n",
    "            if max_key in stats_row.asDict():\n",
    "                col_stats[col_name]['max'] = str(stats_row[max_key]) if stats_row[max_key] is not None else 'null'\n",
    "            \n",
    "            # Get ABS_SUM if it exists\n",
    "            abs_sum_key = f\"{col_name}_abs_sum\"\n",
    "            if abs_sum_key in stats_row.asDict():\n",
    "                col_stats[col_name]['abs_sum'] = str(stats_row[abs_sum_key]) if stats_row[abs_sum_key] is not None else '0'\n",
    "        \n",
    "        return col_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error computing source stats: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ef9558-6ce4-424a-9ac1-362c8af89325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load and Validate Mapping Data\n",
    "\n",
    "Retrieves source-to-target table mapping configuration from the metadata tables. Handles both direct S3-to-table mappings and indirect mappings through staging layers. Uses complex SQL joins to resolve the complete data lineage.\n",
    "\n",
    "**Functionality:**\n",
    "- Queries mapping table for S3 source to managed table relationships\n",
    "- Handles indirect mappings via staging tables\n",
    "- Validates table existence in information_schema\n",
    "- Returns complete source bucket/prefix to target table mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca9d1997-6a15-4be8-a90d-c849829092d5",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"execution_id\":284,\"target_schema_name\":425,\"source_s3_bucket\":321,\"source_bucket_prefix\":300},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762867387762}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1761925808268}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "Load Mapping Data"
    }
   },
   "outputs": [],
   "source": [
    "#Load and validate source-to-target mapping data\n",
    "# Handles both direct S3-to-table mappings and indirect mappings through staging\n",
    "\n",
    "def load_mapping_data():\n",
    "   \n",
    "    mapping_sql = f\"\"\"\n",
    "        WITH managed_tables AS (\n",
    "            SELECT DISTINCT table_name, execution_id\n",
    "            from {CANDIDATES_TABLE_FQN} where  \n",
    "            managed_table_created =true \n",
    "            and s3_bucket_name='{BUCKET_NAME}'\n",
    "            and table_name in ({dataset_name})\n",
    "          \n",
    "        )\n",
    "        SELECT \n",
    "        m.execution_id,\n",
    "        i.dbx_catalog AS target_catalog_name, \n",
    "        i.dbx_managed_table_schema AS target_schema_name, \n",
    "        i.dataset_name AS target_table_name, \n",
    "        i.s3_bucket_name AS source_s3_bucket,\n",
    "        i.bucket_prefix AS source_bucket_prefix\n",
    "        -- concat(b.volume_name,\"/\",i.bucket_prefix) as source_bucket_prefix\n",
    "        FROM {MAPPING_TABLE_FQN} i\n",
    "        INNER JOIN managed_tables m ON i.dataset_name = m.table_name\n",
    "    \"\"\"\n",
    "    \n",
    "    # concat(i.dbx_catalog,'.', i.dbx_managed_table_schema,'.',i.dataset_name) AS target_table_name, \n",
    "    # print(mapping_sql)\n",
    "    try:\n",
    "        return spark.sql(mapping_sql)\n",
    "    except Exception:\n",
    "        return spark.createDataFrame([], StructType([\n",
    "            StructField(\"target_catalog_name\", StringType()),\n",
    "            StructField(\"target_schema_name\", StringType()),\n",
    "            StructField(\"target_table_name\", StringType()),\n",
    "            StructField(\"source_s3_bucket\", StringType()),\n",
    "            StructField(\"source_bucket_prefix\", StringType())\n",
    "        ]))\n",
    "\n",
    "mapping_df = load_mapping_data()\n",
    "display(mapping_df)\n",
    "#print(f\"Loaded {mapping_df.count()} table mappings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada22d8e-12ed-47e2-8bcb-6ccb99c066de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CHECKSUM HELPER FUNCTIONS"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# CHECKSUM HELPER FUNCTIONS\n",
    "# =========================================================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def prepare_for_checksum(\n",
    "    df,\n",
    "    partition_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Standardizes DataFrame for checksum calculation:\n",
    "    - Excludes partition columns\n",
    "    - Orders columns consistently\n",
    "    - Trims whitespace in string columns\n",
    "    - Casts all columns to string and replaces nulls with 'NULL'\n",
    "    \"\"\"\n",
    "    # partition_cols = partition_cols or []\n",
    "    # cols_to_hash = [c for c in df.columns if c not in partition_cols]\n",
    "    # cols_to_hash = sorted(cols_to_hash)\n",
    "    # for c in cols_to_hash:\n",
    "    #     df = df.withColumn(\n",
    "    #         c,\n",
    "    #         F.when(F.col(c).isNull(), F.lit(\"NULL\"))\n",
    "    #          .otherwise(F.trim(F.col(c).cast(\"string\")))\n",
    "    #     )\n",
    "    # return df.select(cols_to_hash)\n",
    "    partition_cols = partition_cols or []\n",
    "    cols_to_hash = [c for c in df.columns if c not in partition_cols]\n",
    "    cols_to_hash = sorted(cols_to_hash)\n",
    "    # exprs = [\n",
    "    #     F.when(F.col(c).isNull(), F.lit(\"NULL\"))\n",
    "    #      .otherwise(F.trim(F.col(c).cast(\"string\")))\n",
    "    #      .alias(c)\n",
    "    #     for c in cols_to_hash\n",
    "    # ]\n",
    "    exprs=[\n",
    "      F.col(c)\n",
    "      for c in cols_to_hash\n",
    "    ]\n",
    "    return df.select(exprs)\n",
    "    \n",
    "def calculate_row_checksum(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with column 'checksum' (sha2 hex string) for each row.\n",
    "    exclude_cols: list of partition columns to exclude from hashing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exclude_cols = exclude_cols or []\n",
    "        cols_to_hash = [c for c in df.columns if c not in exclude_cols]\n",
    "        if not cols_to_hash:\n",
    "            raise ValueError(\"No columns available for checksum after excluding partition columns\")\n",
    "\n",
    "        # concat_expr = F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"NULL\")) for c in cols_to_hash])\n",
    "        concat_expr = F.concat_ws(\"||\", *[F.col(c) for c in cols_to_hash])\n",
    "        return df.withColumn(\"__checksum\", F.sha2(concat_expr, 256)).select(\"__checksum\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating checksums: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# def compare_checksums(src_checksum_df, tgt_checksum_df, partition_dict=None):\n",
    "#     try:\n",
    "#         src_col = [c for c in src_checksum_df.columns if \"checksum\" in c.lower()][0]\n",
    "#         tgt_col = [c for c in tgt_checksum_df.columns if \"checksum\" in c.lower()][0]\n",
    "\n",
    "#         src_count = src_checksum_df.count()\n",
    "#         tgt_count = tgt_checksum_df.count()\n",
    "\n",
    "#         matched_df = src_checksum_df.join(\n",
    "#             tgt_checksum_df, src_checksum_df[src_col] == tgt_checksum_df[tgt_col], \"inner\"\n",
    "#         )\n",
    "#         matched_count = matched_df.count()\n",
    "\n",
    "#         ##status = \"PASS\" if src_count == tgt_count else \"FAIL\"\n",
    "#         status = \"PASS\" if (src_count == tgt_count == matched_count) else \"FAIL\"\n",
    "\n",
    "#         return {\n",
    "#             \"status\": status,\n",
    "#             \"partition\": partition_dict,\n",
    "#             \"source_record_count\": src_count,\n",
    "#             \"target_record_count\": tgt_count,\n",
    "#             \"matched_record_count\": matched_count\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         return {\"status\": \"ERROR\", \"error_message\": str(e), \"partition\": partition_dict}\n",
    "def compare_checksums(src_checksum_df, tgt_checksum_df, partition_dict=None):\n",
    "    try:\n",
    "        src_col = [c for c in src_checksum_df.columns if \"checksum\" in c.lower()][0]\n",
    "        tgt_col = [c for c in tgt_checksum_df.columns if \"checksum\" in c.lower()][0]\n",
    "\n",
    "        total_src_rows = src_checksum_df.count()\n",
    "        total_tgt_rows = tgt_checksum_df.count()\n",
    "\n",
    "        src_distinct = src_checksum_df.select(src_col).distinct().withColumnRenamed(src_col, \"checksum\")\n",
    "        tgt_distinct = tgt_checksum_df.select(tgt_col).distinct().withColumnRenamed(tgt_col, \"checksum\")\n",
    "        \n",
    "        src_distinct_count = src_distinct.count()\n",
    "        tgt_distinct_count = tgt_distinct.count()\n",
    "        \n",
    "        matched = src_distinct.join(tgt_distinct, \"checksum\", \"inner\")\n",
    "        matched_count = matched.count()\n",
    "        \n",
    "        # Simple PASS/FAIL: if all distinct checksums match\n",
    "        status = \"PASS\" if (src_distinct_count == tgt_distinct_count == matched_count) else \"FAIL\"\n",
    "\n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"partition\": partition_dict,\n",
    "            \"source_record_count\": total_src_rows,\n",
    "            \"target_record_count\": total_tgt_rows,\n",
    "            \"matched_record_count\": matched_count\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"ERROR\", \"error_message\": str(e), \"partition\": partition_dict}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8313b23-b7a0-4e64-a8eb-07550fe14378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reconciliation Helper Functions\n",
    "\n",
    "Comprehensive collection of helper functions supporting all reconciliation types. Includes result creation, partition validation, and statistical comparison utilities that work together to provide complete data validation capabilities.\n",
    "\n",
    "**Result Creation:**\n",
    "- `create_recon_result()`: Standardized result factory for all reconciliation types\n",
    "- Generates type-specific summaries and metrics comparison statistics\n",
    "- Handles error logging and JSON formatting\n",
    "\n",
    "**Partition Column Validation:**\n",
    "- `get_s3_partition_columns()`: Extracts partition keys from S3 directory structure\n",
    "- `get_databricks_partition_columns()`: Retrieves partition metadata from Databricks tables\n",
    "- `compare_partition_columns()`: Identifies partition schema differences\n",
    "\n",
    "**Partition Record Count Validation:**\n",
    "- `get_partition_counts_s3()`: Computes per-partition record counts from S3 data\n",
    "- `get_partition_counts_dbx()`: Extracts partition counts from managed tables\n",
    "- `compare_partition_record_counts()`: Validates partition-level data consistency\n",
    "\n",
    "**Supported Reconciliation Types:**\n",
    "- ROW_COUNT, COLUMN_COUNT, SCHEMA_VALIDATION\n",
    "- VERIFIED (statistical comparison)\n",
    "- PARTITION_COLUMNS, PARTITION_RECORD_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc4300a-4d7e-433c-8fbf-b82388afacfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Recon Result Creation Function"
    }
   },
   "outputs": [],
   "source": [
    "# Reconciliation Helper Functions - Complete collection supporting all reconciliation types\n",
    "\n",
    "# =====================================\n",
    "# RESULT CREATION FUNCTIONS\n",
    "# =====================================\n",
    "\n",
    "\n",
    "# Configuration constant - change here to adjust tolerance globally\n",
    "ABS_SUM_TOLERANCE_PCT = 0.001  \n",
    "# 0.0001% tolerance for abs_sum comparisons\n",
    "\n",
    "def compare_numeric_values(src_val, tgt_val, tolerance_pct=ABS_SUM_TOLERANCE_PCT):\n",
    "    \"\"\"\n",
    "    High-performance numeric comparison with configurable tolerance\n",
    "    Uses Python built-in abs() to avoid PySpark column conflict\n",
    "    \n",
    "    Args:\n",
    "        src_val: Source value (string/numeric)\n",
    "        tgt_val: Target value (string/numeric)\n",
    "        tolerance_pct: Percentage tolerance (default 0.10%)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_match: bool, abs_diff: float, diff_pct: float)\n",
    "    \"\"\"\n",
    "    import builtins  # Import Python builtins to access built-in abs()\n",
    "    \n",
    "    # Fast path: exact string match\n",
    "    if src_val == tgt_val:\n",
    "        return True, 0.0, 0.0\n",
    "    \n",
    "    try:\n",
    "        # Convert to string first for consistent handling\n",
    "        src_str = str(src_val).strip()\n",
    "        tgt_str = str(tgt_val).strip()\n",
    "        \n",
    "        # Handle null/empty values\n",
    "        if src_str.lower() in ('null', 'none', ''):\n",
    "            src_str = '0'\n",
    "        if tgt_str.lower() in ('null', 'none', ''):\n",
    "            tgt_str = '0'\n",
    "        \n",
    "        # Convert to float\n",
    "        src_float = float(src_str)\n",
    "        tgt_float = float(tgt_str)\n",
    "        \n",
    "        # Fast path: numeric equality\n",
    "        if src_float == tgt_float:\n",
    "            return True, 0.0, 0.0\n",
    "        \n",
    "        # Calculate absolute difference using Python's built-in abs()\n",
    "        abs_diff = builtins.abs(src_float - tgt_float)\n",
    "        \n",
    "        # Use max absolute value for stable percentage calculation\n",
    "        denominator = builtins.max(builtins.abs(src_float), builtins.abs(tgt_float))\n",
    "        \n",
    "        # Handle edge case: both very close to zero\n",
    "        if denominator < 1e-10:\n",
    "            return True, abs_diff, 0.0\n",
    "        \n",
    "        # Calculate percentage difference\n",
    "        diff_pct = (abs_diff / denominator) * 100.0\n",
    "        \n",
    "        # Check if diff_pct is valid (not inf or nan)\n",
    "        if diff_pct != diff_pct or diff_pct == float('inf'):  # Check for NaN or inf\n",
    "            if abs_diff < 0.01:\n",
    "                return True, abs_diff, 0.0\n",
    "            else:\n",
    "                return False, abs_diff, 100.0\n",
    "        \n",
    "        # Single comparison\n",
    "        return diff_pct <= tolerance_pct, abs_diff, diff_pct\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Debug: print the error\n",
    "        print(f\"Error comparing values: src={src_val}, tgt={tgt_val}, error={str(e)}\")\n",
    "        # Fallback to string comparison\n",
    "        return str(src_val) == str(tgt_val), 0.0, 0.0\n",
    "\n",
    "def create_recon_result(table_name, recon_type, status,execution_id,source_data=None, target_data=None, \n",
    "                       error_msg=None, exec_duration=0.0, missing_src=None, missing_tgt=None,\n",
    "                       mismatched=None, precision_mismatched=None,partitions_used=None,checksum_result=None):\n",
    "    \"\"\"Create a standardized result record for reconciliation outcomes\"\"\"\n",
    "    \n",
    "    import builtins  # Import builtins to access Python's built-in functions\n",
    "    \n",
    "    # execution_id = RUN_ID + \"-\" + JOB_ID\n",
    "    execution_id = execution_id\n",
    "    # print(execution_id)\n",
    "    exec_time = datetime.now()\n",
    "    \n",
    "    source_metrics = json.dumps(source_data) if source_data else None\n",
    "    target_metrics = json.dumps(target_data) if target_data else None\n",
    "    \n",
    "    metrics_compared = 0\n",
    "    metrics_matched = 0\n",
    "    metrics_different = 0\n",
    "    summary = \"\"\n",
    "    error_log = error_msg if error_msg else None\n",
    "    \n",
    "    # Calculate metrics based on reconciliation type\n",
    "    if recon_type == \"ROW_COUNT\":\n",
    "        if source_data is not None and target_data is not None:\n",
    "            # Use Python's built-in abs function explicitly\n",
    "            difference = builtins.abs(int(source_data) - int(target_data))\n",
    "            if difference == 0:\n",
    "                summary = f\"Row count matches: Source={source_data}, Target={target_data}\"\n",
    "            else:\n",
    "                summary = f\"Row count difference: {difference} (Source={source_data}, Target={target_data})\"\n",
    "                status = \"FAIL\"\n",
    "    \n",
    "    elif recon_type == \"COLUMN_COUNT\":\n",
    "        if source_data is not None and target_data is not None:\n",
    "            # Use Python's built-in abs function explicitly\n",
    "            difference = builtins.abs(int(source_data) - int(target_data))\n",
    "            if difference == 0:\n",
    "                summary = f\"Column count matches: Source={source_data}, Target={target_data}\"\n",
    "            else:\n",
    "                summary = f\"Column count difference: {difference} (Source={source_data}, Target={target_data})\"\n",
    "                status = \"FAIL\"\n",
    "    \n",
    "    elif recon_type == \"SCHEMA_VALIDATION\":\n",
    "        issues = []\n",
    "        \n",
    "        if missing_src:\n",
    "            issues.append(f\"Missing in source: {len(missing_src)} columns {missing_src[:3]}{'...' if len(missing_src) > 3 else ''}\")\n",
    "        \n",
    "        if missing_tgt:\n",
    "            issues.append(f\"Missing in target: {len(missing_tgt)} columns {missing_tgt[:3]}{'...' if len(missing_tgt) > 3 else ''}\")\n",
    "        \n",
    "        if mismatched:\n",
    "            issues.append(f\"Type mismatches: {len(mismatched)} columns {mismatched[:2]}{'...' if len(mismatched) > 2 else ''}\")\n",
    "        \n",
    "        if precision_mismatched:\n",
    "            issues.append(f\"Precision mismatches: {len(precision_mismatched)} columns {precision_mismatched[:2]}{'...' if len(precision_mismatched) > 2 else ''}\")\n",
    "        \n",
    "        if issues:\n",
    "            summary = f\"Schema differences found: {' | '.join(issues)}\"\n",
    "            status = \"FAIL\"\n",
    "        else:\n",
    "            summary = \"Schema matches completely: All columns, types, and precision/scale are identical\"\n",
    "    \n",
    "    elif recon_type == \"Partiton_Data_count\":\n",
    "        if status == \"PASS\":\n",
    "            error_msg = ''\n",
    "            summary = (\n",
    "                f\"S3 distinct partition count = {source_data}, \"\n",
    "                f\"Managed table distinct partition count = {target_data}. \"\n",
    "                \"All partitions match.\"\n",
    "            )\n",
    "        else:\n",
    "            status = \"FAIL\"\n",
    "            summary = error_msg\n",
    "            error_msg = ''\n",
    "            error_log = None\n",
    "        \n",
    "    elif recon_type == \"CHECKSUM_VALIDATION\":\n",
    "        if checksum_result is not None:\n",
    "            try:\n",
    "                summary = json.dumps({\n",
    "                    \"partition\": checksum_result.get(\"partition\", {}),\n",
    "                    \"source_partition_record_count\": checksum_result.get(\"source_record_count\", 0),\n",
    "                    \"target_partition_record_count\": checksum_result.get(\"target_record_count\", 0)\n",
    "                })\n",
    "                #source_metrics = None\n",
    "                #target_metrics = None\n",
    "                if checksum_result.get(\"status\") != \"PASS\":\n",
    "                    status = \"FAIL\"\n",
    "            except Exception:\n",
    "                summary = \"Error processing checksum validation\"\n",
    "                status = \"ERROR\"\n",
    "        else:\n",
    "            summary = \"Checksum validation: No checksum_result provided\"\n",
    "            status = \"ERROR\"    \n",
    "\n",
    "    # VERIFIED section with partition info in summary and tolerance for abs_sum\n",
    "    elif recon_type == \"VERIFIED\" and source_data and target_data and isinstance(source_data, dict) and isinstance(target_data, dict):\n",
    "        # Extract metadata\n",
    "        if partitions_used is None:\n",
    "            partitions_used = []\n",
    "        columns_verified = source_data.get('columns_verified', [])\n",
    "        src_stats = source_data.get('statistics', {})\n",
    "        tgt_stats = target_data.get('statistics', {})\n",
    "        \n",
    "        # Build partition summary efficiently\n",
    "        partition_summary = \"\"\n",
    "        if partitions_used:\n",
    "            partition_summary = f\"Partitions [{len(partitions_used)}]: \" + '; '.join(\n",
    "                f\"({', '.join(f'{k}={v}' for k, v in part.items())})\" \n",
    "                for part in partitions_used\n",
    "            )\n",
    "        \n",
    "        # Get common columns using set intersection (faster than loops)\n",
    "        common_cols = (set(src_stats.keys()) - {'_metadata'}) & set(tgt_stats.keys())\n",
    "        \n",
    "        # Initialize result lists\n",
    "        matched_stats = []\n",
    "        tolerance_matched_stats = []\n",
    "        unmatched_stats = []\n",
    "        \n",
    "        # Single-pass comparison loop\n",
    "        for col_name in common_cols:\n",
    "            src_col_stats = src_stats.get(col_name)\n",
    "            tgt_col_stats = tgt_stats.get(col_name)\n",
    "            \n",
    "            # Skip if not both dictionaries\n",
    "            if not (isinstance(src_col_stats, dict) and isinstance(tgt_col_stats, dict)):\n",
    "                continue\n",
    "            \n",
    "            # Get common stat types\n",
    "            common_stat_types = set(src_col_stats.keys()) & set(tgt_col_stats.keys())\n",
    "            \n",
    "            for stat_name in common_stat_types:\n",
    "                metrics_compared += 1\n",
    "                src_stat = str(src_col_stats[stat_name])\n",
    "                tgt_stat = str(tgt_col_stats[stat_name])\n",
    "                \n",
    "                # Apply tolerance for abs_sum, exact match for others\n",
    "                if stat_name == 'abs_sum':\n",
    "                    # Use tolerance comparison (0.10%)\n",
    "                    is_match, abs_diff, diff_pct = compare_numeric_values(src_stat, tgt_stat)\n",
    "                    \n",
    "                    if is_match:\n",
    "                        metrics_matched += 1\n",
    "                        if abs_diff == 0.0:\n",
    "                            # Exact match\n",
    "                            matched_stats.append(f\"{col_name}.{stat_name}={src_stat}\")\n",
    "                        else:\n",
    "                            # Matched within tolerance\n",
    "                            tolerance_matched_stats.append(\n",
    "                                f\"{col_name}.{stat_name}{src_stat}({diff_pct:.6f}%)\"\n",
    "                            )\n",
    "                    else:\n",
    "                        # Outside tolerance\n",
    "                        metrics_different += 1\n",
    "                        unmatched_stats.append(\n",
    "                            f\"{col_name}.{stat_name}(src={src_stat},tgt={tgt_stat},{diff_pct:.6f}%)\"\n",
    "                        )\n",
    "                else:\n",
    "                    # Exact string comparison for min/max (fastest)\n",
    "                    if src_stat == tgt_stat:\n",
    "                        metrics_matched += 1\n",
    "                        matched_stats.append(f\"{col_name}.{stat_name}={src_stat}\")\n",
    "                    else:\n",
    "                        metrics_different += 1\n",
    "                        unmatched_stats.append(\n",
    "                            f\"{col_name}.{stat_name}(src={src_stat},tgt={tgt_stat})\"\n",
    "                        )\n",
    "        \n",
    "        # Build summary efficiently using list join\n",
    "        if metrics_compared > 0:\n",
    "            if metrics_different > 0:\n",
    "                status = \"FAIL\"\n",
    "            \n",
    "            match_rate = (metrics_matched / metrics_compared) * 100.0\n",
    "            \n",
    "            # Build summary parts\n",
    "            summary_parts = [\n",
    "                f\"Stats: {metrics_matched}/{metrics_compared} match ({match_rate:.1f}%)\",\n",
    "                f\"Columns [{len(columns_verified)}]: {', '.join(columns_verified)}\",\n",
    "                partition_summary\n",
    "            ]\n",
    "            \n",
    "            if matched_stats:\n",
    "                summary_parts.append(f\"Matched [{len(matched_stats)}]: {', '.join(matched_stats)}\")\n",
    "            \n",
    "            if tolerance_matched_stats:\n",
    "                summary_parts.append(\n",
    "                    f\"Tolerance-Matched [{len(tolerance_matched_stats)}]: {', '.join(tolerance_matched_stats)}\"\n",
    "                )\n",
    "            \n",
    "            if unmatched_stats:\n",
    "                summary_parts.append(f\"Unmatched [{len(unmatched_stats)}]: {', '.join(unmatched_stats)}\")\n",
    "            \n",
    "            # Join all parts efficiently\n",
    "            summary = \" | \".join(summary_parts)\n",
    "        else:\n",
    "            summary = f\"No common statistics to compare | {partition_summary}\"\n",
    "            status = \"WARNING\" if status == \"PASS\" else status\n",
    "    \n",
    "    \n",
    "    # Handle error cases - CHANGED: FAIL to ERROR\n",
    "    if error_msg and not summary:\n",
    "        summary = f\"{recon_type}: ERROR - {error_msg}\"\n",
    "        status = \"ERROR\"  # Changed from \"FAIL\" to \"ERROR\"\n",
    "    \n",
    "    # Default summary if not set\n",
    "    if not summary:\n",
    "        summary = f\"{recon_type}: {status}\"\n",
    "    \n",
    "    return {\n",
    "        \"execution_id\": execution_id,\n",
    "        \"table_name\": table_name,\n",
    "        \"recon_type\": recon_type,\n",
    "        \"exec_timestamp\": exec_time,\n",
    "        \"exec_duration\": exec_duration,\n",
    "        \"status\": status,\n",
    "        \"source_metrics\": source_metrics,\n",
    "        \"target_metrics\": target_metrics,\n",
    "        \"metrics_compared\": metrics_compared,\n",
    "        \"metrics_matched\": metrics_matched,\n",
    "        \"metrics_different\": metrics_different,\n",
    "        \"summary\": summary,\n",
    "        \"error_log\": error_log\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# PARTITION COLUMN VALIDATION FUNCTIONS\n",
    "# =====================================\n",
    "\n",
    "def get_s3_partition_columns(bucket, prefix):\n",
    "    \"\"\"Extract partition columns from S3 path structure\"\"\"\n",
    "    try:\n",
    "        # First try to get from the candidates table\n",
    "        candidates_query = f\"\"\"\n",
    "        SELECT partition_key \n",
    "        FROM {CANDIDATES_TABLE_FQN}\n",
    "        WHERE s3_bucket_name = '{bucket}' \n",
    "        AND bucket_prefix = '{prefix}'\n",
    "        \"\"\" \n",
    "        # --code_change\n",
    "        # candidates_query = f\"\"\"\n",
    "        # SELECT partition_key \n",
    "        # FROM {CANDIDATES_TABLE_FQN}\n",
    "        # WHERE s3_bucket_name = '{bucket}' \n",
    "        # AND table_name = '{table_name}'\n",
    "        # \"\"\"\n",
    "        partition_cols = []\n",
    "        candidates_df = spark.sql(candidates_query)\n",
    "        if candidates_df.count() > 0:\n",
    "            partition_key = candidates_df.first()[\"partition_key\"]\n",
    "            if partition_key and partition_key.strip():\n",
    "                # Split by comma and clean up whitespace\n",
    "                partition_cols = [col.strip() for col in partition_key.split(',')]\n",
    "                return sorted(partition_cols)\n",
    "        \n",
    "        # Fallback: Read a small sample to infer partition structure\n",
    "        # path = f\"s3://{bucket}/{prefix}\" ##code_change\n",
    "        # path = prefix\n",
    "        \n",
    "       # List files to understand partition structure\n",
    "        # files = dbutils.fs.ls(path)\n",
    "        # partition_cols = []\n",
    "        \n",
    "        # ##Look for partition patterns like key=value in directory structure\n",
    "        # for file_info in files:\n",
    "        #     if file_info.isDir():\n",
    "        #         dir_name = file_info.name.rstrip('/')\n",
    "        #         if '=' in dir_name:\n",
    "        #             partition_key = dir_name.split('=')[0]\n",
    "        #             if partition_key not in partition_cols:\n",
    "        #                 partition_cols.append(partition_key)\n",
    "        #             # Iterate one more directory level\n",
    "        #             sub_files = dbutils.fs.ls(file_info.path)\n",
    "        #             for sub_file_info in sub_files:\n",
    "        #                 if sub_file_info.isDir():\n",
    "        #                     sub_dir_name = sub_file_info.name.rstrip('/')\n",
    "        #                     if '=' in sub_dir_name:\n",
    "        #                         sub_partition_key = sub_dir_name.split('=')[0]\n",
    "        #                         if sub_partition_key not in partition_cols:\n",
    "        #                             partition_cols.append(sub_partition_key)\n",
    "        # return sorted(partition_cols)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting S3 partition columns for {bucket}/{prefix}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_databricks_partition_columns(table_name):\n",
    "    \"\"\"Get partition columns from Databricks table\"\"\"\n",
    "    try:\n",
    "        \n",
    "        describe_df = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "        partition_cols_row = describe_df.select(\"partitionColumns\").first()\n",
    "        \n",
    "        if partition_cols_row and partition_cols_row[\"partitionColumns\"]:\n",
    "            # Parse the partition columns (they come as array)\n",
    "            partition_cols = partition_cols_row[\"partitionColumns\"]\n",
    "            return sorted(partition_cols) if partition_cols else []\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Databricks partition columns for {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "def compare_partition_columns(src_partitions, tgt_partitions):\n",
    "    \"\"\"Compare partition columns between source and target\"\"\"\n",
    "    missing_in_src = [col for col in tgt_partitions if col not in src_partitions]\n",
    "    missing_in_tgt = [col for col in src_partitions if col not in tgt_partitions]\n",
    "    \n",
    "    return missing_in_src, missing_in_tgt\n",
    "\n",
    "# =====================================\n",
    "# PARTITION RECORD COUNT VALIDATION FUNCTIONS\n",
    "# =====================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def get_partition_counts_s3(s3_path: str, partition_cols: list) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dictionary of partition values -> record counts from S3 data.\n",
    "    \n",
    "    Parameters:\n",
    "        s3_path (str): Base S3 path to the dataset.\n",
    "        partition_cols (list): List of partition column names (can be one or more).\n",
    "    \n",
    "    Returns:\n",
    "        dict: {(partition_value1, partition_value2, ...): count, ...}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the data from S3\n",
    "        df = spark.read.parquet(s3_path)\n",
    "\n",
    "        # Compute counts per partition\n",
    "        counts_df = df.groupBy(*partition_cols).agg(F.count(\"*\").alias(\"cnt\"))\n",
    "        counts_df = counts_df.sort(*partition_cols)\n",
    "\n",
    "        # Collect results and create dictionary with tuple keys\n",
    "        partition_counts = {}\n",
    "        for row in counts_df.collect():\n",
    "            key = tuple(row[c] for c in partition_cols)\n",
    "            partition_counts[key] = row[\"cnt\"]\n",
    "\n",
    "        return partition_counts\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting S3 partition counts: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def get_partition_counts_dbx(target_table: str, partition_cols: list) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dictionary of partition values -> record counts from a Databricks managed table.\n",
    "    \n",
    "    Parameters:\n",
    "        target_table (str): Name of the Databricks table.\n",
    "        partition_cols (list): List of partition column names (can be one or more).\n",
    "    \n",
    "    Returns:\n",
    "        dict: {(partition_value1, partition_value2, ...): count, ...}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read table\n",
    "        df = spark.read.table(target_table)\n",
    "\n",
    "        # Compute counts per partition\n",
    "        counts_df = df.groupBy(*partition_cols).agg(F.count(\"*\").alias(\"cnt\"))\n",
    "        counts_df = counts_df.sort(*partition_cols)\n",
    "\n",
    "        # Collect results into a dictionary\n",
    "        partition_counts = {}\n",
    "        for row in counts_df.collect():\n",
    "            key = tuple(row[c] for c in partition_cols)\n",
    "            partition_counts[key] = row[\"cnt\"]\n",
    "\n",
    "        return partition_counts\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Databricks partition counts: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def compare_partition_record_counts(s3_counts: dict, dbx_counts: dict, partition_cols: list):\n",
    "    \"\"\"\n",
    "    Compare partition record counts between S3 and Databricks table.\n",
    "    \n",
    "    Parameters:\n",
    "        s3_counts (dict): S3 partition counts\n",
    "        dbx_counts (dict): Databricks partition counts  \n",
    "        partition_cols (list): List of partition column names\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (status, summary, mismatched_partitions)\n",
    "    \"\"\"\n",
    "    mismatched_partitions = []\n",
    "    \n",
    "    # Get all partition keys from both sources\n",
    "    all_partitions = set(s3_counts.keys()) | set(dbx_counts.keys())\n",
    "    \n",
    "    # Check each partition\n",
    "    for partition_key in sorted(all_partitions):\n",
    "        s3_count = s3_counts.get(partition_key, 0)\n",
    "        dbx_count = dbx_counts.get(partition_key, 0)\n",
    "        \n",
    "        if s3_count != dbx_count:\n",
    "            # Format partition key for display\n",
    "            if len(partition_cols) == 1:\n",
    "                part_display = f\"{partition_cols[0]}={partition_key[0]}\"\n",
    "            else:\n",
    "                part_display = \", \".join([f\"{col}={val}\" for col, val in zip(partition_cols, partition_key)])\n",
    "            \n",
    "            mismatched_partitions.append({\n",
    "                'partition': part_display,\n",
    "                'source_count': s3_count,\n",
    "                'target_count': dbx_count,\n",
    "                'difference': abs(s3_count - dbx_count)\n",
    "            })\n",
    "    \n",
    "    # Generate status and summary\n",
    "    if not mismatched_partitions:\n",
    "        status = \"PASS\"\n",
    "        summary = f\"All {len(all_partitions)} partitions have matching record counts\"\n",
    "    else:\n",
    "        status = \"FAIL\"\n",
    "        total_partitions = len(all_partitions)\n",
    "        mismatched_count = len(mismatched_partitions)\n",
    "        matched_count = total_partitions - mismatched_count\n",
    "        \n",
    "        # Show first few mismatched partitions in summary\n",
    "        mismatch_details = []\n",
    "        for mismatch in mismatched_partitions[:3]:  # Show first 3\n",
    "            mismatch_details.append(\n",
    "                f\"{mismatch['partition']}: Source={mismatch['source_count']}, Target={mismatch['target_count']}\"\n",
    "            )\n",
    "        \n",
    "        remaining = len(mismatched_partitions) - 3\n",
    "        if remaining > 0:\n",
    "            mismatch_details.append(f\"and {remaining} more partitions\")\n",
    "        \n",
    "        summary = f\"Partition record count differences: {matched_count}/{total_partitions} partitions match - Mismatched: [{'; '.join(mismatch_details)}]\"\n",
    "    \n",
    "    return status, summary, mismatched_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434337f9-3f88-4576-9660-5b7db1d445bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Recon Result creation Functions for verified"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def select_columns_for_verification(df: DataFrame, partition_cols: list = None, max_columns: int = 8) -> tuple:\n",
    "    \"\"\"\n",
    "    Select up to 8 columns for verification: 2 int, 2 decimal, 2 string, 2 date\n",
    "    Excludes partition columns dynamically\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        partition_cols: List of partition column names to exclude (if None, will try to detect)\n",
    "        max_columns: Maximum columns to select (default 8)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of selected column names, dict with column types)\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import IntegerType, LongType, DecimalType, DoubleType, FloatType, StringType, DateType, TimestampType\n",
    "    \n",
    "    schema = df.schema\n",
    "    \n",
    "    # If partition columns not provided, use empty list\n",
    "    if partition_cols is None:\n",
    "        partition_cols = []\n",
    "    \n",
    "    # Convert partition columns to lowercase for case-insensitive comparison\n",
    "    partition_cols_lower = [pc.lower() for pc in partition_cols]\n",
    "    \n",
    "    # Categorize columns by type (exclude partition columns)\n",
    "    int_cols = []\n",
    "    decimal_cols = []\n",
    "    string_cols = []\n",
    "    date_cols = []\n",
    "    \n",
    "    for field in schema.fields:\n",
    "        col_name = field.name\n",
    "        \n",
    "        # Skip partition columns (case-insensitive)\n",
    "        if col_name.lower() in partition_cols_lower:\n",
    "            continue\n",
    "            \n",
    "        col_type = field.dataType\n",
    "        \n",
    "        if isinstance(col_type, (IntegerType, LongType)):\n",
    "            int_cols.append(col_name)\n",
    "        elif isinstance(col_type, (DecimalType, DoubleType, FloatType)):\n",
    "            decimal_cols.append(col_name)\n",
    "        elif isinstance(col_type, StringType):\n",
    "            string_cols.append(col_name)\n",
    "        elif isinstance(col_type, (DateType, TimestampType)):\n",
    "            date_cols.append(col_name)\n",
    "    \n",
    "    # Select 2 from each category\n",
    "    selected = {\n",
    "        'int': int_cols[:2],\n",
    "        'decimal': decimal_cols[:2],\n",
    "        'string': string_cols[:2],\n",
    "        'date': date_cols[:2]\n",
    "    }\n",
    "    \n",
    "    # Flatten to list\n",
    "    all_selected = []\n",
    "    for cols in selected.values():\n",
    "        all_selected.extend(cols)\n",
    "    \n",
    "    # print(f\"Column selection: Found {len(int_cols)} int, {len(decimal_cols)} decimal, {len(string_cols)} string, {len(date_cols)} date columns\")\n",
    "    # print(f\"Excluded {len(partition_cols)} partition columns: {partition_cols}\")\n",
    "    \n",
    "    return all_selected[:max_columns], selected\n",
    "\n",
    "def get_latest_partitions(df: DataFrame, partition_cols: list = [\"snapshot_date\", \"edp_run_id\"], n: int = 5) -> tuple:\n",
    "    \"\"\"\n",
    "    Filter DataFrame to include only the latest N partitions\n",
    "    Uses partition pruning for optimal performance\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filtered_df, list of partition dictionaries used)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get distinct partition values with minimal data movement\n",
    "        latest_partitions = df.select(*partition_cols) \\\n",
    "            .distinct() \\\n",
    "            .orderBy(*[col(c).desc() for c in partition_cols]) \\\n",
    "            .limit(n) \\\n",
    "            .collect()  # Small result set, safe to collect\n",
    "        \n",
    "        if not latest_partitions:\n",
    "            print(\"Warning: No partitions found\")\n",
    "            return df, []\n",
    "        \n",
    "        # NEW: Store partition values for summary\n",
    "        partitions_used = []\n",
    "        for partition_row in latest_partitions:\n",
    "            partition_dict = {}\n",
    "            for pc in partition_cols:\n",
    "                value = partition_row[pc]\n",
    "                if hasattr(value,'isoformat'):\n",
    "                    partition_dict[pc] = value.isoformat()\n",
    "                else:\n",
    "                    partition_dict[pc] = str(value)\n",
    "            partitions_used.append(partition_dict)\n",
    "        \n",
    "        # Build filter condition using OR logic without functools\n",
    "        filter_condition = None\n",
    "        \n",
    "        for partition_row in latest_partitions:\n",
    "            # Build AND condition for this partition\n",
    "            partition_condition = None\n",
    "            \n",
    "            for pc in partition_cols:\n",
    "                current_condition = (col(pc) == partition_row[pc])\n",
    "                \n",
    "                if partition_condition is None:\n",
    "                    partition_condition = current_condition\n",
    "                else:\n",
    "                    partition_condition = partition_condition & current_condition\n",
    "            \n",
    "            # Add to overall OR condition\n",
    "            if filter_condition is None:\n",
    "                filter_condition = partition_condition\n",
    "            else:\n",
    "                filter_condition = filter_condition | partition_condition\n",
    "        \n",
    "        # Apply filter - this will leverage partition pruning in Parquet/Delta\n",
    "        filtered_df = df.filter(filter_condition)\n",
    "        \n",
    "        partition_count = filtered_df.select(*partition_cols).distinct().count()\n",
    "        # print(f\"Filtered to {partition_count} partitions from latest {n}\")\n",
    "        \n",
    "        return filtered_df, partitions_used  # **RETURN BOTH**\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error filtering latest partitions: {e}\")\n",
    "        return df, []\n",
    "    \n",
    "def prepare_dataframe_for_stats(df: DataFrame, partition_cols: list, selected_columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare DataFrame with only needed columns and cache\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Select only partition columns + selected columns for verification\n",
    "        columns_to_select = list(set(partition_cols + selected_columns))\n",
    "        \n",
    "        # Project early to reduce data size\n",
    "        df_projected = df.select(*columns_to_select)\n",
    "        \n",
    "        # Cache the filtered, projected DataFrame\n",
    "        df_projected.cache()\n",
    "        \n",
    "        # Force materialization with a count\n",
    "        record_count = df_projected.count()\n",
    "        # print(f\"Cached {record_count:,} records with {len(columns_to_select)} columns for statistics computation\")\n",
    "        \n",
    "        return df_projected\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing DataFrame: {e}\")\n",
    "        return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f363af3-ecb2-4bf6-bfd6-ba9fdfd6c5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Execute Reconciliation for Table\n",
    "\n",
    "Main orchestration function that executes all reconciliation types for a single table mapping. Performs comprehensive validation including row counts, column counts, schema validation, statistical verification, and partition analysis.\n",
    "\n",
    "**Reconciliation Types Executed:**\n",
    "1. **ROW_COUNT**: Compares total record counts\n",
    "2. **COLUMN_COUNT**: Validates column count consistency  \n",
    "3. **SCHEMA_VALIDATION**: Checks data types, precision, and scale\n",
    "4. **VERIFIED**: Comprehensive statistical comparison\n",
    "5. **PARTITION_COLUMNS**: Validates partition schema consistency\n",
    "6. **PARTITION_RECORD_COUNT**: Ensures partition-level data completeness\n",
    "\n",
    "**Error Handling:**\n",
    "- Graceful degradation on individual check failures\n",
    "- Comprehensive error logging and status tracking\n",
    "- Continues execution even if individual validations fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a4d64b-d24e-44fc-a5ae-ee889f021617",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute Reconciliation for Table"
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Execute Reconciliation for Table\n",
    "# Execute all reconciliation types for a single table (Row Count, Column Count, Schema Validation, VERIFIED, Partition Columns)\n",
    "def execute_reconciliation_for_table(row):\n",
    "    \"\"\"Execute comprehensive reconciliation for a single table mapping\"\"\"\n",
    "    src_bucket = row.source_s3_bucket\n",
    "    src_prefix = row.source_bucket_prefix\n",
    "    tgt_table = row.target_table_name\n",
    "    tgt_catalog_name=row.target_catalog_name\n",
    "    tgt_schema_name=row.target_schema_name\n",
    "    execution_id=row.execution_id\n",
    "    print(f\"In execute_reconciliation_for_table method: {execution_id}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if not src_bucket or not src_prefix:\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"source_read_error\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "            error_msg=\"Missing S3 bucket or prefix in mapping\",execution_id=execution_id\n",
    "        ))\n",
    "        return results\n",
    "    \n",
    "    # Initialize variables to store counts, schemas, and dataframes\n",
    "    src_row_cnt = None\n",
    "    tgt_row_cnt = None\n",
    "    src_col_cnt = None\n",
    "    tgt_col_cnt = None\n",
    "    src_schema = None\n",
    "    tgt_schema = None\n",
    "    src_df = None\n",
    "    tgt_df = None\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        src_df = read_source_parquet(src_bucket, src_prefix,tgt_table)\n",
    "        tgt_df = spark.table(f\"{tgt_catalog_name}.{tgt_schema_name}.{tgt_table}\")\n",
    "        \n",
    "        # Filter managed table for snapshot_date >= 2020\n",
    "        if \"snapshot_date\" in tgt_df.columns:\n",
    "            tgt_df = tgt_df.filter(to_date(col(\"snapshot_date\"), 'yyyy-MM-dd') >= lit('2020-01-01'))\n",
    "        \n",
    "        if src_df is None:\n",
    "            results.append(create_recon_result(\n",
    "            tgt_table, \"data_read_error\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "            error_msg=\"Source data is not present\",execution_id=execution_id\n",
    "            ))\n",
    "            return results\n",
    "        else:\n",
    "            \n",
    "            src_row_cnt = src_df.count()\n",
    "            print(src_row_cnt)\n",
    "            tgt_row_cnt = tgt_df.count()\n",
    "            print(tgt_row_cnt)\n",
    "            src_col_cnt = len(src_df.columns)\n",
    "            tgt_col_cnt = len(tgt_df.columns)\n",
    "            src_schema = get_schema_dict(src_df)\n",
    "            tgt_schema = get_schema_dict(tgt_df)\n",
    "\n",
    "            # --- Prepare distinct partition-level DataFrames ---\n",
    "            partition_cols=[\"snapshot_date\", \"edp_run_id\"]\n",
    "            s3_partitions_df = (\n",
    "                src_df.select(*partition_cols)\n",
    "                    .distinct()\n",
    "                    .withColumn(\"source\", F.lit(\"S3\"))\n",
    "            )\n",
    "\n",
    "            managed_partitions_df = (\n",
    "                tgt_df.select(*partition_cols)\n",
    "                        .distinct()\n",
    "                        .withColumn(\"source\", F.lit(\"MANAGED\"))\n",
    "            )\n",
    "\n",
    "            # --- Distinct partition counts ---\n",
    "            s3_distinct_count = s3_partitions_df.count()\n",
    "            managed_distinct_count = managed_partitions_df.count()\n",
    "            # --- Identify mismatched partitions ---\n",
    "            missing_in_managed = (\n",
    "                s3_partitions_df.select(*partition_cols)\n",
    "                .subtract(managed_partitions_df.select(*partition_cols))\n",
    "            )\n",
    "            missing_in_s3 = (\n",
    "                managed_partitions_df.select(*partition_cols)\n",
    "                .subtract(s3_partitions_df.select(*partition_cols))\n",
    "            )\n",
    "\n",
    "            \n",
    "            data_load_duration = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to read source or target data: {str(e)}\"\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"data_read_error\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "            error_msg=error_message,execution_id=execution_id\n",
    "        ))\n",
    "        return results\n",
    "    \n",
    "    # Row Count Check - only proceed if we have valid counts\n",
    "    if src_row_cnt is not None and tgt_row_cnt is not None:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            row_status = \"PASS\" if src_row_cnt == tgt_row_cnt else \"FAIL\"\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"ROW_COUNT\", row_status,\n",
    "                source_data=src_row_cnt,\n",
    "                target_data=tgt_row_cnt,\n",
    "                exec_duration=duration,execution_id=execution_id\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"ROW_COUNT\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "                source_data=src_row_cnt,\n",
    "                target_data=tgt_row_cnt,\n",
    "                error_msg=f\"Row count comparison failed: {str(e)}\",\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "    else:\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"ROW_COUNT\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "            error_msg=\"Could not obtain row counts from source or target\",execution_id=execution_id\n",
    "        ))\n",
    "    \n",
    "    # Column Count Check - only proceed if we have valid counts\n",
    "    if src_col_cnt is not None and tgt_col_cnt is not None:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            col_status = \"PASS\" if src_col_cnt == tgt_col_cnt else \"FAIL\"\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"COLUMN_COUNT\", col_status,\n",
    "                source_data=src_col_cnt,\n",
    "                target_data=tgt_col_cnt,\n",
    "                exec_duration=duration,\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"COLUMN_COUNT\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "                source_data=src_col_cnt,\n",
    "                target_data=tgt_col_cnt,\n",
    "                error_msg=f\"Column count comparison failed: {str(e)}\",\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "    else:\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"COLUMN_COUNT\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "            error_msg=\"Could not obtain column counts from source or target\",\n",
    "            execution_id=execution_id\n",
    "        ))\n",
    "    \n",
    "    # Schema Validation (including precision validation) - only proceed if we have valid schemas\n",
    "    if src_schema is not None and tgt_schema is not None:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            miss_src, miss_tgt, mismatched, precision_mismatched = compare_schemas(src_schema, tgt_schema)\n",
    "            schema_status = \"PASS\" if not miss_src and not miss_tgt and not mismatched and not precision_mismatched else \"FAIL\"\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            results.append(create_recon_result(\n",
    "                tgt_table,\"SCHEMA_VALIDATION\", schema_status,\n",
    "                source_data=src_schema,\n",
    "                target_data=tgt_schema,\n",
    "                missing_src=miss_src,\n",
    "                missing_tgt=miss_tgt,\n",
    "                mismatched=mismatched,\n",
    "                precision_mismatched=precision_mismatched,\n",
    "                exec_duration=duration,\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"SCHEMA_VALIDATION\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "                error_msg=f\"Schema validation failed: {str(e)}\",\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "    \n",
    "    else:\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"SCHEMA_VALIDATION\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "            error_msg=\"Could not obtain schema information from source or target\",\n",
    "            execution_id=execution_id\n",
    "        ))\n",
    "    #Partiton Data count\n",
    "     # --- Status and messages ---\n",
    "    if s3_distinct_count == managed_distinct_count and missing_in_managed.count() == 0 and missing_in_s3.count() == 0:\n",
    "        try:\n",
    "            source_data = s3_distinct_count\n",
    "            target_data = managed_distinct_count\n",
    "    \n",
    "            start_time = datetime.now()\n",
    "            status = \"PASS\"\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # source_metrics = f\"Distinct partitions = {s3_distinct_count}\"\n",
    "            # target_metrics = f\"Distinct partitions = {managed_distinct_count}\"\n",
    "      \n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"Partiton_Data_count\", status,source_data=source_data,target_data=target_data,\n",
    "                exec_duration=duration,\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"Partiton_Data_count\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "                error_msg=f\"Partiton_Data_count validation failed: {str(e)}\",\n",
    "                execution_id=execution_id\n",
    "            )) \n",
    "        \n",
    "    else:\n",
    "        status = \"FAIL\"\n",
    "        s3_sample = [tuple(r[c] for c in partition_cols) for r in s3_partitions_df.limit(3).collect()]\n",
    "        managed_sample = [tuple(r[c] for c in partition_cols) for r in managed_partitions_df.limit(3).collect()]\n",
    "\n",
    "        # source_metrics = f\"Distinct partitions = {s3_distinct_count}, Sample = {s3_sample}\"\n",
    "        # target_metrics = f\"Distinct partitions = {managed_distinct_count}, Sample = {managed_sample}\"\n",
    "        source_metrics = s3_distinct_count\n",
    "        target_metrics = managed_distinct_count\n",
    "        error_msg = \"\"\n",
    "        if missing_in_managed.count() > 0:\n",
    "            missing_vals = missing_in_managed.collect()\n",
    "            error_msg += f\"Missing in Managed: {missing_vals}. \"\n",
    "        if missing_in_s3.count() > 0:\n",
    "            missing_vals = missing_in_s3.collect()\n",
    "            error_msg += f\"Missing in S3: {missing_vals}. \"\n",
    "        summary = (\n",
    "            f\"S3 distinct partition count = {s3_distinct_count}, \"\n",
    "            f\"Managed distinct partition count = {managed_distinct_count}. \"\n",
    "            f\"Partition mismatch detected.\"\n",
    "            f\"Error_Msg : {error_msg}.\" \n",
    "        )\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"Partiton_Data_count\", status,execution_id,source_metrics,target_metrics,\n",
    "            error_msg = summary,\n",
    "            mismatched=missing_vals,\n",
    "            exec_duration=duration\n",
    "        ))\n",
    "    if src_df is not None and tgt_df is not None:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            # Fully qualified table name for managed table\n",
    "            # fq_table = f\"{tgt_table}\"\n",
    "            # print(f\" table name for checksum : {fq_table}\")\n",
    "\n",
    "            \n",
    "            partition_cols=[\"snapshot_date\", \"edp_run_id\"]\n",
    "\n",
    "            if not partition_cols:\n",
    "                results.append(create_recon_result(\n",
    "                    tgt_table,\n",
    "                    \"CHECKSUM_VALIDATION\",\n",
    "                    \"SKIP\",\n",
    "                    error_msg=\"Table not partitioned or no partition columns found\",\n",
    "                    execution_id=execution_id\n",
    "                ))\n",
    "            else:\n",
    "                # Get latest partition(s) from target dataframe using existing helper\n",
    "                src_df_latest, src_partitions_latest = get_latest_partitions(\n",
    "                    src_df, \n",
    "                    partition_cols=partition_cols,\n",
    "                    n=1\n",
    "                )\n",
    "                tgt_df_latest, tgt_partitions_latest = get_latest_partitions(\n",
    "                    tgt_df, \n",
    "                    partition_cols=partition_cols,\n",
    "                    n=1\n",
    "                )\n",
    "                \n",
    "                # **Use source partitions for summary (should match target)**\n",
    "               # partitions_used = src_partitions_used\n",
    "\n",
    "                if not src_partitions_latest:\n",
    "                    results.append(create_recon_result(\n",
    "                        tgt_table,\n",
    "                        \"CHECKSUM_VALIDATION\",\n",
    "                        \"SKIP\",\n",
    "                        error_msg=\"No latest partition found\",\n",
    "                        execution_id=execution_id\n",
    "                    ))\n",
    "                else:\n",
    "                    # Extract latest partition details\n",
    "                    #partitions_used = src_partitions_used\n",
    "\n",
    "                    # Filter both source and target DataFrames for that partition\n",
    "                    # src_partition_df = src_df\n",
    "                    # tgt_partition_df = tgt_df\n",
    "                    # for pcol, pval in latest_partition_dict.items():\n",
    "                    #     src_partition_df = src_partition_df.filter(F.col(pcol) == F.lit(pval))\n",
    "                    #     tgt_partition_df = tgt_partition_df.filter(F.col(pcol) == F.lit(pval))\n",
    "\n",
    "                    # Compute checksums (exclude partition columns)\n",
    "                    src_df_std = prepare_for_checksum(src_df_latest, partition_cols=[\"snapshot_date\", \"edp_run_id\"])\n",
    "                    tgt_df_std = prepare_for_checksum(tgt_df_latest, partition_cols=[\"snapshot_date\", \"edp_run_id\"])\n",
    "\n",
    "                    src_checksum_df = calculate_row_checksum(src_df_std, exclude_cols=[])\n",
    "                    tgt_checksum_df = calculate_row_checksum(tgt_df_std, exclude_cols=[])\n",
    "\n",
    "                    if src_checksum_df is None or tgt_checksum_df is None:\n",
    "                        results.append(create_recon_result(\n",
    "                            tgt_table,\n",
    "                            \"CHECKSUM_VALIDATION\",\n",
    "                            \"ERROR\",\n",
    "                            error_msg=\"Failed to calculate checksums\",\n",
    "                            execution_id=execution_id\n",
    "                        ))\n",
    "                    else:\n",
    "                        checksum_result = compare_checksums(src_checksum_df, tgt_checksum_df, src_partitions_latest[0])\n",
    "                        duration = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "                        results.append(create_recon_result(\n",
    "                            tgt_table,\n",
    "                            \"CHECKSUM_VALIDATION\",\n",
    "                            checksum_result.get(\"status\", \"ERROR\"),\n",
    "                            #source_data={\"checksum_count\": checksum_result.get(\"source_record_count\")},\n",
    "                            #target_data={\"checksum_count\": checksum_result.get(\"target_record_count\")},\n",
    "                            source_data=None,\n",
    "                            target_data=None,\n",
    "                            checksum_result=checksum_result,\n",
    "                            exec_duration=duration,\n",
    "                            execution_id=execution_id\n",
    "                        ))\n",
    "\n",
    "        except Exception as e:\n",
    "            results.append(create_recon_result(\n",
    "                tgt_table,\n",
    "                \"CHECKSUM_VALIDATION\",\n",
    "                \"ERROR\",\n",
    "                error_msg=f\"Checksum validation failed: {str(e)}\",\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "    else:\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"CHECKSUM_VALIDATION\", \"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "            error_msg=\"Could not obtain information from source or target\",\n",
    "            execution_id=execution_id\n",
    "        ))\n",
    "\n",
    "    # VERIFIED Check (Table Statistics) - OPTIMIZED for billions of records\n",
    "    if src_df is not None and tgt_df is not None:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # print(f\"\\n{'='*60}\")\n",
    "            # print(f\"Starting VERIFIED check for: {tgt_table}\")\n",
    "            # print(f\"{'='*60}\")\n",
    "            \n",
    "            # **DYNAMIC PARTITION DETECTION**\n",
    "            try:\n",
    "                tgt_table_fqn = f\"{tgt_catalog_name}.{tgt_schema_name}.{tgt_table}\"\n",
    "                describe_df = spark.sql(f\"DESCRIBE DETAIL {tgt_table_fqn}\")\n",
    "                partition_cols_row = describe_df.select(\"partitionColumns\").first()\n",
    "                \n",
    "                if partition_cols_row and partition_cols_row[\"partitionColumns\"]:\n",
    "                    partition_cols = partition_cols_row[\"partitionColumns\"]\n",
    "                    # print(f\"Detected partition columns: {partition_cols}\")\n",
    "                else:\n",
    "                    partition_cols = [\"snapshot_date\", \"edp_run_id\"]\n",
    "                    # print(f\"Using default partition columns: {partition_cols}\")\n",
    "            except Exception as e:\n",
    "                # print(f\"Could not detect partition columns, using defaults: {e}\")\n",
    "                partition_cols = [\"snapshot_date\", \"edp_run_id\"]\n",
    "            \n",
    "            # Step 1: Select columns for verification (max 8 columns)\n",
    "            selected_columns, column_types = select_columns_for_verification(\n",
    "                src_df, \n",
    "                partition_cols=partition_cols,\n",
    "                max_columns=8\n",
    "            )\n",
    "            \n",
    "            if not selected_columns:\n",
    "                # print(f\"Warning: No columns selected for verification for {tgt_table}\")\n",
    "                results.append(create_recon_result(\n",
    "                    tgt_table, \"VERIFIED\", \"ERROR\",\n",
    "                    error_msg=\"No suitable columns found for verification\",\n",
    "                    execution_id=execution_id\n",
    "                ))\n",
    "            else:\n",
    "                # print(f\"Selected {len(selected_columns)} columns for verification:\")\n",
    "                for cat, cols in column_types.items():\n",
    "                    if cols:\n",
    "                        print(f\"  {cat}: {cols}\")\n",
    "                \n",
    "                # Step 2: Filter to latest 5 partitions - **CAPTURE PARTITION VALUES**\n",
    "                partition_filter_start = datetime.now()\n",
    "                \n",
    "                src_df_filtered, src_partitions_used = get_latest_partitions(\n",
    "                    src_df, \n",
    "                    partition_cols=partition_cols,\n",
    "                    n=1\n",
    "                )\n",
    "                tgt_df_filtered, tgt_partitions_used = get_latest_partitions(\n",
    "                    tgt_df, \n",
    "                    partition_cols=partition_cols,\n",
    "                    n=1\n",
    "                )\n",
    "                \n",
    "                # **Use source partitions for summary (should match target)**\n",
    "                partitions_used = src_partitions_used\n",
    "                \n",
    "                partition_filter_duration = (datetime.now() - partition_filter_start).total_seconds()\n",
    "                # print(f\"Partition filtering completed in {partition_filter_duration:.2f}s\")\n",
    "                # print(f\"Partitions used: {partitions_used}\")\n",
    "                \n",
    "                # Step 3: Project and cache only needed columns\n",
    "                prep_start = datetime.now()\n",
    "                src_df_prepared = prepare_dataframe_for_stats(\n",
    "                    src_df_filtered, \n",
    "                    partition_cols,\n",
    "                    selected_columns\n",
    "                )\n",
    "                tgt_df_prepared = prepare_dataframe_for_stats(\n",
    "                    tgt_df_filtered, \n",
    "                    partition_cols,\n",
    "                    selected_columns\n",
    "                )\n",
    "                prep_duration = (datetime.now() - prep_start).total_seconds()\n",
    "                # print(f\"DataFrame preparation completed in {prep_duration:.2f}s\")\n",
    "                \n",
    "                # Step 4: Compute statistics using separate functions\n",
    "                stats_start = datetime.now()\n",
    "                \n",
    "                # print(\"Computing SOURCE statistics...\")\n",
    "                src_stats = compute_source_stats(\n",
    "                    src_df_prepared, \n",
    "                    selected_columns=selected_columns,\n",
    "                    column_types=column_types\n",
    "                )\n",
    "                \n",
    "                # print(\"Computing TARGET statistics...\")\n",
    "                tgt_table_fqn = f\"{tgt_catalog_name}.{tgt_schema_name}.{tgt_table}\"\n",
    "                \n",
    "                # Create temporary view for target stats computation\n",
    "                # temp_view_name = f\"temp_target_{tgt_table}_{int(datetime.now().timestamp())}\"\n",
    "                # tgt_df_prepared.createOrReplaceTempView(temp_view_name)\n",
    "                \n",
    "                tgt_stats = get_table_stats(\n",
    "                    tgt_df_prepared,\n",
    "                    selected_columns=selected_columns,\n",
    "                    column_types=column_types\n",
    "                )\n",
    "                \n",
    "                # Drop temporary view\n",
    "                # spark.catalog.dropTempView(temp_view_name)\n",
    "                \n",
    "                stats_duration = (datetime.now() - stats_start).total_seconds()\n",
    "                # print(f\"Statistics computation completed in {stats_duration:.2f}s\")\n",
    "                \n",
    "                # Cleanup: Unpersist cached DataFrames\n",
    "                src_df_prepared.unpersist()\n",
    "                tgt_df_prepared.unpersist()\n",
    "                \n",
    "                total_duration = (datetime.now() - start_time).total_seconds()\n",
    "                print(f\"Total VERIFIED check duration: {total_duration:.2f}s\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "                \n",
    "                # Determine status\n",
    "                verified_status = \"PASS\" if (src_stats and tgt_stats) else \"ERROR\"\n",
    "                \n",
    "                # **NEW: Package stats with metadata for summary**\n",
    "                src_stats_with_metadata = {\n",
    "                    \"statistics\": src_stats,\n",
    "                    #\"partitions_used\": partitions_used,\n",
    "                    \"columns_verified\": selected_columns\n",
    "                }\n",
    "                \n",
    "                tgt_stats_with_metadata = {\n",
    "                    \"statistics\": tgt_stats,\n",
    "                    #\"partitions_used\": partitions_used,\n",
    "                    \"columns_verified\": selected_columns\n",
    "                }\n",
    "                \n",
    "                results.append(create_recon_result(\n",
    "                    tgt_table, \"VERIFIED\", verified_status, \n",
    "                    source_data=src_stats_with_metadata, \n",
    "                    target_data=tgt_stats_with_metadata,\n",
    "                    exec_duration=total_duration,\n",
    "                    partitions_used=partitions_used,\n",
    "                    execution_id=execution_id\n",
    "                ))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in VERIFIED check for {tgt_table}: {e}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            \n",
    "            results.append(create_recon_result(\n",
    "                tgt_table, \"VERIFIED\", \"ERROR\",\n",
    "                error_msg=f\"Statistics verification failed: {str(e)}\",\n",
    "                execution_id=execution_id\n",
    "            ))\n",
    "    else:\n",
    "        results.append(create_recon_result(\n",
    "            tgt_table, \"VERIFIED\", \"ERROR\",\n",
    "            error_msg=\"Could not access source or target data for statistics computation\",\n",
    "            execution_id=execution_id\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65c0f47-0d41-4b72-844e-64545ce20d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run Full Reconciliation\n",
    "\n",
    "Orchestrates the complete reconciliation process across all mapped tables. Iterates through the mapping configuration and executes comprehensive validation for each source-to-target table pair.\n",
    "\n",
    "**Process Flow:**\n",
    "- Loads all table mappings from configuration\n",
    "- Executes reconciliation for each table sequentially\n",
    "- Provides progress tracking and error reporting\n",
    "- Collects all results for batch processing\n",
    "- Handles critical errors with detailed logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c896194f-10ab-4e82-9979-397117fa9f76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Full Reconciliation"
    }
   },
   "outputs": [],
   "source": [
    "# Execute reconciliation across all mapped tables with progress tracking\n",
    "def run_full_reconciliation():\n",
    "    \"\"\"Execute reconciliation for all tables in the mapping configuration\"\"\"\n",
    "    all_results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    for row in mapping_df.collect():\n",
    "        try:\n",
    "            processed_count += 1\n",
    "            table_name = row.target_table_name\n",
    "            print(f\"Processing table {processed_count}: {table_name}\")\n",
    "            table_results = execute_reconciliation_for_table(row)\n",
    "            all_results.extend(table_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row.target_table_name}: {str(e)}\")\n",
    "            import traceback\n",
    "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "            \n",
    "            error_result = create_recon_result(\n",
    "                table_name=row.target_table_name,\n",
    "                recon_type=\"critical_error\",\n",
    "                status=\"ERROR\",  # Changed from \"FAIL\" to \"ERROR\"\n",
    "                error_msg=f\"Critical processing error: {str(e)}\", \n",
    "                execution_id=row.execution_id\n",
    "            )\n",
    "            all_results.append(error_result)\n",
    "    \n",
    "    print(f\"Processed {processed_count} tables, generated {len(all_results)} results\")\n",
    "    return all_results\n",
    "\n",
    "reconciliation_results = run_full_reconciliation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c29449-195e-41b7-a3c7-8aeb20526a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Save Results to Enhanced Table\n",
    "\n",
    "Persists all reconciliation results to the Delta table for analysis and reporting. Creates a comprehensive audit trail of all validation activities with detailed metrics and error information.\n",
    "\n",
    "**Features:**\n",
    "- Batch insert of all reconciliation results\n",
    "- Delta table format for ACID compliance\n",
    "- Schema evolution support with overwriteSchema option\n",
    "- Comprehensive error handling for save operations\n",
    "- Returns DataFrame for immediate analysis and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921a4a5a-edef-40cf-b1f9-7e33f1ce8d75",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"execution_id\":281,\"table_name\":240,\"summary\":375},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762790875852}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Save Results to Enhanced Table"
    }
   },
   "outputs": [],
   "source": [
    "# Save comprehensive reconciliation results to the main Delta table\n",
    "def save_results_to_enhanced_table(results):\n",
    "    \"\"\"Save reconciliation results to the enhanced Delta table\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        results_df = spark.createDataFrame(results, recon_results_schema)\n",
    "        \n",
    "        results_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(RESULTS_TABLE_FQN)\n",
    "\n",
    "        return results_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "        return None\n",
    "\n",
    "results_df = save_results_to_enhanced_table(reconciliation_results)\n",
    "results_df=results_df.withColumn(\"summary\",substring(\"summary\",1,1000))\\\n",
    "                .withColumn(\"error_log\",substring(\"error_log\",1,1000))\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459a27e0-febb-42f1-9721-a37e2c3dfe5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Candidate Table Update"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(mapping_df.count())\n",
    "if mapping_df.count()>0:\n",
    "    #print(\"test\")\n",
    "# Expected recon types\n",
    "    expected_recon_types = {\"ROW_COUNT\", \"COLUMN_COUNT\", \"SCHEMA_VALIDATION\",\"Partiton_Data_count\",\"CHECKSUM_VALIDATION\",\"VERIFIED\"}\n",
    "    recon_df =  results_df \n",
    "    candidate_df = spark.table(CANDIDATES_TABLE_FQN)\n",
    "    # # Step 1: Aggregate recon results per table and execution\n",
    "    agg_recon = (\n",
    "        recon_df.groupBy(\"execution_id\", \"table_name\")\n",
    "        .agg(\n",
    "            F.collect_set(\"recon_type\").alias(\"recon_types\"),\n",
    "            F.collect_set(\"status\").alias(\"statuses\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # , \"PARTITION_RECORD_COUNT\"\n",
    "    # Read both tables\n",
    "    # recon_df_table = spark.table(RESULTS_TABLE_FQN)\n",
    "    # # display(recon_df_table.limit(2))\n",
    "    # # display(mapping_df.limit(2))\n",
    "    # recon_df =( recon_df_table.alias(\"r1\").join(\n",
    "    #     mapping_df.alias(\"m1\"),\n",
    "    #     (col(\"r1.execution_id\") == col(\"m1.execution_id\")) &\n",
    "    #     (col(\"r1.table_name\") == col(\"m1.target_table_name\")))\n",
    "    #     .select(\"r1.*\")\n",
    "    #     )\n",
    "   \n",
    "    # CANDIDATES_TABLE_FQN = '89055_ctg_prod_exp.dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp.ccbr_migration_table_candidates_test'\n",
    "    \n",
    "    # #print(execution_id)\n",
    "    # #display(CANDIDATES_TABLE_FQN)\n",
    "    # display(recon_df)\n",
    "    \n",
    "    # # Step 2: Derive job_run and recon_status in a simple way\n",
    "    agg_recon = (\n",
    "        agg_recon.withColumn(\n",
    "            \"job_run\",\n",
    "            F.when(\n",
    "            ( F.size(F.array_except(F.array(*[F.lit(x) for x in expected_recon_types]), F.col(\"recon_types\"))) == 0) | (F.array_contains(F.col(\"recon_types\") ,\"data_read_error\")),\n",
    "                \"true\"\n",
    "            ).otherwise(\"false\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"recon_status\",\n",
    "            F.when(\n",
    "                (F.col(\"job_run\") == \"true\") & (~F.array_contains(F.col(\"statuses\"), \"FAIL\")) & (~F.array_contains(F.col(\"statuses\"), \"ERROR\")),\n",
    "                \"PASS\"\n",
    "            ).when(\n",
    "                (F.col(\"job_run\") == \"true\") & (F.array_contains(F.col(\"recon_types\") ,\"data_read_error\")),\n",
    "                \"SKIPPED\"\n",
    "            )\n",
    "            .otherwise(\"FAIL\")\n",
    "        )\n",
    "    )\n",
    "    # # Step 3: Use simple MERGE to update candidate table\n",
    "    agg_recon.createOrReplaceTempView(\"agg_recon_updates\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    MERGE INTO {CANDIDATES_TABLE_FQN} AS c\n",
    "    USING agg_recon_updates AS r\n",
    "    ON c.table_name = r.table_name AND c.execution_id = r.execution_id  \n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        c.recon_job_run = r.job_run\n",
    "        ,c.recon_status = r.recon_status\n",
    "        ,c.recon_execution_time = now()\n",
    "    \"\"\"\n",
    "    spark.sql(sql)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6799958917661265,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "6_perform_reconciliation_dataset_verified",
   "widgets": {
    "bucket_name": {
     "currentValue": "app-id-89055-dep-id-109792-uu-id-ertg58jid529",
     "nuid": "69764b72-4828-4ec6-8d24-3c057e9c250a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "bucket_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "bucket_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "candidate_table": {
     "currentValue": "ccbr_migration_table_candidates",
     "nuid": "27f1654f-3953-4a87-befd-b709be16c324",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "89055_ctg_prod",
     "nuid": "52d36201-c094-40f7-94c9-562df6b423ee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_mapping_table": {
     "currentValue": "ccbr_migration_dataset_mapping",
     "nuid": "85a72934-1eb4-487c-b34d-6cb04cecfc73",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_name": {
     "currentValue": "'dm_auth_txn'",
     "nuid": "bd999368-7791-435c-8b77-9c43eed90ec1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "dataset_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "dataset_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "inventory_table": {
     "currentValue": "ccbr_migration_table_inventory",
     "nuid": "27421f5b-58a3-4889-8997-b34dafe56217",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_schema_table": {
     "currentValue": "ccbr_migration_parquet_schemas",
     "nuid": "b8a3f6c5-a7d4-4509-a2c7-c4713dbabcea",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "partition_audit_table": {
     "currentValue": "89055_ctg_prod.89055_audit_db_hcd_dora_fdl.dataset_tags",
     "nuid": "258aad3f-8a13-4346-9c4d-2ab4cae047f4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "recon_results_table": {
     "currentValue": "ccbr_migration_recon_results",
     "nuid": "2e14ecff-ecbd-4877-adcf-25f988eab18a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "recon_results_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "recon_results_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "89055_ccbr_migration",
     "nuid": "85e30180-c7a9-4c0c-a1ec-4ae9abf5a2fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}