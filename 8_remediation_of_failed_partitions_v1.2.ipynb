{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bad7850-5860-4bec-a66d-40a9803a97f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter Setup and Configuration\n",
    "dbutils.widgets.text(\"parquet_schema_table\",\"\")\n",
    "dbutils.widgets.text(\"inventory_table\",\"\") \n",
    "dbutils.widgets.text(\"bucket_name\",\"\")\n",
    "dbutils.widgets.text(\"managed_catalog\",\"\")\n",
    "dbutils.widgets.text(\"partition_audit_table\",\"\")\n",
    "dbutils.widgets.text(\"recon_remediation_table\",\"\")\n",
    "dbutils.widgets.text(\"candidate_table\",\"\")\n",
    "dbutils.widgets.text(\"dataset_mapping_table\",\"\")\n",
    "dbutils.widgets.text(\"datasets\",\"\")\n",
    "dbutils.widgets.text(\"catalog\",\"\")\n",
    "dbutils.widgets.text(\"schema\",\"\")\n",
    "##dbutils.widgets.text(\"source_volume_path\",\"\")\n",
    "# dbutils.widgets.text(\"managed_schema\",\"\")\n",
    "\n",
    "parquet_schema_table = dbutils.widgets.get(\"parquet_schema_table\")\n",
    "inventory_table = dbutils.widgets.get(\"inventory_table\")\n",
    "bucket_name=dbutils.widgets.get(\"bucket_name\")\n",
    "partition_audit_table=dbutils.widgets.get(\"partition_audit_table\")\n",
    "managed_catalog_name=dbutils.widgets.get(\"managed_catalog\")\n",
    "recon_remediation_table=dbutils.widgets.get(\"recon_remediation_table\")\n",
    "candidate_table=dbutils.widgets.get(\"candidate_table\")\n",
    "dataset_mapping_table=dbutils.widgets.get(\"dataset_mapping_table\")\n",
    "datasets=dbutils.widgets.get(\"datasets\")\n",
    "# managed_schema_wdgt=dbutils.widgets.get(\"managed_schema\")\n",
    "catalog=dbutils.widgets.get(\"catalog\")\n",
    "schema=dbutils.widgets.get(\"schema\")\n",
    "\n",
    "print(f\"bucket_name: {bucket_name}\")\n",
    "print(f\"candidate_table: {candidate_table}\")\n",
    "print(f\"inventory_table: {inventory_table}\")\n",
    "print(f\"managed_catalog: {managed_catalog_name}\")\n",
    "# print(f\"managed_schema: {managed_schema}\")\n",
    "print(f\"parquet_schema_table: {parquet_schema_table}\")\n",
    "print(f\"recon_remediation_table: {recon_remediation_table}\")\n",
    "print(f\"partition_audit_table: {partition_audit_table}\")\n",
    "print(f\"dataset_mapping_table: {dataset_mapping_table}\")\n",
    "\n",
    "##MANAGED_SCHEMA_NAME=dbutils.widgets.get(\"managed_schema\")\n",
    "##volume=dbutils.widgets.get(\"source_volume_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f48ea8-a293-4839-9382-15c644d878b1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_extract, collect_list, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DecimalType, DateType, TimestampType, BinaryType, ShortType, ByteType, FloatType, DoubleType\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f34188-aef0-4802-bcb8-bb7a80ba597a",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"edp_run_id\":295,\"execution_id\":385,\"s3_bucket_name\":320,\"managed_schema\":323,\"bucket_prefix\":421,\"dataset_name\":258},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764071947215}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Step 1 -Extract failed records (loaded status 'failed') Inventory table based on Bucket_Name."
    }
   },
   "outputs": [],
   "source": [
    "df_failed_partitions = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        inv.execution_id,\n",
    "        inv.s3_bucket_name,\n",
    "        inv.bucket_prefix,\n",
    "        regexp_extract(inv.bucket_prefix, '([^/]+)[/]?$', 1) AS dataset_name,\n",
    "        inv.edp_run_id,\n",
    "        inv.snapshot_date,\n",
    "        map.dbx_managed_table_schema AS managed_schema\n",
    "    FROM {catalog}.{schema}.{inventory_table} inv\n",
    "    LEFT JOIN {catalog}.{schema}.{dataset_mapping_table} map\n",
    "      ON inv.s3_bucket_name = map.s3_bucket_name\n",
    "     AND inv.bucket_prefix = map.bucket_prefix\n",
    "     AND regexp_extract(inv.bucket_prefix, '([^/]+)[/]?$', 1) = map.dataset_name\n",
    "    WHERE inv.load_status = 'failed'\n",
    "      AND inv.extension IS NOT NULL\n",
    "      AND inv.edp_run_id IS NOT NULL\n",
    "      AND inv.snapshot_date >= '2020-01-01'\n",
    "      AND inv.s3_bucket_name = '{bucket_name}'\n",
    "      AND map.dataset_name in ({datasets})\n",
    "    \"\"\"\n",
    ")\n",
    "display(df_failed_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63901d8-5dd7-49d0-b5ca-e64a2b0d434e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "method used for getting the base schema"
    }
   },
   "outputs": [],
   "source": [
    "# this method is used for getting the base schema \n",
    "def parse_type(dtype):\n",
    "    dtype = dtype.lower().strip()\n",
    "\n",
    "    if dtype.startswith(\"decimal\"):\n",
    "        scale = dtype[dtype.find(\"(\") + 1 : dtype.find(\")\")].split(\",\")\n",
    "        return DecimalType(int(scale[0]), int(scale[1]))\n",
    "\n",
    "    if dtype in (\"int8\", \"byte\"):\n",
    "        return ByteType()\n",
    "    \n",
    "    if dtype.startswith(\"bytetype\"):\n",
    "        return ByteType()\n",
    "\n",
    "    if dtype in (\"int16\", \"smallint\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype.startswith(\"shorttype\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype in (\"int32\", \"integer\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype.startswith(\"integertype\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype == \"int64\":\n",
    "        return LongType()\n",
    "\n",
    "    if dtype.startswith(\"longtype\"):\n",
    "        return LongType()\n",
    "    \n",
    "    if dtype in (\"string\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"stringtype\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"date32\") or dtype == \"date\":\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"datetype\"):\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"floattype\"):\n",
    "        return FloatType()\n",
    "    \n",
    "    if dtype.startswith(\"doubletype\"):\n",
    "        return DoubleType()\n",
    "\n",
    "    if dtype.startswith(\"timestamp\"):\n",
    "        # Handles 'timestamp', 'timestamp[ms]', 'timestamp[us]' etc.\n",
    "        return TimestampType()\n",
    "\n",
    "    if dtype == \"bool\":\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype.startswith(\"booleantype\"):\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype == \"binary\":\n",
    "        return BinaryType()\n",
    "    \n",
    "    if dtype.startswith(\"binarytype\"):\n",
    "        return BinaryType()\n",
    "    \n",
    "    raise ValueError(f\"Unsupported type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b496e7a-8f16-47ed-92d9-875789ed7aa4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Iterate through list of failed datasets and get the base schema based on latest snapshot date"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-811489465029817>, line 65\u001B[0m\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(latest_snapshot_date)\n",
       "\u001B[1;32m     56\u001B[0m schema_json_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\n",
       "\u001B[1;32m     57\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     58\u001B[0m \u001B[38;5;124m    SELECT schema_json\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     62\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     63\u001B[0m )\n",
       "\u001B[0;32m---> 65\u001B[0m schema_json \u001B[38;5;241m=\u001B[39m schema_json_df\u001B[38;5;241m.\u001B[39mfirst()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mschema_json\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[1;32m     66\u001B[0m schema_dict \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(schema_json)\n",
       "\u001B[1;32m     68\u001B[0m base_schema \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[1;32m     69\u001B[0m     StructField(f[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m], parse_type(f[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m]), f[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnullable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m schema_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfields\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m     71\u001B[0m ])\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "'NoneType' object is not subscriptable"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-811489465029817>, line 65\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(latest_snapshot_date)\n\u001B[1;32m     56\u001B[0m schema_json_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124m    SELECT schema_json\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     63\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m schema_json \u001B[38;5;241m=\u001B[39m schema_json_df\u001B[38;5;241m.\u001B[39mfirst()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mschema_json\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     66\u001B[0m schema_dict \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(schema_json)\n\u001B[1;32m     68\u001B[0m base_schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m     69\u001B[0m     StructField(f[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m], parse_type(f[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m]), f[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnullable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m schema_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfields\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     71\u001B[0m ])\n",
        "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import json\n",
    "\n",
    "partition_key_combination = \"edp_run_id, snapshot_date\"\n",
    "dataset_schemas = {}\n",
    "\n",
    "failed_datasets = (\n",
    "    df_failed_partitions\n",
    "    .select(\"s3_bucket_name\", \"bucket_prefix\", \"dataset_name\")\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "for row in failed_datasets:\n",
    "    bucket_name = row[\"s3_bucket_name\"]\n",
    "    bucket_prefix = row[\"bucket_prefix\"]\n",
    "    dataset_name = row[\"dataset_name\"]\n",
    "\n",
    "    inventory_df = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT DISTINCT edp_run_id, snapshot_date\n",
    "        FROM (\n",
    "            SELECT DISTINCT edp_run_id, try_cast(snapshot_date AS DATE) AS snapshot_date\n",
    "            FROM {catalog}.{schema}.{inventory_table}\n",
    "            WHERE extension IS NOT NULL\n",
    "              AND partition_key = '{partition_key_combination}'\n",
    "              AND s3_bucket_name = '{bucket_name}'\n",
    "              AND bucket_prefix = '{bucket_prefix}'\n",
    "              AND load_status ='loaded'\n",
    "        ) inventory\n",
    "        JOIN (\n",
    "            SELECT DISTINCT run_id, try_cast(run_tag_value AS DATE) AS run_tag_value\n",
    "            FROM {partition_audit_table}\n",
    "            WHERE dataset_name = '{dataset_name}'\n",
    "              AND lower(run_tag_key) = 'snapshot_date'\n",
    "        ) partition\n",
    "        ON inventory.edp_run_id = partition.run_id\n",
    "           AND inventory.snapshot_date = partition.run_tag_value\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    if not inventory_df.isEmpty():\n",
    "        df_latest_snapshot = (\n",
    "            inventory_df\n",
    "            .orderBy(col(\"snapshot_date\").desc())\n",
    "            .limit(1)\n",
    "        )\n",
    "\n",
    "        latest_snapshot_row = df_latest_snapshot.collect()[0]\n",
    "        latest_edp_run_id = latest_snapshot_row[\"edp_run_id\"]\n",
    "        latest_snapshot_date = latest_snapshot_row[\"snapshot_date\"]\n",
    "\n",
    "        print(latest_edp_run_id)\n",
    "        print(latest_snapshot_date)\n",
    "\n",
    "        schema_json_df = spark.sql(\n",
    "            f\"\"\"\n",
    "            SELECT schema_json\n",
    "            FROM {catalog}.{schema}.{parquet_schema_table}\n",
    "            WHERE bucket_prefix = '{bucket_prefix}'\n",
    "              AND file_path LIKE '%edp_run_id={latest_edp_run_id}/snapshot_date={latest_snapshot_date}%'\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        schema_json = schema_json_df.first()['schema_json']\n",
    "        schema_dict = json.loads(schema_json)\n",
    "\n",
    "        base_schema = StructType([\n",
    "            StructField(f[\"name\"], parse_type(f[\"type\"]), f[\"nullable\"])\n",
    "            for f in schema_dict[\"fields\"]\n",
    "        ])\n",
    "\n",
    "        partition_columns = [\n",
    "            partition_column.strip()\n",
    "            for partition_column in partition_key_combination.split(\",\")\n",
    "            if partition_column.strip()\n",
    "        ]\n",
    "\n",
    "        partition_type_map = {\n",
    "            \"edp_run_id\": StringType(),\n",
    "            \"snapshot_date\": DateType()\n",
    "        }\n",
    "\n",
    "        extended_fields = base_schema.fields.copy()\n",
    "        for partition_column in partition_columns:\n",
    "            col_type = partition_type_map.get(partition_column, StringType())\n",
    "            extended_fields.append(StructField(partition_column, col_type, True))\n",
    "\n",
    "        extended_schema = StructType(extended_fields)\n",
    "\n",
    "        # print(extended_schema)\n",
    "\n",
    "        # Add to dictionary\n",
    "        dataset_schemas[(bucket_name, bucket_prefix, dataset_name)] = extended_schema\n",
    "\n",
    "# Now dataset_schemas is ready to use in your processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b153b0-cfe6-463b-b0c9-86ac841ed7db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Cast Failed DataFrame Columns to Base Schema Types"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def cast_failed_df_to_base_schema(failed_df, base_schema):\n",
    "    exprs = []\n",
    "    mismatched = []\n",
    "    # Map failed_df columns to lowercase for matching\n",
    "    failed_schema = {f.name.lower(): f.dataType for f in failed_df.schema.fields}\n",
    "    # Create a mapping from lowercase to actual column name\n",
    "    failed_col_map = {f.name.lower(): f.name for f in failed_df.schema.fields}\n",
    "    for field in base_schema.fields:\n",
    "        col_name = field.name\n",
    "        col_name_lc = col_name.lower()\n",
    "        target_type = field.dataType\n",
    "        if col_name_lc in failed_schema:\n",
    "            failed_type = failed_schema[col_name_lc]\n",
    "            actual_col_name = failed_col_map[col_name_lc]\n",
    "            if type(failed_type) != type(target_type) or (\n",
    "                hasattr(target_type, \"precision\") and hasattr(failed_type, \"precision\") and\n",
    "                (target_type.precision != failed_type.precision or target_type.scale != failed_type.scale)\n",
    "            ):\n",
    "                mismatched.append(col_name)\n",
    "                exprs.append(col(actual_col_name).cast(target_type).alias(col_name))\n",
    "            else:\n",
    "                exprs.append(col(actual_col_name).alias(col_name))\n",
    "    print(f\"✓ {len(mismatched)} columns will be typecasted: {mismatched}\")\n",
    "    return failed_df.select(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e20ff76-34bf-4738-8c84-0edf001d476c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complete Processing Loop for all partitions"
    }
   },
   "outputs": [],
   "source": [
    "# Complete updated processing cell\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from collections import defaultdict\n",
    "\n",
    "rows = df_failed_partitions.collect()\n",
    "partitions_by_dataset = defaultdict(list)\n",
    "for row in rows:\n",
    "    key = (\n",
    "        row['s3_bucket_name'],\n",
    "        row['bucket_prefix'],\n",
    "        row['dataset_name'],\n",
    "        row['managed_schema']\n",
    "    )\n",
    "    partitions_by_dataset[key].append(row)\n",
    "\n",
    "remediation_status_rows = []\n",
    "\n",
    "for dataset_key, partition_rows in partitions_by_dataset.items():\n",
    "    s3_bucket_name, bucket_prefix, dataset_name, managed_schema = dataset_key\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"Bucket: {s3_bucket_name}\")\n",
    "    print(f\"Prefix: {bucket_prefix}\")\n",
    "    print(f\"Managed schema: {managed_schema}\")\n",
    "    print(f\"Partition key: edp_run_id, snapshot_date\")\n",
    "    print(f\"Total partitions found for dataset: {len(partition_rows)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    extended_schema = dataset_schemas.get((s3_bucket_name, bucket_prefix, dataset_name))\n",
    "    if extended_schema is None:\n",
    "        print(f\"✗ No schema found for {(s3_bucket_name, bucket_prefix, dataset_name)}, skipping ALL partitions for this dataset.\")\n",
    "        continue\n",
    "\n",
    "    partitions_attempted = len(partition_rows)\n",
    "    partitions_remediated = 0\n",
    "    partitions_failed = 0\n",
    "    error_msg = []\n",
    "    execution_ids = set()\n",
    "\n",
    "    for partition_index, row in enumerate(partition_rows, 1):\n",
    "        execution_id = row['execution_id']\n",
    "        run_id = row['edp_run_id']\n",
    "        snapshot_date = row['snapshot_date']\n",
    "        execution_ids.add(execution_id)\n",
    "\n",
    "        print(f\"\\nProcessing partition {partition_index}/{partitions_attempted} for dataset {dataset_name}\")\n",
    "        print(f\"run_id={run_id}, snapshot_date={snapshot_date}\")\n",
    "        \n",
    "        try:\n",
    "            path = f\"s3://{s3_bucket_name}/{bucket_prefix}/edp_run_id={run_id}/snapshot_date={snapshot_date}/\"\n",
    "            print(f\"Reading from s3 path {path}\")\n",
    "            failed_df = spark.read.parquet(path)\n",
    "            print(f\"✓ Read {failed_df.count()} records from failed partition\")\n",
    "\n",
    "            # Add partition columns if not present\n",
    "            for partition_col in [\"edp_run_id\", \"snapshot_date\"]:\n",
    "                if partition_col not in failed_df.columns:\n",
    "                    if partition_col == \"edp_run_id\":\n",
    "                        failed_df = failed_df.withColumn(\"edp_run_id\", lit(run_id))\n",
    "                    elif partition_col == \"snapshot_date\":\n",
    "                        failed_df = failed_df.withColumn(\"snapshot_date\", lit(snapshot_date))\n",
    "\n",
    "            # Cast and align schema\n",
    "            remediated_df = cast_failed_df_to_base_schema(failed_df, extended_schema)\n",
    "            print(f\"✓ Final remediated DataFrame has {len(remediated_df.columns)} columns\")\n",
    "\n",
    "            remediated_df.write\\\n",
    "            .mode(\"append\")\\\n",
    "            .saveAsTable(f\"{managed_catalog_name}.{managed_schema}.{dataset_name}\")\n",
    "\n",
    "            print(f\"✓ Successfully appended {remediated_df.count()} records to {managed_catalog_name}.{managed_schema}.{dataset_name}\")\n",
    "\n",
    "            update_audit_query = f\"\"\"\n",
    "            UPDATE {catalog}.{schema}.{inventory_table}\n",
    "            SET load_status = 'loaded',\n",
    "                last_modified_time = current_timestamp()\n",
    "            WHERE edp_run_id = '{run_id}'\n",
    "              AND snapshot_date = '{snapshot_date}'\n",
    "              AND bucket_prefix = '{bucket_prefix}'\n",
    "            \"\"\"\n",
    "            spark.sql(update_audit_query)\n",
    "\n",
    "            print(f\"✓ Updated inventory table for run_id: {run_id}, snapshot_date: {snapshot_date}\")\n",
    "\n",
    "            partitions_remediated += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ ERROR processing partition run_id={run_id}, snapshot_date={snapshot_date}: {str(e)}\")\n",
    "            error_msg.append(f\"ERROR processing partition run_id={run_id}, snapshot_date={snapshot_date}: {str(e)}\")\n",
    "            partitions_failed += 1\n",
    "            continue\n",
    "\n",
    "    if partitions_remediated == partitions_attempted and partitions_failed == 0:\n",
    "        update_candidates_query = f\"\"\"\n",
    "        UPDATE {catalog}.{schema}.{candidate_table}\n",
    "        SET recon_job_run = NULL\n",
    "        WHERE execution_id IN ({','.join([f\"'{eid}'\" for eid in execution_ids])})\n",
    "          AND table_name = '{dataset_name}'\n",
    "        \"\"\"\n",
    "        spark.sql(update_candidates_query)\n",
    "        \n",
    "        print(f\"✓ Updated ccbr_migration_table_candidates for table_name='{dataset_name}'\")\n",
    "    else:\n",
    "        print(f\"✗ Not updating candidate table for {dataset_name} as not all partitions loaded successfully\")\n",
    "\n",
    "    remediation_status_rows.append(\n",
    "        Row(\n",
    "            execution_id=\",\".join(execution_ids),\n",
    "            s3_bucket_name=s3_bucket_name,\n",
    "            bucket_prefix=bucket_prefix,\n",
    "            dataset_name=dataset_name,\n",
    "            remediation_attempted_time=datetime.now(),\n",
    "            partitions_attempted=partitions_attempted,\n",
    "            partitions_remediated=partitions_remediated,\n",
    "            partitions_failed=partitions_failed,\n",
    "            remediation_status=\"PASS\" if partitions_failed == 0 else \"FAIL\",\n",
    "            error_msg=\"; \".join(error_msg) if error_msg else None\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL PARTITIONS PROCESSED!\")\n",
    "print(f\"✓ Remediation summary written per dataset\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if remediation_status_rows:\n",
    "    schema_df = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"s3_bucket_name\", StringType(), True),\n",
    "        StructField(\"bucket_prefix\", StringType(), True),\n",
    "        StructField(\"dataset_name\", StringType(), True),\n",
    "        StructField(\"remediation_attempted_time\", TimestampType(), True),\n",
    "        StructField(\"partitions_attempted\", IntegerType(), True),\n",
    "        StructField(\"partitions_remediated\", IntegerType(), True),\n",
    "        StructField(\"partitions_failed\", IntegerType(), True),\n",
    "        StructField(\"remediation_status\", StringType(), True),\n",
    "        StructField(\"error_msg\", StringType(), True)\n",
    "    ])\n",
    "    load_remediation_df = spark.createDataFrame(remediation_status_rows, schema_df)\n",
    "    display(load_remediation_df)\n",
    "\n",
    "    load_remediation_df.write\\\n",
    "        .mode(\"append\")\\\n",
    "        .saveAsTable(f\"{catalog}.{schema}.{recon_remediation_table}\")\n",
    "\n",
    "    # Data loaded into recon_remediation table\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5099946312198114,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "8_remediation_of_failed_partitions_v1.2",
   "widgets": {
    "bucket_name": {
     "currentValue": "app-id-89055-dep-id-109792-uu-id-n6ph64imx36e",
     "nuid": "3f4d20c2-d9a1-47d6-bd0e-1097759e5112",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "bucket_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "bucket_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "candidate_table": {
     "currentValue": "ccbr_migration_table_candidates",
     "nuid": "eabcd6a0-f391-40da-90c9-50f5daeb3df3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "89055_ctg_prod_exp",
     "nuid": "ca911ea9-302c-4f0a-b6f0-20778ad2ad2b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_mapping_table": {
     "currentValue": "ccbr_migration_dataset_mapping",
     "nuid": "df9c3b4c-b894-4d0a-9ccb-f7b8759549c7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "datasets": {
     "currentValue": "'lda_dss_txns_rbfa16dcit_daily'",
     "nuid": "52cd245f-530f-4cd6-ac7f-a367bc81e9dc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "datasets",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "datasets",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "inventory_table": {
     "currentValue": "ccbr_migration_table_inventory",
     "nuid": "3aad63c4-f241-474c-a6d0-c92578ccf280",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "managed_catalog": {
     "currentValue": "89055_ctg_prod_exp",
     "nuid": "57896485-eea8-4224-99f6-44276cfd9b66",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "managed_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "managed_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_schema_table": {
     "currentValue": "ccbr_migration_parquet_schemas",
     "nuid": "edb08c32-0fca-477b-8708-97e7d952b3c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "partition_audit_table": {
     "currentValue": "89055_ctg_prod.89055_audit_db_hcd_dora_fdl.dataset_tags",
     "nuid": "35286504-84dc-4fcb-b477-a306f72d6a91",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "recon_remediation_table": {
     "currentValue": "ccbr_migration_recon_remediation",
     "nuid": "8550db8b-18df-4ed7-b470-fecf05c59c01",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "recon_remediation_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "recon_remediation_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp",
     "nuid": "1da762a8-8a5e-4f47-96cb-44b6041f37e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}