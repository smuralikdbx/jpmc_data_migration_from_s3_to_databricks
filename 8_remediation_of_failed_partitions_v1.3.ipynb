{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bad7850-5860-4bec-a66d-40a9803a97f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter Setup and Configuration\n",
    "dbutils.widgets.text(\"parquet_schema_table\",\"\")\n",
    "dbutils.widgets.text(\"inventory_table\",\"\") \n",
    "dbutils.widgets.text(\"bucket_name\",\"\")\n",
    "dbutils.widgets.text(\"managed_catalog\",\"\")\n",
    "dbutils.widgets.text(\"partition_audit_table\",\"\")\n",
    "dbutils.widgets.text(\"recon_remediation_table\",\"\")\n",
    "dbutils.widgets.text(\"candidate_table\",\"\")\n",
    "dbutils.widgets.text(\"dataset_mapping_table\",\"\")\n",
    "dbutils.widgets.text(\"datasets\",\"\")\n",
    "dbutils.widgets.text(\"catalog\",\"\")\n",
    "dbutils.widgets.text(\"schema\",\"\")\n",
    "\n",
    "parquet_schema_table = dbutils.widgets.get(\"parquet_schema_table\")\n",
    "inventory_table = dbutils.widgets.get(\"inventory_table\")\n",
    "bucket_name=dbutils.widgets.get(\"bucket_name\")\n",
    "partition_audit_table=dbutils.widgets.get(\"partition_audit_table\")\n",
    "managed_catalog_name=dbutils.widgets.get(\"managed_catalog\")\n",
    "recon_remediation_table=dbutils.widgets.get(\"recon_remediation_table\")\n",
    "candidate_table=dbutils.widgets.get(\"candidate_table\")\n",
    "dataset_mapping_table=dbutils.widgets.get(\"dataset_mapping_table\")\n",
    "datasets=dbutils.widgets.get(\"datasets\")\n",
    "catalog=dbutils.widgets.get(\"catalog\")\n",
    "schema=dbutils.widgets.get(\"schema\")\n",
    "\n",
    "print(f\"bucket_name: {bucket_name}\")\n",
    "print(f\"candidate_table: {candidate_table}\")\n",
    "print(f\"inventory_table: {inventory_table}\")\n",
    "print(f\"managed_catalog: {managed_catalog_name}\")\n",
    "print(f\"parquet_schema_table: {parquet_schema_table}\")\n",
    "print(f\"recon_remediation_table: {recon_remediation_table}\")\n",
    "print(f\"partition_audit_table: {partition_audit_table}\")\n",
    "print(f\"dataset_mapping_table: {dataset_mapping_table}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f48ea8-a293-4839-9382-15c644d878b1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_extract, collect_list, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DecimalType, DateType, TimestampType, BinaryType, ShortType, ByteType, FloatType, DoubleType\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40298e0e-7dd1-4477-9043-89891978db08",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765798133485}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Step 1 -Extract failed records (loaded status 'failed') Inventory table based on Bucket_Name."
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Step 1 -Extract failed records (loaded status 'failed') Inventory table based on Bucket_Name.\n",
    "df_failed_partitions = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        inv.execution_id,\n",
    "        inv.s3_bucket_name,\n",
    "        inv.bucket_prefix,\n",
    "        inv.edp_run_id,\n",
    "        inv.snapshot_date,\n",
    "        regexp_extract(inv.bucket_prefix, '([^/]+)[/]?$', 1) AS dataset_name,\n",
    "        CONCAT('s3://', inv.s3_bucket_name, '/', LEFT(inv.key, LENGTH(inv.key) - POSITION('/' IN REVERSE(inv.key)))) AS partition_path,\n",
    "        map.dbx_managed_table_schema AS managed_schema\n",
    "    FROM {catalog}.{schema}.{inventory_table} inv\n",
    "    LEFT JOIN {catalog}.{schema}.{dataset_mapping_table} map\n",
    "      ON inv.s3_bucket_name = map.s3_bucket_name\n",
    "     AND inv.bucket_prefix = map.bucket_prefix\n",
    "     AND regexp_extract(inv.bucket_prefix, '([^/]+)[/]?$', 1) = map.dataset_name\n",
    "    WHERE inv.load_status = 'failed'\n",
    "      AND inv.extension IS NOT NULL\n",
    "      AND inv.edp_run_id IS NOT NULL\n",
    "      AND lower(inv.partition_key) in ('edp_run_id, snapshot_date')\n",
    "      AND inv.snapshot_date >= '2020-01-01'\n",
    "      AND inv.s3_bucket_name = '{bucket_name}'\n",
    "      AND map.dataset_name in ({datasets})\n",
    "    \"\"\"\n",
    ")\n",
    "display(df_failed_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63901d8-5dd7-49d0-b5ca-e64a2b0d434e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "method used for getting the base schema"
    }
   },
   "outputs": [],
   "source": [
    "# this method is used for getting the base schema \n",
    "def parse_type(dtype):\n",
    "    dtype = dtype.lower().strip()\n",
    "\n",
    "    if dtype.startswith(\"decimal\"):\n",
    "        scale = dtype[dtype.find(\"(\") + 1 : dtype.find(\")\")].split(\",\")\n",
    "        return DecimalType(int(scale[0]), int(scale[1]))\n",
    "\n",
    "    if dtype in (\"int8\", \"byte\"):\n",
    "        return ByteType()\n",
    "    \n",
    "    if dtype.startswith(\"bytetype\"):\n",
    "        return ByteType()\n",
    "\n",
    "    if dtype in (\"int16\", \"smallint\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype.startswith(\"shorttype\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype in (\"int32\", \"integer\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype.startswith(\"integertype\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype == \"int64\":\n",
    "        return LongType()\n",
    "\n",
    "    if dtype.startswith(\"longtype\"):\n",
    "        return LongType()\n",
    "    \n",
    "    if dtype in (\"string\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"stringtype\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"date32\") or dtype == \"date\":\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"datetype\"):\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"floattype\"):\n",
    "        return FloatType()\n",
    "    \n",
    "    if dtype.startswith(\"doubletype\"):\n",
    "        return DoubleType()\n",
    "\n",
    "    if dtype.startswith(\"timestamp\"):\n",
    "        # Handles 'timestamp', 'timestamp[ms]', 'timestamp[us]' etc.\n",
    "        return TimestampType()\n",
    "\n",
    "    if dtype == \"bool\":\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype.startswith(\"booleantype\"):\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype == \"binary\":\n",
    "        return BinaryType()\n",
    "    \n",
    "    if dtype.startswith(\"binarytype\"):\n",
    "        return BinaryType()\n",
    "    \n",
    "    raise ValueError(f\"Unsupported type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b496e7a-8f16-47ed-92d9-875789ed7aa4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Iterate through list of failed datasets and get the base schema based on latest snapshot date"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import json\n",
    "\n",
    "partition_key_combination = \"edp_run_id, snapshot_date\"\n",
    "dataset_schemas = {}\n",
    "\n",
    "failed_datasets = (\n",
    "    df_failed_partitions\n",
    "    .select(\"s3_bucket_name\", \"bucket_prefix\", \"dataset_name\")\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "for row in failed_datasets:\n",
    "    bucket_name = row[\"s3_bucket_name\"]\n",
    "    bucket_prefix = row[\"bucket_prefix\"]\n",
    "    dataset_name = row[\"dataset_name\"]\n",
    "\n",
    "    inventory_df = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT DISTINCT edp_run_id, snapshot_date\n",
    "        FROM (\n",
    "            SELECT DISTINCT edp_run_id, try_cast(snapshot_date AS DATE) AS snapshot_date\n",
    "            FROM {catalog}.{schema}.{inventory_table}\n",
    "            WHERE extension IS NOT NULL\n",
    "              AND partition_key = '{partition_key_combination}'\n",
    "              AND s3_bucket_name = '{bucket_name}'\n",
    "              AND bucket_prefix = '{bucket_prefix}'\n",
    "              AND load_status ='loaded'\n",
    "        ) inventory\n",
    "        JOIN (\n",
    "            SELECT DISTINCT run_id, try_cast(run_tag_value AS DATE) AS run_tag_value\n",
    "            FROM {partition_audit_table}\n",
    "            WHERE dataset_name = '{dataset_name}'\n",
    "              AND lower(run_tag_key) = 'snapshot_date'\n",
    "        ) partition\n",
    "        ON inventory.edp_run_id = partition.run_id\n",
    "           AND inventory.snapshot_date = partition.run_tag_value\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    if not inventory_df.isEmpty():\n",
    "        df_latest_snapshot = (\n",
    "            inventory_df\n",
    "            .orderBy(col(\"snapshot_date\").desc())\n",
    "            .limit(1)\n",
    "        )\n",
    "\n",
    "        latest_snapshot_row = df_latest_snapshot.collect()[0]\n",
    "        latest_edp_run_id = latest_snapshot_row[\"edp_run_id\"]\n",
    "        latest_snapshot_date = latest_snapshot_row[\"snapshot_date\"]\n",
    "\n",
    "        print(latest_edp_run_id)\n",
    "        print(latest_snapshot_date)\n",
    "\n",
    "        schema_json_df = spark.sql(\n",
    "            f\"\"\"\n",
    "            SELECT schema_json\n",
    "            FROM {catalog}.{schema}.{parquet_schema_table}\n",
    "            WHERE bucket_prefix = '{bucket_prefix}'\n",
    "              AND lower(file_path) LIKE '%edp_run_id={latest_edp_run_id}/snapshot_date={latest_snapshot_date}%'\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        schema_json = schema_json_df.first()['schema_json']\n",
    "        schema_dict = json.loads(schema_json)\n",
    "\n",
    "        base_schema = StructType([\n",
    "            StructField(f[\"name\"], parse_type(f[\"type\"]), f[\"nullable\"])\n",
    "            for f in schema_dict[\"fields\"]\n",
    "        ])\n",
    "\n",
    "        partition_columns = [\n",
    "            partition_column.strip()\n",
    "            for partition_column in partition_key_combination.split(\",\")\n",
    "            if partition_column.strip()\n",
    "        ]\n",
    "\n",
    "        partition_type_map = {\n",
    "            \"edp_run_id\": StringType(),\n",
    "            \"snapshot_date\": DateType()\n",
    "        }\n",
    "\n",
    "        extended_fields = base_schema.fields.copy()\n",
    "        for partition_column in partition_columns:\n",
    "            col_type = partition_type_map.get(partition_column, StringType())\n",
    "            extended_fields.append(StructField(partition_column, col_type, True))\n",
    "\n",
    "        extended_schema = StructType(extended_fields)\n",
    "\n",
    "        # print(extended_schema)\n",
    "\n",
    "        # Add to dictionary\n",
    "        dataset_schemas[(bucket_name, bucket_prefix, dataset_name)] = extended_schema\n",
    "\n",
    "# Now dataset_schemas is ready to use in your processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b153b0-cfe6-463b-b0c9-86ac841ed7db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Cast Failed DataFrame Columns to Base Schema Types"
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Function: Cast Failed DataFrame Columns to Base Schema Types\n",
    "def cast_failed_df_to_base_schema(failed_df, base_schema):\n",
    "    exprs = []\n",
    "    mismatched = []\n",
    "    # Map failed_df columns to lowercase for matching\n",
    "    failed_schema = {f.name.lower(): f.dataType for f in failed_df.schema.fields}\n",
    "    # Create a mapping from lowercase to actual column name\n",
    "    failed_col_map = {f.name.lower(): f.name for f in failed_df.schema.fields}\n",
    "    for field in base_schema.fields:\n",
    "        col_name = field.name\n",
    "        col_name_lc = col_name.lower()\n",
    "        target_type = field.dataType\n",
    "        if col_name_lc in failed_schema:\n",
    "            failed_type = failed_schema[col_name_lc]\n",
    "            actual_col_name = failed_col_map[col_name_lc]\n",
    "            if type(failed_type) != type(target_type) or (\n",
    "                hasattr(target_type, \"precision\") and hasattr(failed_type, \"precision\") and\n",
    "                (target_type.precision != failed_type.precision or target_type.scale != failed_type.scale)\n",
    "            ):\n",
    "                mismatched.append(col_name)\n",
    "                exprs.append(col(actual_col_name).cast(target_type).alias(col_name))\n",
    "            else:\n",
    "                exprs.append(col(actual_col_name).alias(col_name))\n",
    "    print(f\"✓ {len(mismatched)} columns will be typecasted\")\n",
    "    return failed_df.select(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef02cc3d-1fb8-416b-b1cf-772aeb3d1a47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Threading"
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Threading\n",
    "\n",
    "def process_partition(partition_index, row, partitions_attempted, s3_bucket_name, bucket_prefix, extended_schema, dataset_name, managed_schema, max_retries=0):\n",
    "        execution_id = row['execution_id']\n",
    "        partition_path = row['partition_path']\n",
    "        run_id = row['edp_run_id']\n",
    "        snapshot_date = row['snapshot_date']\n",
    "        # execution_ids.add(execution_id)\n",
    "\n",
    "        print(f\"\\nProcessing partition {partition_index}/{partitions_attempted} for dataset {dataset_name}\")\n",
    "        print(f\"run_id={run_id}, snapshot_date={snapshot_date}\")\n",
    "        print(f\"partition_path={partition_path}\")\n",
    "        \n",
    "        try:\n",
    "            # s3_path = f\"s3://{s3_bucket_name}/{bucket_prefix}/\"\n",
    "            print(f\"Reading from partition path: {partition_path}\")\n",
    "            \n",
    "            # Read without enforcing schema first to get raw data\n",
    "            failed_df = spark.read.parquet(partition_path)\n",
    "            print(f\"✓ Read {failed_df.count()} records from failed partition\")\n",
    "\n",
    "            # Cast and align schema\n",
    "            remediated_df = cast_failed_df_to_base_schema(failed_df, extended_schema)\n",
    "            print(f\"✓ Schema cast complete. DataFrame has {len(remediated_df.columns)} columns\")\n",
    "\n",
    "            # Drop partition columns if they exist (to avoid conflicts with source data)\n",
    "            for partition_col in [\"edp_run_id\", \"snapshot_date\"]:\n",
    "                if partition_col in remediated_df.columns:\n",
    "                    remediated_df = remediated_df.drop(partition_col)\n",
    "                    print(f\"✓ Dropped existing column: {partition_col}\")\n",
    "\n",
    "            # Re-add partition columns with correct values from partition path metadata\n",
    "            remediated_df = remediated_df.withColumn(\"edp_run_id\", lit(run_id))\n",
    "            remediated_df = remediated_df.withColumn(\"snapshot_date\", lit(snapshot_date))\n",
    "            print(f\"✓ Added partition columns with metadata values: edp_run_id={run_id}, snapshot_date={snapshot_date}\")\n",
    "            print(f\"✓ Final remediated DataFrame has {len(remediated_df.columns)} columns\")\n",
    "\n",
    "            # Re-add partition columns with correct values extracted from partition_path\n",
    "            # Extract edp_run_id and snapshot_date from partition_path\n",
    "            # import re\n",
    "            # from pyspark.sql.functions import to_date\n",
    "            \n",
    "            # edp_run_id_match = re.search(r'edp_run_id=([^/]+)', partition_path, re.IGNORECASE)\n",
    "            # snapshot_date_match = re.search(r'snapshot_date=([^/]+)', partition_path, re.IGNORECASE)\n",
    "            \n",
    "            # if edp_run_id_match and snapshot_date_match:\n",
    "            #     run_id = edp_run_id_match.group(1)\n",
    "            #     snap_date = snapshot_date_match.group(1)\n",
    "                \n",
    "            #     remediated_df = remediated_df.withColumn(\"edp_run_id\", lit(run_id))\n",
    "            #     remediated_df = remediated_df.withColumn(\"snapshot_date\", to_date(lit(snap_date)))\n",
    "            #     print(f\"✓ Added partition columns with metadata values: edp_run_id={run_id}, snapshot_date={snap_date}\")\n",
    "            \n",
    "            # print(f\"✓ Final remediated DataFrame has {len(remediated_df.columns)} columns\")\n",
    "\n",
    "            remediated_df.write\\\n",
    "            .mode(\"append\")\\\n",
    "            .saveAsTable(f\"{managed_catalog_name}.{managed_schema}.{dataset_name}\")\n",
    "\n",
    "            print(f\"✓ Successfully appended {remediated_df.count()} records to {managed_catalog_name}.{managed_schema}.{dataset_name}\")\n",
    "\n",
    "            return (execution_id, s3_bucket_name, bucket_prefix, run_id, snapshot_date, partition_path, \"PASS\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ ERROR processing partition partition_path={partition_path}: {str(e)}\")\n",
    "            error_msg.append(f\"ERROR processing partition_path={partition_path}: {str(e)}\")\n",
    "            return (execution_id, s3_bucket_name, bucket_prefix, run_id, snapshot_date, partition_path, \"FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8b0b87-fa5b-4bf6-ad04-1da9d7647138",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"execution_id\":250},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765415251745}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Complete Process - Loop for all partitions"
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Complete Process - Loop for all partitions\n",
    "# Complete updated processing cell\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from collections import defaultdict\n",
    "\n",
    "rows = df_failed_partitions.collect()\n",
    "partitions_by_dataset = defaultdict(list)\n",
    "for row in rows:\n",
    "    key = (\n",
    "        row['s3_bucket_name'],\n",
    "        row['bucket_prefix'],\n",
    "        row['dataset_name'],\n",
    "        row['managed_schema']\n",
    "    )\n",
    "    partitions_by_dataset[key].append(row)\n",
    "\n",
    "remediation_status_rows = []\n",
    "\n",
    "for dataset_key, partition_rows in partitions_by_dataset.items():\n",
    "    s3_bucket_name, bucket_prefix, dataset_name, managed_schema = dataset_key\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"Bucket: {s3_bucket_name}\")\n",
    "    print(f\"Prefix: {bucket_prefix}\")\n",
    "    print(f\"Managed schema: {managed_schema}\")\n",
    "    print(f\"Partition key: edp_run_id, snapshot_date\")\n",
    "    print(f\"Total partitions found for dataset: {len(partition_rows)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    extended_schema = dataset_schemas.get((s3_bucket_name, bucket_prefix, dataset_name))\n",
    "    if extended_schema is None:\n",
    "        print(f\"✗ No schema found for {(s3_bucket_name, bucket_prefix, dataset_name)}, skipping ALL partitions for this dataset.\")\n",
    "        continue\n",
    "    \n",
    "    remediate_status_schema = StructType([\n",
    "            StructField(\"execution_id\", StringType(), True), \n",
    "            StructField(\"s3_bucket_name\", StringType(), True),\n",
    "            StructField(\"bucket_prefix\", StringType(), True),\n",
    "            StructField(\"edp_run_id\", StringType(), True),\n",
    "            StructField(\"snapshot_date\", DateType(), True),\n",
    "            StructField(\"partition_path\", StringType(), True),\n",
    "            StructField(\"load_status\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    remediate_status_df = spark.createDataFrame([], remediate_status_schema)\n",
    "    partitions_attempted = len(partition_rows)\n",
    "    partitions_remediated = 0\n",
    "    partitions_failed = 0\n",
    "    successful_partitions = []\n",
    "    failed_partitions = []\n",
    "    # execution_ids = set()\n",
    "    execution_id = None\n",
    "    max_workers = 6 \n",
    "    \n",
    "    # Initialize error_msg list for this dataset\n",
    "    error_msg = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                # Submit all partition processing tasks\n",
    "                remediate_paritions = {\n",
    "                    executor.submit(\n",
    "                        process_partition,\n",
    "                        partition_index,\n",
    "                        row,\n",
    "                        partitions_attempted,\n",
    "                        s3_bucket_name,\n",
    "                        bucket_prefix,\n",
    "                        extended_schema,\n",
    "                        dataset_name,\n",
    "                        managed_schema\n",
    "                    ): (partition_index, row)\n",
    "                    for partition_index, row in enumerate(partition_rows, 1)\n",
    "                }\n",
    "\n",
    "                for remediation in as_completed(remediate_paritions):\n",
    "                    execution_id, s3_bucket_name, bucket_prefix, edp_run_id, snapshot_date,partition_path, remediation_status = remediation.result()\n",
    "\n",
    "                    if remediation_status == \"PASS\":\n",
    "                        # Map PASS to 'loaded' for inventory table\n",
    "                        successful_partitions.append((execution_id, s3_bucket_name, bucket_prefix, edp_run_id, snapshot_date, partition_path, \"loaded\"))\n",
    "                        partitions_remediated += 1\n",
    "                        # print(\"pass\")\n",
    "                    else:\n",
    "                        # Map FAIL to 'failed' for inventory table\n",
    "                        failed_partitions.append((execution_id, s3_bucket_name, bucket_prefix, edp_run_id, snapshot_date, partition_path, \"failed\"))\n",
    "                        partitions_failed += 1\n",
    "                        # print(\"fail\")\n",
    "\n",
    "    # Retry for failed partitions\n",
    "    if failed_partitions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Retrying {len(failed_partitions)} failed partitions for dataset: {dataset_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Extract failed partition rows for retry\n",
    "        retry_partition_rows = []\n",
    "        for exec_id, s3_bucket, prefix, run_id, snap_date, part_path, status in failed_partitions:\n",
    "            # Find the original row that matches this failed partition\n",
    "            # for row in partition_rows:\n",
    "            #     if (row['execution_id'] == exec_id and\n",
    "            #         row['partition_path'] == part_path and\n",
    "            #         row['s3_bucket_name'] == s3_bucket and\n",
    "            #         row['bucket_prefix'] == prefix):\n",
    "            #         retry_partition_rows.append(row)\n",
    "            #         break\n",
    "            row = {\n",
    "                    'edp_run_id': run_id,\n",
    "                    'snapshot_date': snap_date,  \n",
    "                    'partition_path': part_path,\n",
    "                    's3_bucket_name': s3_bucket,\n",
    "                    'bucket_prefix': prefix,\n",
    "                    'execution_id': exec_id\n",
    "                }\n",
    "            print (f\"Failed Paritions Retry: {part_path}\")\n",
    "            retry_partition_rows.append(row)    \n",
    "\n",
    "        print(f\"Found {len(retry_partition_rows)} partition rows to retry\")\n",
    "\n",
    "        # Reset failed_partitions list for retry\n",
    "        failed_partitions = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            retry_futures = {\n",
    "                executor.submit(\n",
    "                    process_partition,\n",
    "                    partition_index,\n",
    "                    row,\n",
    "                    len(retry_partition_rows),\n",
    "                    s3_bucket_name,\n",
    "                    bucket_prefix,\n",
    "                    extended_schema,\n",
    "                    dataset_name,\n",
    "                    managed_schema\n",
    "                ): (partition_index, row)\n",
    "                for partition_index, row in enumerate(retry_partition_rows, 1)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(retry_futures):\n",
    "                execution_id, s3_bucket_name, bucket_prefix, edp_run_id, snapshot_date, partition_path, remediation_status = future.result()\n",
    "                \n",
    "                if remediation_status == \"PASS\":\n",
    "                    # Map PASS to 'loaded' for inventory table\n",
    "                    successful_partitions.append((execution_id, s3_bucket_name, bucket_prefix,edp_run_id, snapshot_date, partition_path, \"loaded\"))\n",
    "                    partitions_remediated += 1\n",
    "                    partitions_failed -= 1  # Decrement failed count\n",
    "                    print(f\"✓ Retry successful for partition_path={partition_path}\")\n",
    "                else:\n",
    "                    # Map FAIL to 'failed' for inventory table\n",
    "                    failed_partitions.append((execution_id, s3_bucket_name, bucket_prefix,edp_run_id, snapshot_date, partition_path, \"failed\"))\n",
    "                    # Only add to error_msg if it failed after retry\n",
    "                    error_msg.append(f\"Failed after retry: partition_path={partition_path}\")\n",
    "                    print(f\"✗ Retry failed for partition_path={partition_path}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Retry Complete for {dataset_name}\")\n",
    "        print(f\"Successful after retry: {partitions_remediated}\")\n",
    "        print(f\"Still failed: {partitions_failed}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    successful_partitions_df = spark.createDataFrame(successful_partitions, remediate_status_schema)\n",
    "    failed_partitions_df = spark.createDataFrame(failed_partitions, remediate_status_schema)\n",
    "\n",
    "    remediate_status_df = remediate_status_df.union(successful_partitions_df).union(failed_partitions_df)\n",
    "\n",
    "    inventory_table_name = f\"{catalog}.{schema}.{inventory_table}\"\n",
    "    s3_inventory_table = DeltaTable.forName(spark, inventory_table_name)\n",
    "    \n",
    "    s3_inventory_table.alias(\"target\").merge(\n",
    "        remediate_status_df.alias(\"updates\"),\n",
    "        \"\"\"\n",
    "        target.s3_bucket_name = updates.s3_bucket_name AND\n",
    "        target.bucket_prefix = updates.bucket_prefix AND\n",
    "        target.edp_run_id = updates.edp_run_id AND\n",
    "        target.snapshot_date = updates.snapshot_date\n",
    "        \"\"\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"target.load_status = 'failed'\",\n",
    "        set={\"load_status\": col(\"updates.load_status\")}\n",
    "    ).execute()\n",
    "\n",
    "    if partitions_remediated == partitions_attempted and partitions_failed == 0:\n",
    "        \n",
    "        update_candidates_query = f\"\"\"\n",
    "        UPDATE {catalog}.{schema}.{candidate_table}\n",
    "        SET recon_job_run = NULL\n",
    "         WHERE execution_id = '{execution_id}' \n",
    "          AND table_name = '{dataset_name}'\n",
    "        \"\"\"\n",
    "        # WHERE execution_id IN ({','.join([f\"'{eid}'\" for eid in execution_ids])})\n",
    "\n",
    "        spark.sql(update_candidates_query)\n",
    "        \n",
    "        print(f\"✓ Updated ccbr_migration_table_candidates for table_name='{dataset_name}'\")\n",
    "    else:\n",
    "        print(f\"✗ Not updating candidate table for {dataset_name} as not all partitions loaded successfully\")\n",
    "\n",
    "    remediation_status_rows.append(\n",
    "        Row(\n",
    "            execution_id=execution_id,\n",
    "            s3_bucket_name=s3_bucket_name,\n",
    "            bucket_prefix=bucket_prefix,\n",
    "            dataset_name=dataset_name,\n",
    "            remediation_attempted_time=datetime.now(),\n",
    "            partitions_attempted=partitions_attempted,\n",
    "            partitions_remediated=partitions_remediated,\n",
    "            partitions_failed=partitions_failed,\n",
    "            remediation_status=\"PASS\" if partitions_failed == 0 else \"FAIL\",\n",
    "            error_msg=\"; \".join(error_msg) if error_msg else None\n",
    "        )\n",
    "    )\n",
    "    #  execution_id=\",\".join(execution_ids),\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL PARTITIONS PROCESSED!\")\n",
    "print(f\"✓ Remediation summary written per dataset\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if remediation_status_rows:\n",
    "    schema_df = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"s3_bucket_name\", StringType(), True),\n",
    "        StructField(\"bucket_prefix\", StringType(), True),\n",
    "        StructField(\"dataset_name\", StringType(), True),\n",
    "        StructField(\"remediation_attempted_time\", TimestampType(), True),\n",
    "        StructField(\"partitions_attempted\", IntegerType(), True),\n",
    "        StructField(\"partitions_remediated\", IntegerType(), True),\n",
    "        StructField(\"partitions_failed\", IntegerType(), True),\n",
    "        StructField(\"remediation_status\", StringType(), True),\n",
    "        StructField(\"error_msg\", StringType(), True)\n",
    "    ])\n",
    "    load_remediation_df = spark.createDataFrame(remediation_status_rows, schema_df)\n",
    "    display(load_remediation_df)\n",
    "\n",
    "    load_remediation_df.write\\\n",
    "        .mode(\"append\")\\\n",
    "        .saveAsTable(f\"{catalog}.{schema}.{recon_remediation_table}\")\n",
    "\n",
    "    # Data loaded into recon_remediation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64c759c9-6159-4332-a80a-bb3e5a345408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5099946312198114,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "8_remediation_of_failed_partitions_v1.3",
   "widgets": {
    "bucket_name": {
     "currentValue": "app-id-89055-dep-id-109792-uu-id-6ou03e070iso",
     "nuid": "3f4d20c2-d9a1-47d6-bd0e-1097759e5112",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "bucket_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "bucket_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "candidate_table": {
     "currentValue": "ccbr_migration_table_candidates",
     "nuid": "eabcd6a0-f391-40da-90c9-50f5daeb3df3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "89055_ctg_prod",
     "nuid": "ca911ea9-302c-4f0a-b6f0-20778ad2ad2b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_mapping_table": {
     "currentValue": "ccbr_migration_dataset_mapping",
     "nuid": "df9c3b4c-b894-4d0a-9ccb-f7b8759549c7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "datasets": {
     "currentValue": "'acq_scr_card'",
     "nuid": "52cd245f-530f-4cd6-ac7f-a367bc81e9dc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "datasets",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "datasets",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "inventory_table": {
     "currentValue": "89055_ctg_prod_exp.dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp.ccbr_migration_table_inventory",
     "nuid": "3aad63c4-f241-474c-a6d0-c92578ccf280",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "managed_catalog": {
     "currentValue": "89055_ctg_prod_exp",
     "nuid": "57896485-eea8-4224-99f6-44276cfd9b66",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "managed_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "managed_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_schema_table": {
     "currentValue": "ccbr_migration_parquet_schemas",
     "nuid": "edb08c32-0fca-477b-8708-97e7d952b3c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "partition_audit_table": {
     "currentValue": "89055_ctg_prod.89055_audit_db_hcd_dora_fdl.dataset_tags",
     "nuid": "35286504-84dc-4fcb-b477-a306f72d6a91",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "recon_remediation_table": {
     "currentValue": "ccbr_migration_recon_remediation",
     "nuid": "8550db8b-18df-4ed7-b470-fecf05c59c01",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "recon_remediation_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "recon_remediation_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "89055_ccbr_migration",
     "nuid": "1da762a8-8a5e-4f47-96cb-44b6041f37e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}