{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2140d3b4-776d-4502-80f5-2106b6b3c080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_extract, collect_list, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DecimalType, DateType, TimestampType, BinaryType, ShortType, ByteType\n",
    "from delta.tables import DeltaTable\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import logging\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5eba2e3-d9ce-418b-b263-a4692c063ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\",\"\",\"Catalog\")\n",
    "dbutils.widgets.text(\"schema\",\"\",\"Schema\")\n",
    "dbutils.widgets.text(\"bucket_name\",\"\",\"Bucket Name\")\n",
    "dbutils.widgets.text(\"candidate_table\",\"\",\"Candiate Table\")\n",
    "dbutils.widgets.text(\"parquet_schema_table\",\"\",\"Parquet Schema Table\")\n",
    "dbutils.widgets.text(\"s3_inventory_table\",\"\",\"S3 Inventory Table\")\n",
    "dbutils.widgets.text(\"dataset_mapping_table\",\"\",\"Dataset Mapping Table\")\n",
    "dbutils.widgets.text(\"partition_table\",\"\",\"Partition Table\")\n",
    "dbutils.widgets.text(\"file_counts\",\"\",\"File Counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30e3c1c-d242-4b53-99cc-58ba8387f1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "bucket_name = dbutils.widgets.get(\"bucket_name\")\n",
    "candidate_table = dbutils.widgets.get(\"candidate_table\")\n",
    "parquet_schema_table = dbutils.widgets.get(\"parquet_schema_table\")\n",
    "s3_inventory_table = dbutils.widgets.get(\"s3_inventory_table\")\n",
    "dataset_mapping_table = dbutils.widgets.get(\"dataset_mapping_table\")\n",
    "partition_table = dbutils.widgets.get(\"partition_table\")\n",
    "file_counts = dbutils.widgets.get(\"file_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fea305-4a0c-4316-8722-b7f8172c7f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67e7cfd-b202-4887-b38d-b9f5da6df857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert type strings to Spark types\n",
    "def parse_type(dtype):\n",
    "    dtype = dtype.lower().strip()\n",
    "\n",
    "    if dtype.startswith(\"decimal\"):\n",
    "        scale = dtype[dtype.find(\"(\") + 1 : dtype.find(\")\")].split(\",\")\n",
    "        return DecimalType(int(scale[0]), int(scale[1]))\n",
    "\n",
    "    if dtype in (\"int8\", \"byte\"):\n",
    "        return ByteType()\n",
    "    \n",
    "    if dtype.startswith(\"bytetype\"):\n",
    "        return ByteType()\n",
    "\n",
    "    if dtype in (\"int16\", \"smallint\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype.startswith(\"shorttype\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype in (\"int32\", \"integer\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype.startswith(\"integertype\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype == \"int64\":\n",
    "        return LongType()\n",
    "\n",
    "    if dtype.startswith(\"longtype\"):\n",
    "        return LongType()\n",
    "    \n",
    "    if dtype in (\"string\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"stringtype\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"date32\") or dtype == \"date\":\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"datetype\"):\n",
    "        return DateType()\n",
    "\n",
    "    if dtype.startswith(\"timestamp\"):\n",
    "        # Handles 'timestamp', 'timestamp[ms]', 'timestamp[us]' etc.\n",
    "        return TimestampType()\n",
    "\n",
    "    if dtype == \"bool\":\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype.startswith(\"booleantype\"):\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype == \"binary\":\n",
    "        return BinaryType()\n",
    "    \n",
    "    if dtype.startswith(\"binarytype\"):\n",
    "        return BinaryType()\n",
    "\n",
    "    raise ValueError(f\"Unsupported type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cef7b94-4638-4f4c-992a-09dfab8fa3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a lock for thread-safe Delta writes\n",
    "write_lock = Lock()\n",
    "\n",
    "def process_partition(edp_run_id, snapshot_date, bucket_name, bucket_prefix, s3_path, extended_schema, managed_table_name, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process a single partition: read parquet and write to Delta table with retry logic\n",
    "    \"\"\"\n",
    "    path = f\"{s3_path}edp_run_id={edp_run_id}/snapshot_date={snapshot_date}\"\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading partition path: {path}\")\n",
    "        \n",
    "        filtered_df = (\n",
    "            spark.read\n",
    "            .schema(extended_schema)\n",
    "            .option(\"basePath\", s3_path)\n",
    "            .parquet(path)\n",
    "        )\n",
    "        \n",
    "        # Acquire lock before writing to Delta table to ensure thread-safe writes\n",
    "        with write_lock:\n",
    "            # Write the dataframe to the Delta table (mergeSchema ensures evolution safety)\n",
    "            (\n",
    "                filtered_df.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"append\")\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .saveAsTable(managed_table_name)\n",
    "            )\n",
    "        \n",
    "        print(f\"Successfully processed for {path}\")\n",
    "        return (bucket_name, bucket_prefix, edp_run_id, snapshot_date, \"loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {path}\")\n",
    "        return (bucket_name, bucket_prefix, edp_run_id, snapshot_date, \"failed\")\n",
    "        \n",
    "    return (bucket_name, bucket_prefix, edp_run_id, snapshot_date, \"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fee1570-0527-4320-99ab-a25afbc00ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_table_candidates = spark.sql(f\"\"\"\n",
    "                                select distinct s3_bucket_name, bucket_prefix, table_name\n",
    "                                from  {catalog}.{schema}.{candidate_table}\n",
    "                                where s3_bucket_name = '{bucket_name}' \n",
    "                                and candidate_for_managed_table_creation in ('true', 'false') \n",
    "                                and managed_table_created is null\n",
    "                                and structured_file_count between {file_counts}\n",
    "                                \"\"\")\n",
    "\n",
    "# Collect all rows into Python memory\n",
    "rows = df_table_candidates.collect()\n",
    "\n",
    "inventory_table_name = f\"{catalog}.{schema}.{s3_inventory_table}\"\n",
    "#print(inventory_table_name)\n",
    "\n",
    "inventory_table = DeltaTable.forName(spark, inventory_table_name)\n",
    "\n",
    "for row in rows:\n",
    "    bucket_name = row[\"s3_bucket_name\"]\n",
    "    bucket_prefix = row[\"bucket_prefix\"]\n",
    "    dataset_name = row[\"table_name\"]\n",
    "    \n",
    "    try:\n",
    "        load_status_schema = StructType([\n",
    "            StructField(\"bucket_name\", StringType(), True),\n",
    "            StructField(\"bucket_prefix\", StringType(), True),\n",
    "            StructField(\"edp_run_id\", StringType(), True),\n",
    "            StructField(\"snapshot_date\", DateType(), True),\n",
    "            StructField(\"load_status\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        load_status_df = spark.createDataFrame([], load_status_schema)\n",
    "\n",
    "        partition_key_combination = \"edp_run_id, snapshot_date\"\n",
    "        print(f\"Processing {dataset_name}\")\n",
    "\n",
    "        inventory_df = spark.sql(f\"\"\"SELECT distinct edp_run_id, snapshot_date\n",
    "                                FROM \n",
    "                                (\n",
    "                                    select distinct edp_run_id, try_cast(snapshot_date as date) as snapshot_date\n",
    "                                    from {catalog}.{schema}.{s3_inventory_table}\n",
    "                                    where 1 = 1\n",
    "                                    and extension is not null \n",
    "                                    and lower(partition_key) = '{partition_key_combination}'\n",
    "                                    and s3_bucket_name = '{bucket_name}' \n",
    "                                    and bucket_prefix = '{bucket_prefix}'\n",
    "                                    and load_status is null\n",
    "                                ) inventory\n",
    "                                join\n",
    "                                (\n",
    "                                    select distinct run_id, try_cast(run_tag_value as date) as run_tag_value \n",
    "                                    from {partition_table}\n",
    "                                    where dataset_name = '{dataset_name}'\n",
    "                                    and lower(run_tag_key) = 'snapshot_date'\n",
    "                                ) partition\n",
    "                                on inventory.edp_run_id = partition.run_id\n",
    "                                and inventory.snapshot_date = partition.run_tag_value\n",
    "                                \"\"\")\n",
    "        \n",
    "        df_latest_snapshot = (inventory_df\n",
    "                              .orderBy(col(\"snapshot_date\").desc())\n",
    "                              .limit(1)\n",
    "                              )\n",
    "\n",
    "        latest_snapshot_row = df_latest_snapshot.collect()[0]\n",
    "\n",
    "        latest_edp_run_id = latest_snapshot_row[\"edp_run_id\"]\n",
    "        latest_snapshot_date = latest_snapshot_row[\"snapshot_date\"]\n",
    "\n",
    "        schema_json_df = spark.sql(f\"\"\"select schema_json \n",
    "                                    from {catalog}.{schema}.{parquet_schema_table}\n",
    "                                    where 1 = 1\n",
    "                                    and s3_bucket_name = '{bucket_name}'\n",
    "                                    and bucket_prefix = '{bucket_prefix}'\n",
    "                                    and file_path like '%edp_run_id={latest_edp_run_id}/snapshot_date={latest_snapshot_date}%'\n",
    "                                    \"\"\")\n",
    "\n",
    "        # Extract the schema_json value into a Python string variable\n",
    "        schema_json = schema_json_df.first()['schema_json']\n",
    "        #print(schema_json)\n",
    "\n",
    "        schema_dict = json.loads(schema_json)\n",
    "        #print(f\"schema_dict: {schema_dict}\")\n",
    "\n",
    "        # Convert to StructType\n",
    "        base_schema = StructType([\n",
    "            StructField(f[\"name\"], parse_type(f[\"type\"]), f[\"nullable\"])\n",
    "            for f in schema_dict[\"fields\"]\n",
    "        ])\n",
    "        #print(f\"base_schema: {base_schema}\")\n",
    "\n",
    "        partition_columns = [partition_column.strip() for partition_column in partition_key_combination.split(\",\") if partition_column.strip()]\n",
    "\n",
    "        # You can define mapping for known types here\n",
    "        partition_type_map = {\n",
    "            \"edp_run_id\": StringType(),\n",
    "            \"snapshot_date\": DateType()\n",
    "        }\n",
    "\n",
    "        extended_fields = base_schema.fields.copy()\n",
    "\n",
    "        for partition_column in partition_columns:\n",
    "            col_type = partition_type_map.get(partition_column, StringType())  # default to STRING\n",
    "            extended_fields.append(StructField(partition_column, col_type, True))\n",
    "\n",
    "        extended_schema = StructType(extended_fields)\n",
    "        #print(extended_schema)\n",
    "\n",
    "        target_df = spark.sql(f\"\"\"select \n",
    "                            concat(dbx_catalog, '.', dbx_managed_table_schema, '.', dataset_name) as managed_table_name,\n",
    "                            concat('s3://', s3_bucket_name, '/', bucket_prefix) as s3_path\n",
    "                            from {catalog}.{schema}.{dataset_mapping_table}\n",
    "                            where 1 = 1\n",
    "                            and s3_bucket_name = '{bucket_name}'\n",
    "                            and bucket_prefix = '{bucket_prefix}'\n",
    "                            \"\"\")\n",
    "\n",
    "        \n",
    "        target_row = target_df.first()\n",
    "        \n",
    "        if target_row:\n",
    "            managed_table_name, s3_path = target_row[\"managed_table_name\"], target_row[\"s3_path\"]\n",
    "        \n",
    "        #Check if tracker has partitions for this dataset\n",
    "        if inventory_df.count() == 0: \n",
    "            print(\"No files found — No data to write.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Files identified — Starting the data laad.\")\n",
    "            print(f\"Total Partition to Load: {inventory_df.count()}\")\n",
    "            # Collect partition values as tuples\n",
    "            partition_filters = [(row.edp_run_id, row.snapshot_date) for row in inventory_df.collect()]\n",
    "            print(f\"Creating Table: {managed_table_name}\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {managed_table_name} (\n",
    "                    edp_run_id string,\n",
    "                    snapshot_date date\n",
    "                )\n",
    "                USING DELTA\n",
    "                TBLPROPERTIES (\n",
    "                    'delta.columnMapping.mode' = 'name',\n",
    "                    'delta.enableIcebergCompatV2' = 'true',\n",
    "                    'delta.universalFormat.enabledFormats' = 'iceberg'\n",
    "                )\n",
    "                cluster by (edp_run_id, snapshot_date)\n",
    "            \"\"\")\n",
    "\n",
    "            # ============================================================\n",
    "            # PARALLELIZED PARTITION PROCESSING WITH 40 THREADS\n",
    "            # ============================================================\n",
    "        \n",
    "            max_workers = 40  # 40 threads for parallel processing\n",
    "            successful_partitions = []\n",
    "            failed_partitions = []\n",
    "            \n",
    "            print(f\"Starting parallel processing of {len(partition_filters)} partitions with {max_workers} threads\")\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                # Submit all partition processing tasks\n",
    "                future_to_partition = {\n",
    "                    executor.submit(\n",
    "                        process_partition,\n",
    "                        edp_run_id,\n",
    "                        snapshot_date,\n",
    "                        bucket_name,\n",
    "                        bucket_prefix,\n",
    "                        s3_path,\n",
    "                        extended_schema,\n",
    "                        managed_table_name\n",
    "                    ): (edp_run_id, snapshot_date)\n",
    "                    for edp_run_id, snapshot_date in partition_filters\n",
    "                }\n",
    "                \n",
    "                # Process completed tasks as they finish\n",
    "                completed = 0\n",
    "                for future in as_completed(future_to_partition):\n",
    "                    bucket_name, bucket_prefix, edp_run_id, snapshot_date, load_status = future.result()\n",
    "                    completed += 1\n",
    "                    \n",
    "                    if load_status == \"loaded\":\n",
    "                        successful_partitions.append((bucket_name, bucket_prefix, edp_run_id, snapshot_date, load_status))\n",
    "                    else:\n",
    "                        failed_partitions.append((bucket_name, bucket_prefix, edp_run_id, snapshot_date, load_status))\n",
    "                    \n",
    "                    # Progress update every 50 partitions\n",
    "                    if completed % 50 == 0 or completed == len(partition_filters):\n",
    "                        print(f\"Progress: {completed}/{len(partition_filters)} partitions processed\")\n",
    "            \n",
    "            \n",
    "            successful_partitions_df = spark.createDataFrame(successful_partitions, load_status_schema)\n",
    "            failed_partitions_df = spark.createDataFrame(failed_partitions, load_status_schema)\n",
    "\n",
    "            load_status_df = load_status_df.union(successful_partitions_df).union(failed_partitions_df)\n",
    "            #display(load_status_df)\n",
    "\n",
    "            # Perform merge to update the target table\n",
    "            inventory_table.alias(\"target\").merge(\n",
    "                load_status_df.alias(\"updates\"),\n",
    "                \"\"\"\n",
    "                target.s3_bucket_name = updates.bucket_name AND\n",
    "                target.bucket_prefix = updates.bucket_prefix AND\n",
    "                target.edp_run_id = updates.edp_run_id AND\n",
    "                target.snapshot_date = updates.snapshot_date\n",
    "                \"\"\"\n",
    "            ).whenMatchedUpdate(\n",
    "                condition=\"target.load_status is null\",\n",
    "                set={\"load_status\": col(\"updates.load_status\")}\n",
    "            ).execute()\n",
    "\n",
    "\n",
    "            # Summary\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"Processing Complete for {dataset_name}\")\n",
    "            print(f\"Total partitions: {len(partition_filters)}\")\n",
    "            print(f\"Successful: {len(successful_partitions)}\")\n",
    "            print(f\"Failed: {len(failed_partitions)}\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            spark.sql(f\"\"\"update {catalog}.{schema}.{candidate_table}\n",
    "                    set managed_table_created = 'true'\n",
    "                    where 1 = 1\n",
    "                    and managed_table_created is null\n",
    "                    and s3_bucket_name = '{bucket_name}'\n",
    "                    and bucket_prefix = '{bucket_prefix}'\"\"\")\n",
    "            \n",
    "            print(f\"Migration Completed\")\n",
    "\n",
    "    except Exception as e: \n",
    "        error_message = str(e)\n",
    "        stack = traceback.format_exc()\n",
    "        print(f\"Failed to process {dataset_name}: {error_message}\")\n",
    "        spark.sql(f\"\"\"update {catalog}.{schema}.{candidate_table}\n",
    "                    set error_message = 'Failed to Process'\n",
    "                    where 1 = 1\n",
    "                    and managed_table_created is null\n",
    "                    and s3_bucket_name = '{bucket_name}'\n",
    "                    and bucket_prefix = '{bucket_prefix}'\"\"\")\n",
    "        continue  # move to next dataset    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4541416149735695,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5_migrate_from_s3_to_managed_tables_v2",
   "widgets": {
    "bucket_name": {
     "currentValue": "app-id-89055-dep-id-109792-uu-id-6ou03e070iso",
     "nuid": "3ba5efe6-618c-4949-aaa9-95c94ed9d6b3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Bucket Name",
      "name": "bucket_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Bucket Name",
      "name": "bucket_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "candidate_table": {
     "currentValue": "ccbr_migration_table_candidates",
     "nuid": "3f07f855-53d6-42ef-b129-9694850a4aa7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Candiate Table",
      "name": "candidate_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Candiate Table",
      "name": "candidate_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "89055_ctg_prod_exp",
     "nuid": "fcef1a8c-e20d-4709-b6f4-2fc89ba1cdb8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_mapping_table": {
     "currentValue": "ccbr_migration_dataset_mapping",
     "nuid": "dfcbbb83-1ca7-470b-82e9-4e35349334c8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Dataset Mapping Table",
      "name": "dataset_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Dataset Mapping Table",
      "name": "dataset_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_schema_table": {
     "currentValue": "ccbr_migration_parquet_schemas",
     "nuid": "87b180a8-044d-46fd-9cad-aa52e968315c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Parquet Schema Table",
      "name": "parquet_schema_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Parquet Schema Table",
      "name": "parquet_schema_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "partition_table": {
     "currentValue": "89055_ctg_prod_exp.dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp.ccbr_migration_partition_audit_table",
     "nuid": "72d50076-60e5-44f5-b778-713a78d760c1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Partition Table",
      "name": "partition_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Partition Table",
      "name": "partition_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_bucket_volume_mapping_table": {
     "currentValue": "ccbr_migration_s3_bucket_volume_mapping",
     "nuid": "ce3a8e01-79ed-429e-bfdf-a673a8b30f86",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Bucket To Mapping Table",
      "name": "s3_bucket_volume_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Bucket To Mapping Table",
      "name": "s3_bucket_volume_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_inventory_table": {
     "currentValue": "ccbr_migration_table_inventory",
     "nuid": "358acedb-c84b-4614-96c3-2b3d28bf4241",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "S3 Inventory Table",
      "name": "s3_inventory_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "S3 Inventory Table",
      "name": "s3_inventory_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp",
     "nuid": "3ee841c6-3cde-419d-a0a2-09b7f8c7244c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}