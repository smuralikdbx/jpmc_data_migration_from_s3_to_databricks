{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2140d3b4-776d-4502-80f5-2106b6b3c080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_extract, collect_list, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DecimalType, DateType, TimestampType, BinaryType, ShortType, ByteType\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import logging\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5eba2e3-d9ce-418b-b263-a4692c063ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\",\"\",\"Catalog\")\n",
    "dbutils.widgets.text(\"schema\",\"\",\"Schema\")\n",
    "dbutils.widgets.text(\"bucket_name\",\"\",\"Bucket Name\")\n",
    "dbutils.widgets.text(\"candidate_table\",\"\",\"Candiate Table\")\n",
    "dbutils.widgets.text(\"parquet_schema_table\",\"\",\"Parquet Schema Table\")\n",
    "dbutils.widgets.text(\"s3_inventory_table\",\"\",\"S3 Inventory Table\")\n",
    "dbutils.widgets.text(\"dataset_mapping_table\",\"\",\"Dataset Mapping Table\")\n",
    "dbutils.widgets.text(\"s3_bucket_volume_mapping_table\",\"\",\"Bucket To Mapping Table\")\n",
    "dbutils.widgets.text(\"partition_table\",\"\",\"Partition Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30e3c1c-d242-4b53-99cc-58ba8387f1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "bucket_name = dbutils.widgets.get(\"bucket_name\")\n",
    "candidate_table = dbutils.widgets.get(\"candidate_table\")\n",
    "parquet_schema_table = dbutils.widgets.get(\"parquet_schema_table\")\n",
    "s3_inventory_table = dbutils.widgets.get(\"s3_inventory_table\")\n",
    "dataset_mapping_table = dbutils.widgets.get(\"dataset_mapping_table\")\n",
    "s3_bucket_volume_mapping_table = dbutils.widgets.get(\"s3_bucket_volume_mapping_table\")\n",
    "partition_table = dbutils.widgets.get(\"partition_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fea305-4a0c-4316-8722-b7f8172c7f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95149313-5754-4b74-99b2-0e32e20ccf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert type strings to Spark types\n",
    "def parse_type(dtype):\n",
    "    dtype = dtype.lower().strip()\n",
    "\n",
    "    if dtype.startswith(\"decimal\"):\n",
    "        scale = dtype[dtype.find(\"(\") + 1 : dtype.find(\")\")].split(\",\")\n",
    "        return DecimalType(int(scale[0]), int(scale[1]))\n",
    "\n",
    "    if dtype in (\"int8\", \"byte\"):\n",
    "        return ByteType()\n",
    "\n",
    "    if dtype in (\"int16\", \"smallint\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype in (\"int32\", \"integer\"):\n",
    "        return IntegerType()\n",
    "\n",
    "    if dtype == \"int64\":\n",
    "        return LongType()\n",
    "\n",
    "    if dtype in (\"string\",):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"date32\") or dtype == \"date\":\n",
    "        return DateType()\n",
    "\n",
    "    if dtype.startswith(\"timestamp\"):\n",
    "        # Handles 'timestamp', 'timestamp[ms]', 'timestamp[us]' etc.\n",
    "        return TimestampType()\n",
    "\n",
    "    if dtype == \"bool\":\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype == \"binary\":\n",
    "        return BinaryType()\n",
    "\n",
    "    raise ValueError(f\"Unsupported type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cef7b94-4638-4f4c-992a-09dfab8fa3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a lock for thread-safe Delta writes\n",
    "write_lock = Lock()\n",
    "\n",
    "def process_partition(edp_run_id, snapshot_date, volume_path, extended_schema, managed_table_name, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process a single partition: read parquet and write to Delta table with retry logic\n",
    "    \"\"\"\n",
    "    path = f\"{volume_path}edp_run_id={edp_run_id}/snapshot_date={snapshot_date}\"\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Reading partition path: {path}\")\n",
    "            \n",
    "            filtered_df = (\n",
    "                spark.read\n",
    "                .schema(extended_schema)\n",
    "                .option(\"basePath\", volume_path)\n",
    "                .parquet(path)\n",
    "            )\n",
    "            \n",
    "            # Acquire lock before writing to Delta table to ensure thread-safe writes\n",
    "            with write_lock:\n",
    "                # Write the dataframe to the Delta table (mergeSchema ensures evolution safety)\n",
    "                (\n",
    "                    filtered_df.write\n",
    "                    .format(\"delta\")\n",
    "                    .mode(\"append\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .saveAsTable(managed_table_name)\n",
    "                )\n",
    "            \n",
    "            print(f\"Successfully processed for {path}\")\n",
    "            return (edp_run_id, snapshot_date, \"SUCCESS\", None)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing partition {path} (Attempt {attempt}/{max_retries}): {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            if attempt < max_retries:\n",
    "                wait_time = attempt * 2  # Exponential backoff\n",
    "                print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to process {path} after {max_retries} attempts\")\n",
    "                return (edp_run_id, snapshot_date, \"FAILED\", str(e))\n",
    "    \n",
    "    return (edp_run_id, snapshot_date, \"FAILED\", \"Unknown error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fee1570-0527-4320-99ab-a25afbc00ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_table_candidates = spark.sql(f\"\"\"\n",
    "                                select distinct s3_bucket_name, bucket_prefix, table_name\n",
    "                                from  {catalog}.{schema}.{candidate_table}\n",
    "                                where s3_bucket_name = '{bucket_name}' \n",
    "                                and candidate_for_managed_table_creation in ('true', 'false') \n",
    "                                and managed_table_created is null\n",
    "                                \"\"\")\n",
    "\n",
    "# Collect all rows into Python memory\n",
    "rows = df_table_candidates.collect()\n",
    "\n",
    "for row in rows:\n",
    "    bucket_name = row[\"s3_bucket_name\"]\n",
    "    bucket_prefix = row[\"bucket_prefix\"]\n",
    "    dataset_name = row[\"table_name\"]\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        partition_key_combination = \"edp_run_id, snapshot_date\"\n",
    "        print(f\"Processing {dataset_name}\")\n",
    "\n",
    "        partition_key_df = spark.sql(f\"\"\"select partition_key from {catalog}.{schema}.{s3_inventory_table}\n",
    "                                        where 1 = 1\n",
    "                                        and extension = 'parquet' \n",
    "                                        and s3_bucket_name = '{bucket_name}' \n",
    "                                        and bucket_prefix = '{bucket_prefix}' \n",
    "                                        and  partition_key = '{partition_key_combination}'\n",
    "                                        and load_status is null\n",
    "                                        and last_modified_time = (select max(last_modified_time) from {catalog}.{schema}.{s3_inventory_table} \n",
    "                                        where 1 = 1\n",
    "                                        and extension = 'parquet' \n",
    "                                        and s3_bucket_name = '{bucket_name}' \n",
    "                                        and bucket_prefix = '{bucket_prefix}'\n",
    "                                        and partition_key = '{partition_key_combination}'\n",
    "                                        and load_status is null)\n",
    "                                        limit 1\n",
    "                                    \"\"\")\n",
    "        \n",
    "        partition_key_str = partition_key_df.first()[\"partition_key\"]\n",
    "        #print(partition_key_str)\n",
    "\n",
    "        #Discover all available partitions from the base parquet dataset\n",
    "        # print(\"Scanning dataset for available partitions\")\n",
    "        # all_partitions_df = (\n",
    "        #     spark.read\n",
    "        #     .option(\"basePath\", volume_path)\n",
    "        #     .parquet(volume_path)\n",
    "        #     .select(\"edp_run_id\", \"snapshot_date\")\n",
    "        #     .distinct()\n",
    "        # )\n",
    "        #display(all_partitions_df)\n",
    "\n",
    "        inventory_df = spark.sql(f\"\"\"SELECT distinct inventory.key, buc_vol.volume_name\n",
    "                                ,CONCAT(buc_vol.volume_name, '/', inventory.key) AS volume_path_key \n",
    "                                FROM \n",
    "                                (select distinct s3_bucket_name, bucket_prefix, key \n",
    "                                from {catalog}.{schema}.{s3_inventory_table}\n",
    "                                where 1 = 1\n",
    "                                and extension = 'parquet' \n",
    "                                and partition_key = '{partition_key_str}'\n",
    "                                and s3_bucket_name = '{bucket_name}' \n",
    "                                and bucket_prefix = '{bucket_prefix}'\n",
    "                                and load_status is null) inventory\n",
    "                                join \n",
    "                                (select s3_bucket_name, volume_name from  {catalog}.{schema}.{s3_bucket_volume_mapping_table}\n",
    "                                where 1 = 1\n",
    "                                and s3_bucket_name = '{bucket_name}') buc_vol\n",
    "                                on inventory.s3_bucket_name = buc_vol.s3_bucket_name\n",
    "                                \"\"\")\n",
    "        \n",
    "        #display(inventory_df)\n",
    "        inventory_df = (\n",
    "            inventory_df\n",
    "            # Must have edp_run_id=.../snapshot_date=.../ structure\n",
    "            .filter(col(\"volume_path_key\").rlike(r\"/edp_run_id=[^/]+/snapshot_date=[^/]+/\"))\n",
    "            # Exclude files that have multiple snapshot_date entries\n",
    "            .filter(~col(\"volume_path_key\").rlike(r\"snapshot_date=[^/]+/.*snapshot_date=\"))\n",
    "        )\n",
    "        #display(inventory_df)\n",
    "\n",
    "        inventory_df = (\n",
    "            inventory_df\n",
    "            .withColumn(\"edp_run_id\", regexp_extract(col(\"volume_path_key\"), r\"edp_run_id=([^/]+)\", 1))\n",
    "            .withColumn(\"snapshot_date\", regexp_extract(col(\"volume_path_key\"), r\"snapshot_date=([^/]+)\", 1))\n",
    "            .select(\"edp_run_id\", \"snapshot_date\").distinct()\n",
    "        )\n",
    "\n",
    "\n",
    "        schema_json_df = spark.sql(f\"\"\"select schema_json \n",
    "                                    from {catalog}.{schema}.{parquet_schema_table}\n",
    "                                    where 1 = 1\n",
    "                                    and s3_bucket_name = '{bucket_name}'\n",
    "                                    and bucket_prefix = '{bucket_prefix}'\n",
    "                                    and last_modified_time = (select max(last_modified_time) \n",
    "                                                            from {catalog}.{schema}.{parquet_schema_table} \n",
    "                                                            where 1 = 1\n",
    "                                                            and s3_bucket_name = '{bucket_name}'\n",
    "                                                            and bucket_prefix = '{bucket_prefix}')\n",
    "                                    limit 1\n",
    "                                    \"\"\")\n",
    "        \n",
    "        # Extract the schema_json value into a Python string variable\n",
    "        schema_json = schema_json_df.first()['schema_json']\n",
    "\n",
    "        schema_dict = json.loads(schema_json)\n",
    "\n",
    "        # Convert to StructType\n",
    "        base_schema = StructType([\n",
    "            StructField(f[\"name\"], parse_type(f[\"type\"]), f[\"nullable\"])\n",
    "            for f in schema_dict[\"fields\"]\n",
    "        ])\n",
    "\n",
    "        partition_columns = [partition_column.strip() for partition_column in partition_key_str.split(\",\") if partition_column.strip()]\n",
    "\n",
    "        # You can define mapping for known types here\n",
    "        partition_type_map = {\n",
    "            \"edp_run_id\": StringType(),\n",
    "            \"snapshot_date\": DateType()\n",
    "        }\n",
    "\n",
    "        extended_fields = base_schema.fields.copy()\n",
    "\n",
    "        for partition_column in partition_columns:\n",
    "            col_type = partition_type_map.get(partition_column, StringType())  # default to STRING\n",
    "            extended_fields.append(StructField(partition_column, col_type, True))\n",
    "\n",
    "        extended_schema = StructType(extended_fields)\n",
    "        #print(extended_schema)\n",
    "\n",
    "        target_df = spark.sql(f\"\"\"select \n",
    "                            concat(dbx_catalog, '.', dbx_managed_table_schema, '.', dataset_name) as managed_table_name,\n",
    "                            concat(buc_vol.volume_name, '/', bucket_prefix) as volume_path\n",
    "                            from {catalog}.{schema}.{dataset_mapping_table} src_trgt\n",
    "                            inner join {catalog}.{schema}.{s3_bucket_volume_mapping_table} buc_vol\n",
    "                            on src_trgt.s3_bucket_name = buc_vol.s3_bucket_name\n",
    "                            where 1 = 1\n",
    "                            and src_trgt.s3_bucket_name = '{bucket_name}'\n",
    "                            and src_trgt.bucket_prefix = '{bucket_prefix}'\n",
    "                            \"\"\")\n",
    "        target_row = target_df.first()\n",
    "        \n",
    "        if target_row:\n",
    "            managed_table_name, volume_path = target_row[\"managed_table_name\"], target_row[\"volume_path\"]\n",
    "        #print(volume_path)\n",
    "        \n",
    "        # 1️Read partition tracker\n",
    "        partition_df = spark.sql(f\"\"\"\n",
    "                                select run_id, run_tag_value from {partition_table}\n",
    "                                where dataset_name = '{dataset_name}'\n",
    "                                and lower(run_tag_key) = 'snapshot_date'\n",
    "                                \"\"\")\n",
    "        #display(partition_df)\n",
    "\n",
    "        dataset_snapshot_col = \"snapshot_date\"\n",
    "        dataset_run_id_col = \"edp_run_id\"\n",
    "\n",
    "        tracker_snapshot_col = \"run_tag_value\"\n",
    "        tracker_run_id_col = \"run_id\"\n",
    "\n",
    "        # Align tracker columns to dataset schema\n",
    "        partition_df_renamed = (\n",
    "            partition_df\n",
    "            .withColumnRenamed(tracker_snapshot_col, dataset_snapshot_col)\n",
    "            .withColumnRenamed(tracker_run_id_col, dataset_run_id_col)\n",
    "            .select(dataset_run_id_col, dataset_snapshot_col)\n",
    "            .distinct()\n",
    "        )\n",
    "        #display(partition_df_renamed)\n",
    "\n",
    "        # Perform join to get files for blessed partitions only\n",
    "        print(\"Fetching files for blessed partitions\")\n",
    "        inventory_filtered_df = (\n",
    "            inventory_df.alias(\"a\")\n",
    "            .join(\n",
    "                partition_df_renamed.alias(\"b\"),\n",
    "                (col(\"a.snapshot_date\") == col(\"b.snapshot_date\")) &\n",
    "                (col(\"a.edp_run_id\") == col(\"b.edp_run_id\")),\n",
    "                how=\"inner\"\n",
    "            )\n",
    "            .select(\"a.edp_run_id\", \"a.snapshot_date\").distinct()\n",
    "        )\n",
    "        #display(inventory_filtered_df)\n",
    "\n",
    "        #Check if tracker has partitions for this dataset\n",
    "        if partition_df.count() == 0: \n",
    "            print(\"No partition information found in tracker — No data to write.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Partition information found — reading only matching partitions.\")\n",
    "            print(f\"Total Partition to Load: {inventory_filtered_df.count()}\")\n",
    "            # Collect partition values as tuples\n",
    "            partition_filters = [(row.edp_run_id, row.snapshot_date) for row in inventory_filtered_df.collect()]\n",
    "            print(f\"Creating Table: {managed_table_name}\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {managed_table_name} (\n",
    "                    edp_run_id string,\n",
    "                    snapshot_date date\n",
    "                )\n",
    "                USING DELTA\n",
    "                TBLPROPERTIES (\n",
    "                    'delta.columnMapping.mode' = 'name',\n",
    "                    'delta.enableIcebergCompatV2' = 'true',\n",
    "                    'delta.universalFormat.enabledFormats' = 'iceberg'\n",
    "                )\n",
    "                cluster by (edp_run_id, snapshot_date)\n",
    "            \"\"\")\n",
    "\n",
    "            # ============================================================\n",
    "            # PARALLELIZED PARTITION PROCESSING WITH 40 THREADS\n",
    "            # ============================================================\n",
    "        \n",
    "            max_workers = 40  # 40 threads for parallel processing\n",
    "            successful_partitions = []\n",
    "            failed_partitions = []\n",
    "            \n",
    "            print(f\"Starting parallel processing of {len(partition_filters)} partitions with {max_workers} threads\")\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                # Submit all partition processing tasks\n",
    "                future_to_partition = {\n",
    "                    executor.submit(\n",
    "                        process_partition,\n",
    "                        edp_run_id,\n",
    "                        snapshot_date,\n",
    "                        volume_path,\n",
    "                        extended_schema,\n",
    "                        managed_table_name\n",
    "                    ): (edp_run_id, snapshot_date)\n",
    "                    for edp_run_id, snapshot_date in partition_filters\n",
    "                }\n",
    "                \n",
    "                # Process completed tasks as they finish\n",
    "                completed = 0\n",
    "                for future in as_completed(future_to_partition):\n",
    "                    edp_run_id, snapshot_date, status, error = future.result()\n",
    "                    completed += 1\n",
    "                    \n",
    "                    if status == \"SUCCESS\":\n",
    "                        successful_partitions.append((edp_run_id, snapshot_date))\n",
    "                    else:\n",
    "                        failed_partitions.append((edp_run_id, snapshot_date, error))\n",
    "                    \n",
    "                    # Progress update every 50 partitions\n",
    "                    if completed % 50 == 0 or completed == len(partition_filters):\n",
    "                        print(f\"Progress: {completed}/{len(partition_filters)} partitions processed\")\n",
    "            \n",
    "            # Retry failed partitions if any\n",
    "            if failed_partitions:\n",
    "                print(f\"\\nRetrying {len(failed_partitions)} failed partitions...\")\n",
    "                retry_filters = [(edp_run_id, snapshot_date) for edp_run_id, snapshot_date, _ in failed_partitions]\n",
    "                failed_partitions = []  # Reset\n",
    "                \n",
    "                with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    future_to_partition = {\n",
    "                        executor.submit(\n",
    "                            process_partition,\n",
    "                            edp_run_id,\n",
    "                            snapshot_date,\n",
    "                            volume_path,\n",
    "                            extended_schema,\n",
    "                            managed_table_name\n",
    "                        ): (edp_run_id, snapshot_date)\n",
    "                        for edp_run_id, snapshot_date in retry_filters\n",
    "                    }\n",
    "                    \n",
    "                    for future in as_completed(future_to_partition):\n",
    "                        edp_run_id, snapshot_date, status, error = future.result()\n",
    "                        \n",
    "                        if status == \"SUCCESS\":\n",
    "                            successful_partitions.append((edp_run_id, snapshot_date))\n",
    "                        else:\n",
    "                            failed_partitions.append((edp_run_id, snapshot_date, error))\n",
    "            \n",
    "            # Summary\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"Processing Complete for {dataset_name}\")\n",
    "            print(f\"Total partitions: {len(partition_filters)}\")\n",
    "            print(f\"Successful: {len(successful_partitions)}\")\n",
    "            print(f\"Failed: {len(failed_partitions)}\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            spark.sql(f\"\"\"update {catalog}.{schema}.{candidate_table}\n",
    "                    set managed_table_created = 'true'\n",
    "                    where 1 = 1\n",
    "                    and managed_table_created is null\n",
    "                    and s3_bucket_name = '{bucket_name}'\n",
    "                    and bucket_prefix = '{bucket_prefix}'\"\"\")\n",
    "            \n",
    "            spark.sql(f\"\"\"update {catalog}.{schema}.{s3_inventory_table}\n",
    "                    set load_status = 'loaded'\n",
    "                    where 1 = 1\n",
    "                    and load_status is null\n",
    "                    and extension = 'parquet'\n",
    "                    and s3_bucket_name = '{bucket_name}'\n",
    "                    and bucket_prefix = '{bucket_prefix}'\"\"\")\n",
    "            \n",
    "            if failed_partitions:\n",
    "                print(\"\\nFailed partitions:\")\n",
    "                for edp_run_id, snapshot_date, error in failed_partitions:\n",
    "                    print(f\"  - edp_run_id={edp_run_id}, snapshot_date={snapshot_date}: {error}\")\n",
    "                \n",
    "                raise Exception(f\"Failed to process {len(failed_partitions)} partitions. Check logs for details.\")\n",
    "            else:\n",
    "                print(f\"All partitions processed successfully!\")\n",
    "\n",
    "    except Exception as e: \n",
    "        error_message = str(e)\n",
    "        stack = traceback.format_exc()\n",
    "        print(f\"Failed to process {dataset_name}: {error_message}\")\n",
    "        spark.sql(f\"\"\"update {catalog}.{schema}.{candidate_table}\n",
    "                    set error_message = 'Failed to Process'\n",
    "                    where 1 = 1\n",
    "                    and managed_table_created is null\n",
    "                    and s3_bucket_name = '{bucket_name}'\n",
    "                    and bucket_prefix = '{bucket_prefix}'\"\"\")\n",
    "        continue  # move to next dataset    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4541416149735695,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5_migrate_from_s3_to_managed_tables_v1",
   "widgets": {
    "bucket_name": {
     "currentValue": "app-id-89055-dep-id-109792-uu-id-wj1in46hrkbl",
     "nuid": "3ba5efe6-618c-4949-aaa9-95c94ed9d6b3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Bucket Name",
      "name": "bucket_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Bucket Name",
      "name": "bucket_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "bucket_to_volume_mapping_view": {
     "currentValue": "ccbr_migration_s3_bucket_volume_mapping",
     "nuid": "ae4b7976-4bea-4ed6-8ca4-bee309ceea3b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Bucket To Volume Mapping View",
      "name": "bucket_to_volume_mapping_view",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Bucket To Volume Mapping View",
      "name": "bucket_to_volume_mapping_view",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "candidate_table": {
     "currentValue": "ccbr_migration_table_candidates",
     "nuid": "3f07f855-53d6-42ef-b129-9694850a4aa7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Candiate Table",
      "name": "candidate_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Candiate Table",
      "name": "candidate_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "89055_ctg_prod_exp",
     "nuid": "fcef1a8c-e20d-4709-b6f4-2fc89ba1cdb8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_mapping_table": {
     "currentValue": "ccbr_migration_dataset_mapping",
     "nuid": "dfcbbb83-1ca7-470b-82e9-4e35349334c8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Dataset Mapping Table",
      "name": "dataset_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Dataset Mapping Table",
      "name": "dataset_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_schema_table": {
     "currentValue": "ccbr_migration_parquet_schemas",
     "nuid": "87b180a8-044d-46fd-9cad-aa52e968315c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Parquet Schema Table",
      "name": "parquet_schema_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Parquet Schema Table",
      "name": "parquet_schema_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "partition_table": {
     "currentValue": "ccbr_migration_partition_audit_table",
     "nuid": "72d50076-60e5-44f5-b778-713a78d760c1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Partition Table",
      "name": "partition_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Partition Table",
      "name": "partition_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_bucket_volume_mapping_table": {
     "currentValue": "ccbr_migration_s3_bucket_volume_mapping",
     "nuid": "ce3a8e01-79ed-429e-bfdf-a673a8b30f86",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Bucket To Mapping Table",
      "name": "s3_bucket_volume_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Bucket To Mapping Table",
      "name": "s3_bucket_volume_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_inventory_table": {
     "currentValue": "ccbr_migration_table_inventory",
     "nuid": "358acedb-c84b-4614-96c3-2b3d28bf4241",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "S3 Inventory Table",
      "name": "s3_inventory_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "S3 Inventory Table",
      "name": "s3_inventory_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp",
     "nuid": "3ee841c6-3cde-419d-a0a2-09b7f8c7244c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}