{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2bad7850-5860-4bec-a66d-40a9803a97f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter Setup and Configuration\n",
    "# dbutils.widgets.text(\"catalog\", \"\") #smuralik_catalog\n",
    "# dbutils.widgets.text(\"schema\", \"\") #jpmc_ccbr\n",
    "dbutils.widgets.text(\"candidate_table\", \"\") #ccbr_migration_table_candidates\n",
    "dbutils.widgets.text(\"partition_audit_table\",\"\") #89055_ctg_prod_exp.default.dataset_tags\n",
    "dbutils.widgets.text(\"dataset_name\",\"\") \n",
    "dbutils.widgets.text(\"parquet_schema_table\",\"\")\n",
    "dbutils.widgets.text(\"inventory_table\",\"\") #ccbr_migration_table_inventory\n",
    "dbutils.widgets.text(\"managed_table\",\"\")\n",
    "dbutils.widgets.text(\"schema_comparision_results\",\"\")\n",
    "dbutils.widgets.text(\"dataset_mapping_table\",\"\")\n",
    "\n",
    "# CATALOG_NAME = dbutils.widgets.get(\"catalog\")\n",
    "# SCHEMA_NAME = dbutils.widgets.get(\"schema\")\n",
    "parquet_schema_table = dbutils.widgets.get(\"parquet_schema_table\")\n",
    "inventory_table = dbutils.widgets.get(\"inventory_table\")\n",
    "dataset_name=dbutils.widgets.get(\"dataset_name\")\n",
    "managed_table=dbutils.widgets.get(\"managed_table\")\n",
    "candidate_table=dbutils.widgets.get(\"candidate_table\")\n",
    "partition_audit_table=dbutils.widgets.get(\"partition_audit_table\")\n",
    "schema_comparision_results=dbutils.widgets.get(\"schema_comparision_results\")\n",
    "dataset_mapping_table=dbutils.widgets.get(\"dataset_mapping_table\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 89055_ctg_prod_managed.89055_trusted_db_hl_hm_loan_orgn_hcd_dora_fdl.vls_rt_acct_ext_m (done)\n",
    "# 89055_ctg_prod_managed.89055_trusted_db_hl_hm_loan_srvc_hcd_dora_fdl.alsc_rt_acct_fee_fin_m (done)\n",
    "# 89055_ctg_prod_managed.89055_trusted_db_hl_hm_loan_srvc_hcd_dora_fdl.rmi_loan_dim (done)\n",
    "# 89055_ctg_prod_managed.89055_trusted_db_hl_hm_loan_srvc_hcd_dora_fdl.rmi_loan_mo_fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e1f48ea8-a293-4839-9382-15c644d878b1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_extract, collect_list, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DecimalType, DateType, TimestampType, BinaryType, ShortType, ByteType\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d27e82e-bd53-49ce-825d-a64b39b1febe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defination"
    }
   },
   "outputs": [],
   "source": [
    "# this method is used for getting the base schema \n",
    "def parse_type(dtype):\n",
    "    dtype = dtype.lower().strip()\n",
    "\n",
    "    if dtype.startswith(\"decimal\"):\n",
    "        scale = dtype[dtype.find(\"(\") + 1 : dtype.find(\")\")].split(\",\")\n",
    "        return DecimalType(int(scale[0]), int(scale[1]))\n",
    "\n",
    "    if dtype in (\"int8\", \"byte\"):\n",
    "        return ByteType()\n",
    "    \n",
    "    if dtype.startswith(\"bytetype\"):\n",
    "        return ByteType()\n",
    "\n",
    "    if dtype in (\"int16\", \"smallint\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype.startswith(\"shorttype\"):\n",
    "        return ShortType()\n",
    "    \n",
    "    if dtype in (\"int32\", \"integer\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype.startswith(\"integertype\"):\n",
    "        return IntegerType()\n",
    "    \n",
    "    if dtype == \"int64\":\n",
    "        return LongType()\n",
    "\n",
    "    if dtype.startswith(\"longtype\"):\n",
    "        return LongType()\n",
    "    \n",
    "    if dtype in (\"string\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"stringtype\"):\n",
    "        return StringType()\n",
    "\n",
    "    if dtype.startswith(\"date32\") or dtype == \"date\":\n",
    "        return DateType()\n",
    "    \n",
    "    if dtype.startswith(\"datetype\"):\n",
    "        return DateType()\n",
    "\n",
    "    if dtype.startswith(\"timestamp\"):\n",
    "        # Handles 'timestamp', 'timestamp[ms]', 'timestamp[us]' etc.\n",
    "        return TimestampType()\n",
    "\n",
    "    if dtype == \"bool\":\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype.startswith(\"booleantype\"):\n",
    "        return BooleanType()\n",
    "    \n",
    "    if dtype == \"binary\":\n",
    "        return BinaryType()\n",
    "    \n",
    "    if dtype.startswith(\"binarytype\"):\n",
    "        return BinaryType()\n",
    "    \n",
    "    raise ValueError(f\"Unsupported type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39bda8fa-2fe3-4b88-a720-4889475c4ab0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Base Schema"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df_table_candidates = spark.sql(f\"\"\"\n",
    "                                select distinct s3_bucket_name, bucket_prefix, table_name\n",
    "                                from {candidate_table}\n",
    "                                where  \n",
    "                                table_name = '{dataset_name}'\n",
    "                                \"\"\")\n",
    "\n",
    "# Collect all rows into Python memory\n",
    "rows = df_table_candidates.collect()\n",
    "\n",
    "for row in rows:\n",
    "    bucket_name = row[\"s3_bucket_name\"]\n",
    "    bucket_prefix = row[\"bucket_prefix\"]\n",
    "    dataset_name = row[\"table_name\"]\n",
    "\n",
    "\n",
    "    partition_key_combination = \"edp_run_id, snapshot_date\"\n",
    "    print(f\"Processing {dataset_name}\")\n",
    "    print(f\"Processing {partition_key_combination}\")\n",
    "\n",
    "    inventory_df = spark.sql(f\"\"\"SELECT distinct edp_run_id, snapshot_date\n",
    "                            FROM \n",
    "                            (\n",
    "                                select distinct edp_run_id, try_cast(snapshot_date as date) as snapshot_date\n",
    "                                from {inventory_table}\n",
    "                                where 1 = 1\n",
    "                                and extension is not null \n",
    "                                and partition_key = '{partition_key_combination}'\n",
    "                                and s3_bucket_name = '{bucket_name}' \n",
    "                                and bucket_prefix = '{bucket_prefix}'\n",
    "                                --and (load_status is null or load_status = 'failed')\n",
    "                            ) inventory\n",
    "                            join\n",
    "                            (\n",
    "                                select distinct run_id, try_cast(run_tag_value as date) as run_tag_value \n",
    "                                from {partition_audit_table}\n",
    "                                where dataset_name = '{dataset_name}'\n",
    "                                and lower(run_tag_key) = 'snapshot_date'\n",
    "                            ) partition\n",
    "                            on inventory.edp_run_id = partition.run_id\n",
    "                            and inventory.snapshot_date = partition.run_tag_value\n",
    "                            \"\"\")\n",
    "\n",
    "    df_latest_snapshot = (inventory_df\n",
    "                            .orderBy(col(\"snapshot_date\").desc())\n",
    "                            .limit(1)\n",
    "                            )\n",
    "\n",
    "    latest_snapshot_row = df_latest_snapshot.collect()[0]\n",
    "\n",
    "    latest_edp_run_id = latest_snapshot_row[\"edp_run_id\"]\n",
    "    latest_snapshot_date = latest_snapshot_row[\"snapshot_date\"]\n",
    "\n",
    "    print (f\"Processing Partition {latest_edp_run_id}:{latest_snapshot_date}\")\n",
    "\n",
    "    schema_json_df = spark.sql(f\"\"\"select schema_json \n",
    "                                from {parquet_schema_table}\n",
    "                                where 1 = 1\n",
    "                                and bucket_prefix = '{bucket_prefix}'\n",
    "                                and file_path like '%edp_run_id={latest_edp_run_id}/snapshot_date={latest_snapshot_date}%'\n",
    "                                \"\"\")\n",
    "    #and s3_bucket_name = '{bucket_name}'\n",
    "    # Extract the schema_json value into a Python string variable\n",
    "    schema_json = schema_json_df.first()['schema_json']\n",
    "\n",
    "    schema_dict = json.loads(schema_json)\n",
    "\n",
    "    # Convert to StructType\n",
    "    base_schema = StructType([\n",
    "        StructField(f[\"name\"], parse_type(f[\"type\"]), f[\"nullable\"])\n",
    "        for f in schema_dict[\"fields\"]\n",
    "    ])\n",
    "\n",
    "    partition_columns = [partition_column.strip() for partition_column in partition_key_combination.split(\",\") if partition_column.strip()]\n",
    "\n",
    "    # You can define mapping for known types here\n",
    "    partition_type_map = {\n",
    "        \"edp_run_id\": StringType(),\n",
    "        \"snapshot_date\": DateType()\n",
    "    }\n",
    "\n",
    "    extended_fields = base_schema.fields.copy()\n",
    "\n",
    "    for partition_column in partition_columns:\n",
    "        col_type = partition_type_map.get(partition_column, StringType())  # default to STRING\n",
    "        extended_fields.append(StructField(partition_column, col_type, True))\n",
    "\n",
    "    extended_schema = StructType(extended_fields)\n",
    "    # print(extended_schema)\n",
    "    schema_dtls=[]\n",
    "    for field in extended_schema:\n",
    "        dtype=str(field.dataType)\n",
    "        dtype=dtype.replace(\"Type\",\"\")\n",
    "        schema_dtls.append(Row(Column_Name=field.name.upper(),Data_Type=dtype))\n",
    "\n",
    "    base_df_schema=spark.createDataFrame(schema_dtls)\n",
    "    display(base_df_schema)\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a80bfcf-2c3d-4db4-a676-5b4d02fffc79",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"bucket_prefix\":315},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763625485487}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Get Failed Partitions"
    }
   },
   "outputs": [],
   "source": [
    "# df_failed_partitions=spark.sql (f\"\"\"\n",
    "# select a.* from\n",
    "# (select run_id, try_cast(run_tag_value as date) run_tag_value from 89055_ctg_prod_exp.default.dataset_tags\n",
    "#                                 where dataset_name = '{dataset_name}'\n",
    "#                                 and lower(run_tag_key) = 'snapshot_date'\n",
    "\n",
    "# ) a\n",
    "# left join\n",
    "# (select distinct edp_run_id, try_cast(snapshot_date as date) snapshot_date from {managed_table}) b\n",
    "# on a.run_id = b.edp_run_id\n",
    "# and a.run_tag_value = b.snapshot_date\n",
    "# where b.edp_run_id is null \"\"\")\n",
    "\n",
    "df_failed_partitions = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        inv.execution_id,\n",
    "        inv.s3_bucket_name,\n",
    "        inv.bucket_prefix,\n",
    "        regexp_extract(inv.bucket_prefix, '([^/]+)[/]?$', 1) AS dataset_name,\n",
    "        inv.edp_run_id,\n",
    "        inv.snapshot_date,\n",
    "        map.dbx_managed_table_schema AS managed_schema\n",
    "    FROM {inventory_table} inv\n",
    "    LEFT JOIN {dataset_mapping_table} map\n",
    "     -- ON inv.s3_bucket_name = map.s3_bucket_name\n",
    "     ON inv.bucket_prefix = map.bucket_prefix\n",
    "     AND regexp_extract(inv.bucket_prefix, '([^/]+)[/]?$', 1) = map.dataset_name\n",
    "    WHERE inv.load_status = 'failed'\n",
    "      AND inv.extension IS NOT NULL\n",
    "      -- AND  inv.extension ='parquet'\n",
    "      AND inv.edp_run_id IS NOT NULL\n",
    "      AND inv.snapshot_date IS NOT NULL\n",
    "      AND regexp_extract(inv.bucket_prefix, '([^/]+)[/]?$', 1)='{dataset_name}'\n",
    "    \"\"\"\n",
    ")\n",
    "display(df_failed_partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7057afc3-b5f4-4745-81cb-cfa439412a98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the failed partition record count"
    }
   },
   "outputs": [],
   "source": [
    "failed_snapshot_row = df_failed_partitions.collect()\n",
    "total_record_count=0\n",
    "bucket_name= 'app-id-89055-dep-id-109792-uu-id-n6ph64imx36e'\n",
    "bucket_prefix='trusted/analytics/data_dlvr/checks_ccbdl/'\n",
    "for row in failed_snapshot_row:\n",
    "    edp_run_id = row[\"edp_run_id\"]\n",
    "    snapshot_date = row[\"snapshot_date\"]\n",
    "    path=f\"s3://{bucket_name}/{bucket_prefix}/edp_run_id={edp_run_id}/snapshot_date={snapshot_date}/\"\n",
    "    failed_df = spark.read.parquet(path)\n",
    "    record_count=failed_df.count()\n",
    "    total_record_count+=record_count\n",
    "    # display(failed_df)\n",
    "\n",
    "print(total_record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3708a3b3-fa31-4598-b066-d099599b1e19",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":52},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762542457717}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"count\":101},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762974281103}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "Get schema for failed partitions and compare with base schema"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4284970994240863>, line 81\u001B[0m\n",
       "\u001B[1;32m     79\u001B[0m base_schema_column_count\u001B[38;5;241m=\u001B[39mbase_df_schema\u001B[38;5;241m.\u001B[39mcount()\n",
       "\u001B[1;32m     80\u001B[0m display(final_report_df_schema)\n",
       "\u001B[0;32m---> 81\u001B[0m passed_final_report_df_schema\u001B[38;5;241m=\u001B[39mfinal_report_df_schema\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStatus=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPASS\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     82\u001B[0m passed_final_report_df_schema\u001B[38;5;241m=\u001B[39mpassed_final_report_df_schema\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBase_Column_Name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n",
       "\u001B[1;32m     83\u001B[0m matched_schema_columns_count\u001B[38;5;241m=\u001B[39mpassed_final_report_df_schema\u001B[38;5;241m.\u001B[39mcount()\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'filter'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AttributeError",
        "evalue": "'NoneType' object has no attribute 'filter'"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-4284970994240863>, line 81\u001B[0m\n\u001B[1;32m     79\u001B[0m base_schema_column_count\u001B[38;5;241m=\u001B[39mbase_df_schema\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m     80\u001B[0m display(final_report_df_schema)\n\u001B[0;32m---> 81\u001B[0m passed_final_report_df_schema\u001B[38;5;241m=\u001B[39mfinal_report_df_schema\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStatus=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPASS\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     82\u001B[0m passed_final_report_df_schema\u001B[38;5;241m=\u001B[39mpassed_final_report_df_schema\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBase_Column_Name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m     83\u001B[0m matched_schema_columns_count\u001B[38;5;241m=\u001B[39mpassed_final_report_df_schema\u001B[38;5;241m.\u001B[39mcount()\n",
        "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'filter'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lit,when\n",
    "from pyspark.sql import Row\n",
    "\n",
    "failed_snapshot_row = df_failed_partitions.collect()\n",
    "final_report_df_schema=None\n",
    "for row in failed_snapshot_row:\n",
    "    failed_edp_run_id = row[\"edp_run_id\"]\n",
    "    failed_snapshot_date = row[\"snapshot_date\"]\n",
    "\n",
    "    print (f\"Processing Partition {failed_edp_run_id}:{failed_snapshot_date}\")\n",
    "\n",
    "    schema_json_df = spark.sql(f\"\"\"select schema_json \n",
    "                                from {parquet_schema_table}\n",
    "                                where 1 = 1\n",
    "                                and bucket_prefix = '{bucket_prefix}'\n",
    "                                and file_path like '%edp_run_id={failed_edp_run_id}/snapshot_date={failed_snapshot_date}%'\n",
    "                                \"\"\")\n",
    "    #and s3_bucket_name = '{bucket_name}'\n",
    "    # Extract the schema_json value into a Python string variable\n",
    "    schema_json = schema_json_df.first()['schema_json']\n",
    "\n",
    "    schema_dict = json.loads(schema_json)\n",
    "\n",
    "    # Convert to StructType\n",
    "    base_schema = StructType([\n",
    "        StructField(f[\"name\"], parse_type(f[\"type\"]), f[\"nullable\"])\n",
    "        for f in schema_dict[\"fields\"]\n",
    "    ])\n",
    "\n",
    "    partition_columns = [partition_column.strip() for partition_column in partition_key_combination.split(\",\") if partition_column.strip()]\n",
    "\n",
    "    # You can define mapping for known types here\n",
    "    partition_type_map = {\n",
    "        \"edp_run_id\": StringType(),\n",
    "        \"snapshot_date\": DateType()\n",
    "    }\n",
    "\n",
    "    extended_fields = base_schema.fields.copy()\n",
    "\n",
    "    for partition_column in partition_columns:\n",
    "        col_type = partition_type_map.get(partition_column, StringType())  # default to STRING\n",
    "        extended_fields.append(StructField(partition_column, col_type, True))\n",
    "\n",
    "    extended_schema = StructType(extended_fields)\n",
    "    # print(extended_schema)\n",
    "    schema_dtls=[]\n",
    "    for field in extended_schema:\n",
    "        dtype=str(field.dataType)\n",
    "        dtype=dtype.replace(\"Type\",\"\")\n",
    "        schema_dtls.append(Row(Column_Name=field.name.upper(),Data_Type=dtype))\n",
    "\n",
    "    failed_df_schema=spark.createDataFrame(schema_dtls)\n",
    "    matched_colums_df=None\n",
    "    matched_colums_df=base_df_schema.alias(\"s\").join(\n",
    "                                            failed_df_schema.alias(\"t\"), \n",
    "                                            on=\"Column_Name\", \n",
    "                                            how=\"inner\"\n",
    "                                        )\n",
    "    matched_colums_df=matched_colums_df.select(\n",
    "                            col(\"s.Column_Name\").alias(\"Base_Column_Name\"),\n",
    "                            col(\"s.Data_Type\").alias(\"Base_Data_Type\"),\n",
    "                            col(\"t.Column_Name\").alias(\"Failed_Column_Name\"),\n",
    "                            col(\"t.Data_Type\").alias(\"Failed_Data_Type\")\n",
    "                        )\n",
    "\n",
    "    matched_colums_df=matched_colums_df.withColumn(\"Status\",\n",
    "                            when(\n",
    "                                col(\"Base_Data_Type\")==col(\"Failed_Data_Type\"), lit(\"PASS\")\n",
    "                            ).otherwise(lit(\"FAIL\"))\n",
    "                        ).withColumn(\"dataset_name\", lit(dataset_name))\n",
    "    \n",
    "    # matched\n",
    "    \n",
    "    if final_report_df_schema is None:\n",
    "        final_report_df_schema=matched_colums_df\n",
    "    else:\n",
    "        final_report_df_schema=final_report_df_schema.unionAll(matched_colums_df)\n",
    "\n",
    "base_schema_column_count=base_df_schema.count()\n",
    "display(final_report_df_schema)\n",
    "passed_final_report_df_schema=final_report_df_schema.filter(\"Status='PASS'\")\n",
    "passed_final_report_df_schema=passed_final_report_df_schema.groupBy(\"Base_Column_Name\").count()\n",
    "matched_schema_columns_count=passed_final_report_df_schema.count()\n",
    "\n",
    "failed_final_report_df_schema=final_report_df_schema.filter(\"Status='FAIL'\")\n",
    "failed_final_report_df_schema=failed_final_report_df_schema.groupBy(\"Failed_Column_Name\").count()\n",
    "unmatched_schema_columns_count=failed_final_report_df_schema.count()\n",
    "display(failed_final_report_df_schema)\n",
    "print(f\"Total Columns:{base_schema_column_count} \\n Matched Columns:{matched_schema_columns_count} \\n Unmatched Columns:{unmatched_schema_columns_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2953136-b339-49d5-b2e2-0c77aac54728",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the results into table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "candidate_table": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_table_candidates",
        "dataset_mapping_table": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_dataset_mapping",
        "dataset_name": "alsc_rt_acct_loan_ext_m",
        "inventory_table": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_table_inventory",
        "managed_table": "89055_ctg_prod_managed.89055_trusted_db_xlob_hcd_dora_fdl.edw_dim_eci_dtl",
        "parquet_schema_table": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_parquet_schemas",
        "partition_audit_table": "89055_ctg_prod.89055_audit_db_hcd_dora_fdl.dataset_tags",
        "schema_comparision_results": "89055_ctg_prod_exp.dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp.ccbr_migration_schema_comparision_results_2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_report_df_schema.write.mode(\"append\").saveAsTable(schema_comparision_results)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5792998528381691,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "9_compare_schema_for_failed_partitions",
   "widgets": {
    "candidate_table": {
     "currentValue": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_table_candidates",
     "nuid": "2a8742b4-c65c-41ea-af56-ccae87d75520",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "candidate_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_mapping_table": {
     "currentValue": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_dataset_mapping",
     "nuid": "c100d853-d763-4f52-a2a7-bbd0f5670b1b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "dataset_mapping_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dataset_name": {
     "currentValue": "alsc_rt_acct_loan_ext_m",
     "nuid": "3d9b578c-378f-443b-b333-145fd4549f85",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "dataset_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "dataset_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "inventory_table": {
     "currentValue": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_table_inventory",
     "nuid": "3aad63c4-f241-474c-a6d0-c92578ccf280",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "inventory_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "managed_table": {
     "currentValue": "89055_ctg_prod_managed.89055_trusted_db_xlob_hcd_dora_fdl.edw_dim_eci_dtl",
     "nuid": "b75e0d96-6b51-43cf-8401-cce56bdd7e00",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "managed_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "managed_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_schema_table": {
     "currentValue": "89055_ctg_prod.89055_ccbr_migration.ccbr_migration_parquet_schemas",
     "nuid": "edb08c32-0fca-477b-8708-97e7d952b3c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "parquet_schema_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "partition_audit_table": {
     "currentValue": "89055_ctg_prod.89055_audit_db_hcd_dora_fdl.dataset_tags",
     "nuid": "f59e1ef1-f249-4fa4-b9b2-a7fd23d732fd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "partition_audit_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_comparision_results": {
     "currentValue": "89055_ctg_prod_exp.dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp.ccbr_migration_schema_comparision_results",
     "nuid": "e5333456-8e3a-481a-989b-6f2712c8d5ca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_comparision_results",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_comparision_results",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}