{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44292e1b-f698-42d1-acbd-27b765c9b410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CCBR Migration get_schema_from_parquet_files\n",
    "\n",
    "Purpose:\n",
    "This notebook reads the inventory table which contains the S3 buckets and file paths. \n",
    "\n",
    "For performance reasons, the notebook uses pyarrow in conjunction with Databricks volumes so we use a \n",
    "static map table for the conversion of S3 buckets to DB Volumes\n",
    "\n",
    "The number of bucket_prefixes woul be passed in as a comma separated string and each prefix is processed.\n",
    "\n",
    "The process will read the schemas for all the files, determine if the file is a valid parquet file\n",
    "and also detect if the schema has changed from its baseline using a hash. The baseline is the oldest file.\n",
    "\n",
    "The schema is stored in the schema table as a json string for detailed comparison as needed. \n",
    " \n",
    "Once complete, bucket prefix in the candidate table will be updated with a true (ready for migration) or false which means\n",
    "that the schema has changed from the baseline which could cause a table migration to fail.\n",
    "\n",
    "The compare_schema notebook can compare the schemas within a prefix and provide a detailed view of how the schema has changed over time to help make a decision about how to move forward.\n",
    "\n",
    "Once all of the prefix parameters are processed, the job will exit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb63c7d-9b22-4a87-92da-64fa18334e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    udf, col, lit, current_timestamp, hash as spark_hash,\n",
    "    sum as spark_sum, count\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, BooleanType, TimestampType\n",
    ")\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0cb3d1-6970-40c6-9815-734618e0fd18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema\", \"\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"s3_inventory_table\", \"\", \"S3 Inventory Table\")\n",
    "dbutils.widgets.text(\"parquet_schema_table\", \"\", \"Parquet Schema Table\")\n",
    "dbutils.widgets.text(\"candidate_table\", \"\", \"Candidate Table\")\n",
    "dbutils.widgets.text(\"bucket_name\", \"\",\"Bucket Name\")\n",
    "dbutils.widgets.text(\"file_counts\", \"\",\"File Counts\")\n",
    "dbutils.widgets.text(\"partition_table\", \"\",\"Partition Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca179888-ed7d-43c3-8235-d4c11ffd224d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog =                         dbutils.widgets.get(\"catalog\")\n",
    "app_schema =                      dbutils.widgets.get(\"schema\")\n",
    "parquet_schema_table =            dbutils.widgets.get(\"parquet_schema_table\")\n",
    "s3_inventory_table =              dbutils.widgets.get(\"s3_inventory_table\")\n",
    "candidate_table =                 dbutils.widgets.get(\"candidate_table\")\n",
    "bucket_name =                     dbutils.widgets.get(\"bucket_name\")\n",
    "file_counts =                     dbutils.widgets.get(\"file_counts\")\n",
    "partition_table =                     dbutils.widgets.get(\"partition_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93b2ac8-aa6b-4f63-800d-ac333319109e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This function will be called for each prefix to be processed\n",
    "# This notebook will collect the schemas based on the DB Volume\n",
    "\n",
    "def getSql(pPrefix):\n",
    "    partition_key_combination = \"edp_run_id, snapshot_date\"\n",
    "    sql_query =  f\"\"\"\n",
    "                    SELECT distinct\n",
    "                         inv.file_path,\n",
    "                         inv.s3_bucket_name,\n",
    "                         inv.bucket_prefix\n",
    "                    FROM \n",
    "                        (select distinct \n",
    "                            CONCAT('s3://', s3_bucket_name, '/', left(key, LENGTH(key) - POSITION('/' IN REVERSE(key)))) as file_path\n",
    "                            ,edp_run_id, snapshot_date, s3_bucket_name, bucket_prefix\n",
    "                        from {catalog}.{app_schema}.{s3_inventory_table}\n",
    "                        where \n",
    "                            bucket_prefix = '{pPrefix}'\n",
    "                            and extension is not null\n",
    "                            and load_status is null\n",
    "                            and lower(partition_key) = '{partition_key_combination}'\n",
    "                        ) inv\n",
    "                    JOIN \n",
    "                        (select \n",
    "                            s3_bucket_name, bucket_prefix\n",
    "                        from {catalog}.{app_schema}.{candidate_table}\n",
    "                        where \n",
    "                            bucket_prefix = '{pPrefix}' \n",
    "                            and candidate_for_managed_table_creation is null\n",
    "                        ) c \n",
    "                    ON  inv.s3_bucket_name = c.s3_bucket_name \n",
    "                        and inv.bucket_prefix = c.bucket_prefix\n",
    "                    JOIN\n",
    "                        (\n",
    "                            select distinct dataset_name, run_id, try_cast(run_tag_value as date) as run_tag_value \n",
    "                            from {partition_table}\n",
    "                            where lower(run_tag_key) = 'snapshot_date'\n",
    "                        ) partition\n",
    "                    on regexp_extract(inv.bucket_prefix, '([^/]+)/?$', 1) = partition.dataset_name\n",
    "                    and inv.edp_run_id = partition.run_id\n",
    "                    and try_cast(inv.snapshot_date as date) = partition.run_tag_value\n",
    "                \"\"\"\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbee27f-5fe2-49ab-ba7b-2a6e2b30531b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getPrefixes():\n",
    "    sql_query =  f\"\"\"\n",
    "                    SELECT \n",
    "                         distinct c.execution_id, c.bucket_prefix\n",
    "                     FROM \n",
    "                        {catalog}.{app_schema}.{candidate_table} c \n",
    "                     WHERE \n",
    "                         c.managed_table_created is null           AND\n",
    "                         c.candidate_for_managed_table_creation is null AND\n",
    "                         c.s3_bucket_name = '{bucket_name}' AND\n",
    "                         c.structured_file_count between {file_counts}\n",
    "                    \"\"\"\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "761a27cc-1981-448e-803b-c6acd6da8edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 1. Get prefixes\n",
    "# --------------------------------------------------------------\n",
    "dfBucketPrefixes = spark.sql(getPrefixes())\n",
    "#display(dfBucketPrefixes)\n",
    "bucket_prefixes = [(row['execution_id'], row['bucket_prefix']) for row in dfBucketPrefixes.collect()]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Main loop\n",
    "# --------------------------------------------------------------\n",
    "for execution_id, prefix in bucket_prefixes:\n",
    "    print(f\"\\n=== Processing prefix: {prefix} ===\")\n",
    "\n",
    "    # ---- 2.1 Get file list -------------------------------------------------\n",
    "    try:\n",
    "        file_df = spark.sql(getSql(prefix))\n",
    "        file_count = file_df.count()\n",
    "        print(f\" file paths retrieved for {prefix} : {file_count}\")\n",
    "\n",
    "        # default_p = spark.sparkContext.defaultParallelism\n",
    "        # partition_count = min(max(file_count // 1000, 200), default_p * 2)\n",
    "        # file_df = file_df.repartition(partition_count)\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying file list: {e}\")\n",
    "        raise\n",
    "\n",
    "    # ---- 2.2 COLLECT to driver --------------------------------------------\n",
    "    file_paths = [\n",
    "        (row.file_path,  row.s3_bucket_name, row.bucket_prefix)\n",
    "        for row in file_df.select(\"file_path\", \"s3_bucket_name\", \"bucket_prefix\").collect()\n",
    "    ]\n",
    "\n",
    "    if not file_paths:\n",
    "        print(\" No files found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Collected {len(file_paths)} file paths to driver...\")\n",
    "\n",
    "    # ---- 2.3 Extract schema ON DRIVER -------------------------------------\n",
    "    schema_records = []\n",
    "    for file_path, bucket, bucket_prefix in file_paths:\n",
    "        print(f\"Reading schema: {file_path}\")\n",
    "        try:\n",
    "            df_schema = spark.read.parquet(file_path)\n",
    "            schema = df_schema.schema\n",
    "\n",
    "            fields = [\n",
    "                {\n",
    "                    \"name\": f.name.lower(),\n",
    "                    \"type\": str(f.dataType),\n",
    "                    \"nullable\": f.nullable,\n",
    "                    \"metadata\": {}\n",
    "                }\n",
    "                for f in schema.fields\n",
    "            ]\n",
    "            fields = sorted(fields, key=lambda x: x[\"name\"])\n",
    "            schema_json = json.dumps({\"type\": \"struct\", \"fields\": fields}, sort_keys=True)\n",
    "\n",
    "            is_corrupted = False\n",
    "            error_msg = None\n",
    "        except Exception as e:\n",
    "            schema_json = \"corrupted\"\n",
    "            is_corrupted = True\n",
    "            error_msg = str(e).split(\"\\n\")[0]\n",
    "\n",
    "        schema_records.append((\n",
    "            file_path, schema_json, is_corrupted, error_msg,\n",
    "            prefix, execution_id, bucket\n",
    "        ))\n",
    "\n",
    "    # ---- 2.4 Create DF ----------------------------------------------------\n",
    "    schema_df = spark.createDataFrame(\n",
    "        schema_records,\n",
    "        StructType([\n",
    "            StructField(\"file_path\", StringType(), False),\n",
    "            StructField(\"schema_json\", StringType(), True),\n",
    "            StructField(\"is_corrupted\", BooleanType(), False),\n",
    "            StructField(\"error_message\", StringType(), True),\n",
    "            StructField(\"bucket_prefix\", StringType(), False),\n",
    "            StructField(\"execution_id\", StringType(), False),\n",
    "            StructField(\"s3_bucket_name\", StringType(), False),\n",
    "        ])\n",
    "    ).withColumn(\"run_timestamp\", current_timestamp())\n",
    "\n",
    "    # ---- 2.5 Write --------------------------------------------------------\n",
    "    try:\n",
    "        schema_df.write.mode(\"append\").saveAsTable(f\"{catalog}.{app_schema}.{parquet_schema_table}\")\n",
    "        print(f\"Schemas written\")\n",
    "    except Exception as e:\n",
    "        print(f\"Delta write error: {e}\")\n",
    "        raise\n",
    "\n",
    "    # ---- 2.6 Baseline -----------------------------------------------------\n",
    "    baseline_schema = spark.sql(f\"\"\"\n",
    "        SELECT first_value(schema_json) OVER (ORDER BY lower(file_path)) AS baseline_schema\n",
    "        FROM {catalog}.{app_schema}.{parquet_schema_table}\n",
    "        WHERE bucket_prefix = '{prefix}' AND execution_id = '{execution_id}' AND is_corrupted = false\n",
    "        LIMIT 1\n",
    "    \"\"\").collect()[0][\"baseline_schema\"]\n",
    "\n",
    "    baseline_filepath = spark.sql(f\"\"\"\n",
    "        SELECT first_value(file_path) OVER (ORDER BY lower(file_path)) AS baseline_filepath\n",
    "        FROM {catalog}.{app_schema}.{parquet_schema_table}\n",
    "        WHERE bucket_prefix = '{prefix}' AND execution_id = '{execution_id}' AND is_corrupted = false\n",
    "        LIMIT 1\n",
    "    \"\"\").collect()[0][\"baseline_filepath\"]\n",
    "\n",
    "    # ---- 2.7 Partition ----------------------------------------------------\n",
    "    def partition_structure(fp):\n",
    "        parts = re.findall(r'([^/]+)=[^/]+', fp)\n",
    "        return '/'.join(sorted(parts)) if parts else \"\"\n",
    "\n",
    "    part_udf = udf(partition_structure, StringType())\n",
    "    analysis_df = schema_df.withColumn(\"partition_structure\", part_udf(col(\"file_path\")))\n",
    "    baseline_part = partition_structure(baseline_filepath)\n",
    "\n",
    "    # ---- 2.8 Consistency --------------------------------------------------\n",
    "    analysis_df = analysis_df.filter(\n",
    "        (~col(\"is_corrupted\")) & (col(\"execution_id\") == lit(execution_id))\n",
    "    )\n",
    "    total_good = analysis_df.count()\n",
    "\n",
    "    opt = analysis_df \\\n",
    "        .withColumn(\"schema_hash\", spark_hash(col(\"schema_json\"))) \\\n",
    "        .withColumn(\"base_hash\", lit(spark_hash(lit(baseline_schema)))) \\\n",
    "        .withColumn(\"schema_match\", col(\"schema_hash\") == col(\"base_hash\")) \\\n",
    "        .withColumn(\"part_match\", col(\"partition_structure\") == lit(baseline_part))\n",
    "\n",
    "    stats = opt.agg(\n",
    "        spark_sum(col(\"schema_match\").cast(\"int\")).alias(\"match_schema\"),\n",
    "        spark_sum(col(\"part_match\").cast(\"int\")).alias(\"match_part\"),\n",
    "        count(\"*\").alias(\"total\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    schema_ok = stats[\"match_schema\"] == total_good\n",
    "    part_ok   = stats[\"match_part\"]   == total_good\n",
    "\n",
    "    corrupted = schema_df.filter(\n",
    "        col(\"is_corrupted\") & (col(\"execution_id\") == lit(execution_id))\n",
    "    ).count()\n",
    "\n",
    "    candidate = not (corrupted > 0 or not schema_ok or not part_ok)\n",
    "\n",
    "    # ---- 2.9 Summary ------------------------------------------------------\n",
    "    result = {\n",
    "        \"total_non_corrupted_files\": total_good,\n",
    "        \"corrupted_files\": corrupted,\n",
    "        \"schema_consistent\": schema_ok,\n",
    "        \"partition_consistent\": part_ok,\n",
    "    }\n",
    "    result_json = json.dumps(result)\n",
    "\n",
    "    print(f\"Schema consistency: good={total_good}, corrupted={corrupted}, candidate={candidate}\")\n",
    "\n",
    "    # ---- 2.10 MERGE (FIXED) -----------------------------------------------\n",
    "    exists = spark.sql(f\"\"\"\n",
    "        SELECT 1 FROM {catalog}.{app_schema}.{candidate_table}\n",
    "        WHERE bucket_prefix = '{prefix}' AND execution_id = '{execution_id}'\n",
    "    \"\"\").limit(1).collect()\n",
    "\n",
    "    if not exists:\n",
    "        print(\"SKIPPED  no row in candidate table\")\n",
    "        print(\"-\" * 80)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO {catalog}.{app_schema}.{candidate_table} AS tgt\n",
    "            USING (\n",
    "                SELECT\n",
    "                    '{prefix}'      AS bucket_prefix,\n",
    "                    {str(candidate).lower()}::boolean AS candidate_for_managed_table_creation,\n",
    "                    '{execution_id}' AS execution_id,\n",
    "                    '{result_json}'  AS schema_analysis_results\n",
    "            ) AS src\n",
    "            ON tgt.bucket_prefix = src.bucket_prefix\n",
    "           AND tgt.execution_id   = src.execution_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                candidate_for_managed_table_creation = src.candidate_for_managed_table_creation,\n",
    "                schema_analysis_results               = src.schema_analysis_results\n",
    "        \"\"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"MERGE error: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"Processed {len(file_paths)} files\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n=== Completed ===\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7158679028990128,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3_get_schema_from_parquet_files",
   "widgets": {
    "bucket_name": {
     "currentValue": "app-id-89055-dep-id-109792-uu-id-wj1in46hrkbl",
     "nuid": "54ba1977-a136-4631-8839-c6fb4ff2841d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Bucket Name",
      "name": "bucket_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Bucket Name",
      "name": "bucket_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "candidate_table": {
     "currentValue": "ccbr_migration_table_candidates",
     "nuid": "92750a2d-5e8a-4652-9551-74144dbcb27d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Candidate Table",
      "name": "candidate_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Candidate Table",
      "name": "candidate_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "89055_ctg_prod_exp",
     "nuid": "903e6388-198d-49e0-80e4-6ef475bd1ba4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_counts": {
     "currentValue": "1 AND 999999",
     "nuid": "6b274e09-5a52-4387-9d70-a977eb881d89",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "File Counts",
      "name": "file_counts",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "File Counts",
      "name": "file_counts",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_schema_table": {
     "currentValue": "ccbr_migration_parquet_schemas",
     "nuid": "acd1407a-00de-43ee-8c32-7354a457bf82",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Parquet Schema Table",
      "name": "parquet_schema_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Parquet Schema Table",
      "name": "parquet_schema_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "partition_table": {
     "currentValue": "89055_ctg_prod_exp.default.dataset_tags",
     "nuid": "8ab05c72-de95-4982-b2ba-ddda2c73274a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Partition Table",
      "name": "partition_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Partition Table",
      "name": "partition_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_inventory_table": {
     "currentValue": "ccbr_migration_table_inventory",
     "nuid": "0c533824-22f7-4b78-b035-dc06b60ec6ab",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "S3 Inventory Table",
      "name": "s3_inventory_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "S3 Inventory Table",
      "name": "s3_inventory_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "dbx_89055_trusted_db_mdas_ais_hcd_dora_fdl_prod_exp",
     "nuid": "9fceaabe-fab1-4523-bfcb-583a1b7df16b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}